#!/bin/sh

check_backup() {
  local NODE="$1"

  check_wal_archive "$NODE"

  check_automatic_backup "$NODE"

  if check_timelines
  then
    echo "SECCESS. All timelines are correct"
  else
    echo "FAIL. Some timelines are not correct"
    return 1
  fi
}

check_wal_archive() {
  local RESULT
  local EXIT_CODE
  try_function wait_until try_check_wal_archive "$NODE"
  if "$RESULT"
  then
    echo "SUCCESS. The WAL is available"
  else
    echo "FAIL. The WAL is not available"
    return 1
  fi
}

try_check_wal_archive() {
  local NODE="$1"
  local CURRENT_WAL_FILE
  CURRENT_WAL_FILE="$(kubectl exec -t -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
    psql -t -A -U postgres -p 5432 -c \
    'SELECT r.file_name from pg_walfile_name_offset(pg_current_wal_lsn()) as r')"
  kubectl exec -t -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
    psql -t -A -U postgres -p 5432 -c 'CHECKPOINT'
  wait_until -t "$((E2E_TIMEOUT / 10))" eval '[ "$(kubectl exec -t -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
      psql -t -A -U postgres -p 5432 -c \
      "SELECT r.file_name from pg_walfile_name_offset(pg_switch_wal()) as r")" != "$CURRENT_WAL_FILE" ]'
  wait_until -t "$((E2E_TIMEOUT / 10))" timeout -s KILL "$((E2E_TIMEOUT / 20))" \
    kubectl exec -t -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c patroni -- \
    exec-with-env backup -- wal-g wal-fetch "$CURRENT_WAL_FILE" "/tmp/$CURRENT_WAL_FILE"
}

check_automatic_backup() {
  local NODE="$1"
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" --all
  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The backup CR is available"
  else
    echo "FAIL. The backup CR is not available"
    return 1
  fi
}

check_timelines(){
  local BACKUP_LINE
  local EXPECTED_TIMELINE
  local ACTUAL_TIMELINE
  local CUSTOM_COLUMNS=NAME:.metadata.name,PHASE:.status.process.status,WAL_FILE:.status.backupInformation.startWalFile,TIMELINE:.status.backupInformation.timeline
  BACKUP_LINE=$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | sed 's/\s\+/ /g' | grep "Completed")

  EXPECTED_TIMELINE=$(printf "%d" "0x$(echo "$BACKUP_LINE" | cut -d ' ' -f 3 | grep -o '^.\{8\}')")
  ACTUAL_TIMELINE=$(echo "$BACKUP_LINE" | cut -d ' ' -f 4)

  if [ "$EXPECTED_TIMELINE" != "$ACTUAL_TIMELINE" ]
  then
   echo "FAIL. Timeline of $(echo "$BACKUP_LINE" | cut -d ' ' -f 1) with walFile $(echo "$BACKUP_LINE" | cut -d ' ' -f 3) is incorrect. Actual timeline: $ACTUAL_TIMELINE"
   return 1
  else
    echo "Backup $(echo "$BACKUP_LINE" | cut -d ' ' -f 1) timeline is correct"
  fi
}

check_automatic_backup_cr() {
  local NODE="$1"

  if wait_until is_automatic_backup_cr_completed "$NODE"
  then
    echo "SUCCESS. The full backup is available"
  else
    echo "FAIL. The full backup is not available"
    return 1
  fi
}

is_automatic_backup_cr_completed() {
  local NODE="$1"
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod
  local BACKUP_NAME

  BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"
  
  [ "$(kubectl exec -t -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$BACKUP_NAME " | wc -l)" -gt 0 ]
}

check_manual_backup() {
  local NODE="$1"
  local BACKUP_NAME="${CLUSTER_NAME}-${NODE}-$(shuf -i 0-65535 -n 1)"
  local ACTUAL_BACKUP_NAME
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  # Avoid automatic backups while testing manual backups
  kubectl patch sgbackupconfig -n "$CLUSTER_NAMESPACE" backupconf --type json \
    --patch '[{"op":"replace","path":"/spec/baseBackups/cronSchedule","value":"0 5 31 2 *"}]'

  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" --all

  create_backup "$BACKUP_NAME" false

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The created backup CR did complete"
  else
    echo "FAIL. The created backup CR did not complete"
    return 1
  fi

  ACTUAL_BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  if [ -z "$ACTUAL_BACKUP_NAME" ]
  then
    echo "FAIL. Backup name not found"
    return 1
  fi

  if [ "$(kubectl exec -t -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$ACTUAL_BACKUP_NAME " | wc -l)" -gt 0 ]
  then
    echo "SUCCESS. The full backup of created CR is available"
  else
    echo "FAIL. The full backup of created CR is not available"
    return 1
  fi

  check_is_managed_lifecycle_value false

  kubectl patch sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/managedLifecycle","value":true}]'

  create_backup "$BACKUP_NAME-2" true

  wait_until check_is_managed_lifecycle_value true

  local DEFAULT_ANNOTATION=$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" -o json | jq '.metadata.annotations["stackgres.io/operatorVersion"]')

  if [ -z "$DEFAULT_ANNOTATION" ] || [ "$DEFAULT_ANNOTATION" = "null" ]
  then
    echo "Fail. Default annotations not created"
    return 1
  else
    echo "Success. Defaults annotations created"
  fi

  create_backup "$BACKUP_NAME-3" true

  if wait_until eval '! kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" > /dev/null 2>&1'
  then
    echo "SUCCESS. The backup retention has been honored. Old backups deleted"
  else
    echo "FAIL. The backup retention has not been honored. Old backups not deleted"
    return 1
  fi

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o name | wc -l)" = "2" ]'
  then
    echo "SUCCESS. The backup retention has been honored. Remaining backups are 2"
  else
    echo "FAIL. The backup retention has not been honored. Remaining backups are $(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o name | wc -l)"
    return 1
  fi

  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME-2"
  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME-3"

  # Avoid automatic backups while testing manual backups
  kubectl patch sgbackupconfig -n "$CLUSTER_NAMESPACE" backupconf --type json \
    --patch '[{"op":"replace","path":"/spec/baseBackups/cronSchedule","value":"*/1 * * * *"}]'

  if wait_until eval '[ "$(kubectl exec -t -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$ACTUAL_BACKUP_NAME " | wc -l)" -eq 0 ]'
  then
    echo "SUCCESS. The full backup of deleted CR has been removed"
  else
    echo "FAIL. The full backup of deleted CR has not been removed"
    return 1
  fi
}

create_backup() {
  cat << EOF | kubectl create -f -
apiVersion: stackgres.io/v1beta1
kind: SGBackup
metadata:
  namespace: "$CLUSTER_NAMESPACE"
  name: "$1"
spec:
  sgCluster: "$CLUSTER_NAME"
  managedLifecycle: $2
EOF
}

check_is_managed_lifecycle_value() {
  local CUSTOM_COLUMNS=MANAGED_LYFECYCLE:.status.process.managedLifecycle,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod
  local ACTUAL_MANAGED_LYFECYCLE="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  if [ "$ACTUAL_MANAGED_LYFECYCLE" != "$1" ]
  then
    echo "FAIL. Backup /status/process/managedLifecycle expected to be $1 but was $ACTUAL_MANAGED_LYFECYCLE"
    return 1
  else
    echo "SUCCESS. Backup /status/process/managedLifecycle was $1"
  fi
}

check_backup_retention() {
  local NODE="$1"
  local CUSTOM_COLUMNS=NAME:.metadata.name,BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
      -o custom-columns="$CUSTOM_COLUMNS" | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -ge 2 ]'
  then
    echo "SUCCESS. The maximum retainable backups are available"
  else
    echo "FAIL. The maximum retainable backups are not available"
    return 1
  fi

  local BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" --sort-by '{.metadata.creationTimestamp}' \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  if wait_until eval '! kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" > /dev/null 2>&1'
  then
    echo "SUCCESS. The backup retention has been honored. Old backups deleted"
  else
    echo "FAIL. The backup retention has not been honored. Old backups not deleted"
    return 1
  fi

  if wait_until -t 5 eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o name | wc -l)" = "2" ]'
  then
    echo "SUCCESS. The backup retention has been honored. Remaining backups are 2"
  else
    echo "FAIL. The backup retention has not been honored. Remaining backups are $(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o name | wc -l)"
    return 1
  fi
}

check_single_backup_job() {
  if ! wait_until -t 60 -i 0 eval '[ "$(get_manual_backup_pod_count)" -gt 0 ]'
  then
    echo "Good. Only 1 running backup job"
  else
    echo "Only 1 backup job should be running"
    return 1
  fi
}

get_manual_backup_pod_count() {
  kubectl get pod -n "$CLUSTER_NAMESPACE" -l backup=true \
    --template '{{ range .items }}{{ printf "%s %s\n" .metadata.ownerReferences .metadata.labels }}{{ end }}' \
    | grep ' kind:Job ' | wc -l
}

check_switchover() {
  if kubectl exec -t -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-0 -c patroni -- \
    patronictl switchover --master "$CLUSTER_NAME"-0 --candidate "$CLUSTER_NAME"-1 --force
  then
    echo "SUCCESS. The switchover has been performed"
  else
    echo "FAIL. The switchover operation failed"
    return 1
  fi

  if wait_until run_query -i 0 -p 5432
  then
    echo "SUCCESS. The replica becomed available"
  else
    echo "FAIL. The replica is not available"
    return 1
  fi

  if ! run_query -i 1 -p 5432 -q "'SELECT pg_is_in_recovery()'" | grep -q '^t$'
  then
    echo "SUCCESS. The primary is now the node at index 1"
  else
    echo "FAIL. The node at index 1 is not the primary"
    return 1
  fi

  if ! run_query -i 0 -p 5432 -q "'SELECT pg_is_in_recovery()'" | grep -q '^f$'
  then
    echo "SUCCESS. The replica is now the node at index 0"
  else
    echo "FAIL. The node at index 0 is not a replica"
    return 1
  fi
}
