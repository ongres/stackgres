#!/bin/sh

. "$SPEC_PATH/abstract/backup"

. "$SPEC_PATH/abstract/sql-scripts"

e2e_test_extra_hash() {
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/backup")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/abstract/sql-scripts")"
  "$SHELL" "$PROJECT_PATH/stackgres-k8s/ci/build/build-functions.sh" path_hash \
    "$(realpath --relative-to "$PROJECT_PATH" "$SPEC_PATH/sql-scripts.sakila.sql")"
}

e2e_test_install() {
  STREAM_NAME="$(get_sgstreams_name "$SPEC_NAME-operation")"
  TARGET_CLUSTER_NAME="$(get_sgstreams_name "$SPEC_NAME-target")"
  POSTGIS_VERSION="$(get_latest_version_of_extension postgis "$E2E_POSTGRES_VERSION" || true)"

  install_minio

  cat << 'EOF' | kubectl create -n "$CLUSTER_NAMESPACE" secret generic sql-scripts-sakila-user \
    --from-literal=create-sakila-user.sql="$(cat)"
DO $$
BEGIN
  IF NOT EXISTS (SELECT * FROM pg_roles WHERE rolname = 'sakila') THEN
    EXECUTE 'CREATE USER sakila WITH PASSWORD ''sakila'';';
  END IF;
END$$;
EOF

  kubectl create -n "$CLUSTER_NAMESPACE" configmap sql-scripts-sakila-schema \
    --from-file=create-sakila-schema.sql="$SPEC_PATH/sql-scripts.sakila.sql"

  create_or_replace_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE" 1 \
    --set-string "cluster.managedSql.scripts[0].script=CREATE DATABASE sakila" \
    --set-string "cluster.managedSql.scripts[1].database=sakila" \
    --set-string "cluster.managedSql.scripts[1].scriptFrom.secretKeyRef.name=sql-scripts-sakila-user" \
    --set-string "cluster.managedSql.scripts[1].scriptFrom.secretKeyRef.key=create-sakila-user.sql" \
    --set-string "cluster.managedSql.scripts[2].database=sakila" \
    --set-string "cluster.managedSql.scripts[2].wrapInTransaction=repeatable-read" \
    --set-string "cluster.managedSql.scripts[2].scriptFrom.configMapKeyRef.name=sql-scripts-sakila-schema" \
    --set-string "cluster.managedSql.scripts[2].scriptFrom.configMapKeyRef.key=create-sakila-schema.sql"
  wait_until kubectl -n "$CLUSTER_NAMESPACE" get secret "$CLUSTER_NAME" >/dev/null 2>&1

  create_or_replace_cluster "$TARGET_CLUSTER_NAME" "$CLUSTER_NAMESPACE" 1 \
    --set configurations.create=false \
    --set instanceProfiles=null \
    --set-string cluster.postgres.extensions[0].name=postgis \
    --set-string "cluster.postgres.extensions[0].version=$POSTGIS_VERSION" \
    --set-string "cluster.managedSql.scripts[0].script=CREATE EXTENSION postgis" \
    --set-string "cluster.managedSql.scripts[1].script=CREATE DATABASE sakila"

  deploy_curl_pod "$CLUSTER_NAMESPACE"

  wait_pods_running "$CLUSTER_NAMESPACE" 4
  wait_cluster "$CLUSTER_NAME" "$CLUSTER_NAMESPACE"
  wait_cluster "$TARGET_CLUSTER_NAME" "$CLUSTER_NAMESPACE"
}

e2e_test() {
  if [ -z "$POSTGIS_VERSION" ]
  then
    echo "Skipping stream-to-cluster since postgis not available for $(uname -m)"
    return
  fi

  run_test "Checking that stream is working" check_stream_is_working

  run_test "Checking that stream is working skipping DDL import" check_stream_is_working_skippig_ddl_import

  run_test "Checking that stream copying schema first is working" check_stream_copy_schema_is_working
}

check_stream_is_working_skippig_ddl_import() {
  check_stream_is_working true
}

check_stream_is_working() {
  SKIP_DDL_IMPORT="${1:-false}"
  cat << 'EOF' | tee "$LOG_PATH/list-types.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on -tA > "$LOG_PATH/types"
SELECT typcategory || ' ' || typtype || ' ' || typname || ' ' || typformattype
FROM (
  SELECT
    t.typname AS typname,
    pg_catalog.format_type(t.oid, NULL) AS typformattype,
    text(t.typcategory) AS typcategory,
    text(t.typtype) AS typtype
  FROM pg_catalog.pg_type t LEFT JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace
  WHERE (t.typrelid = 0 OR (SELECT c.relkind = 'c' FROM pg_catalog.pg_class c WHERE c.oid = t.typrelid))
    AND NOT EXISTS(SELECT 1 FROM pg_catalog.pg_type el WHERE el.oid = t.typelem AND el.typarray = t.oid)
    AND pg_catalog.pg_type_is_visible(t.oid)
    AND t.typtype NOT IN ('p')
    AND t.typcategory NOT IN ('Z')
    AND t.typname NOT LIKE 'reg%'
    AND t.typname != 'int2vector' -- See https://stackoverflow.com/a/74612592
    AND t.typname != 'oidvector' -- See https://stackoverflow.com/a/74612592
    AND t.typname != 'gtsvector' -- See https://doxygen.postgresql.org/tsgistidx_8c_source.html#l00094
    AND t.typname != 'refcursor' -- See https://www.postgresql.org/docs/current/plpgsql-cursors.html
    AND t.typname != 'pg_dependencies' -- See https://github.com/postgres/postgres/blob/035f99cbebe5ffcaf52f8370394446cd59621ab7/src/backend/statistics/dependencies.c#L646-L664
    AND t.typname != 'pg_mcv_list' -- See https://github.com/postgres/postgres/blob/master/src/backend/statistics/README.mcv
    AND t.typname != 'pg_ndistinct' -- See https://www.postgresql.org/message-id/MN2PR05MB68795FCDB5B560D350084753B6FA9%40MN2PR05MB6879.namprd05.prod.outlook.com
    AND t.typname != 'pg_node_tree' -- See https://www.postgresql.org/message-id/20595.1347653162%40sss.pgh.pa.us
  UNION ALL
  SELECT
    (CASE
      WHEN t.typname = 'int2' THEN 'smallserial'
      WHEN t.typname = 'int4' THEN 'serial'
      ELSE 'bigserial'
      END) AS typname,
    pg_catalog.format_type(t.oid, NULL) AS typformattype,
    text(t.typcategory) AS typcategory,
    text(t.typtype) AS typtype
  FROM pg_catalog.pg_type t LEFT JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace
  WHERE t.typname IN ('int2', 'int4', 'int8')) _
ORDER BY typname;
EOF

  cat << EOF | tee "$LOG_PATH/reset-target-status.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$TARGET_CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
ALTER DATABASE postgres SET sgstream.ddl_import_completed = false;
EOF

  TYPE_COUNT="$(wc -l "$LOG_PATH/types" | cut -d ' ' -f 1)"
  cat << EOF | tee "$LOG_PATH/init-tables.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
DROP TABLE IF EXISTS test;
CREATE TABLE test(i bigint, t text, PRIMARY KEY(i));
DROP TABLE IF EXISTS pop;

INSERT INTO test SELECT i, 'test' FROM generate_series(1, 3) AS i ON CONFLICT (i) DO UPDATE SET t=EXCLUDED.t;

DO \$\$BEGIN
EXECUTE \$execute\$CREATE OR REPLACE FUNCTION create_complex_table() RETURNS void AS \$sql\$
$(
cat "$LOG_PATH/types" | while read -r TYPE_CATEGORY TYPE_TYPE TYPE_NAME TYPE_FORMAT_TYPE
do
	cat << INNER_EOF
DROP TABLE IF EXISTS complex_$TYPE_NAME;
CREATE TABLE complex_$TYPE_NAME(i bigint,\$execute\$ || quote_ident('c_$TYPE_NAME') || ' $TYPE_NAME' || \$execute\$, PRIMARY KEY (i));
INNER_EOF
done
)
\$sql\$ LANGUAGE sql\$execute\$;
END\$\$;

DO \$\$BEGIN
EXECUTE 'CREATE OR REPLACE FUNCTION insert_complex(i bigint) RETURNS void LANGUAGE plpgsql AS \$plpgsql\$BEGIN
$(
cat "$LOG_PATH/types" | while read -r TYPE_CATEGORY TYPE_TYPE TYPE_NAME TYPE_FORMAT_TYPE
do
	cat << INNER_EOF
  EXECUTE \$insert\$INSERT INTO complex_$TYPE_NAME SELECT \$insert\$ || i || \$insert\$, '
    || CASE
        WHEN '$TYPE_NAME' = 'aclitem' THEN '''' || makeaclitem('postgres'::regrole, 'authenticator'::regrole, 'SELECT', FALSE) || ''''
        WHEN '$TYPE_NAME' IN ('json', 'jsonb') THEN '''true'''
        WHEN '$TYPE_NAME' = 'jsonpath' THEN '''$.a'''
        WHEN '$TYPE_NAME' = 'macaddr' THEN '''08:00:2b:01:02:03'''
        WHEN '$TYPE_NAME' = 'macaddr8' THEN '''08:00:2b:01:02:03:04:05'''
        WHEN '$TYPE_NAME' = 'pg_lsn' THEN '''FFFFFFFF/FFFFFFFF'''
        WHEN '$TYPE_NAME' = 'pg_snapshot' THEN '''' || txid_current_snapshot() || ''''
        WHEN '$TYPE_NAME' = 'txid_snapshot' THEN '''10:20:10,14,15'''
        WHEN '$TYPE_NAME' = 'uuid' THEN '''a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'''
        WHEN '$TYPE_NAME' = 'datemultirange' THEN '''{(,)}'''
        WHEN '$TYPE_NAME' = 'daterange' THEN '''(,)'''
        WHEN '$TYPE_NAME' = 'int4multirange' THEN '''{[1,2), [3,4)}'''
        WHEN '$TYPE_NAME' = 'int4range' THEN '''[2,4)'''
        WHEN '$TYPE_NAME' = 'int8multirange' THEN '''{[4,12)}'''
        WHEN '$TYPE_NAME' = 'int8range' THEN '''(3,7)'''
        WHEN '$TYPE_NAME' = 'nummultirange' THEN '''{[1.1,2.2)}'''
        WHEN '$TYPE_NAME' = 'numrange' THEN '''(1.1,2.2)'''
        WHEN '$TYPE_NAME' IN ('tsmultirange', 'tstzmultirange') THEN '''{[2011-01-01,2011-03-01)}'''
        WHEN '$TYPE_NAME' IN ('tsrange', 'tstzrange') THEN '''[2011-01-01,2011-03-01)'''
        WHEN '$TYPE_NAME' = 'dblink_pkey_results' THEN '''(1,2)'''
        WHEN '$TYPE_NAME' = 'line' THEN '''{1,2,3}'''
        WHEN '$TYPE_NAME' IN ('tid', 'point') THEN '''(1,2)'''
        WHEN '$TYPE_NAME' = 'circle' THEN '''<(1,2),3>'''
        WHEN '$TYPE_NAME' IN ('lseg','box','path','polygon') THEN '''((1,2),(3,4))'''
        WHEN '$TYPE_CATEGORY' IN ('I') THEN '''1.2.3.4'''
        WHEN '$TYPE_CATEGORY' IN ('D') THEN '''' || NOW() || ''''
        WHEN '$TYPE_CATEGORY' IN ('Z') THEN '''t'''
        WHEN '$TYPE_TYPE' IN ('r','m') OR '$TYPE_CATEGORY' IN ('A') THEN 'array[]'
        WHEN '$TYPE_CATEGORY' IN ('N','V','T') OR '$TYPE_NAME' IN ('cid', 'xid', 'xid8') THEN '''1'''
        ELSE '''t''' END
    || '::$TYPE_FORMAT_TYPE\$insert\$;
INNER_EOF
done
)
  END\$plpgsql\$;';
END\$\$;

SELECT create_complex_table();
SELECT insert_complex(i) FROM generate_series(1, 3) AS i; 
EOF

  cat << EOF | tee "$LOG_PATH/sgstream-working.yaml" | kubectl replace --force -f -
apiVersion: stackgres.io/v1alpha1
kind: SGStream
metadata:
  namespace: $CLUSTER_NAMESPACE 
  name: "$STREAM_NAME"
spec:
  maxRetries: 0
  source:
$(
  if [ "$SKIP_DDL_IMPORT" = true ]
  then
    cat << INNER_EOF
    type: Postgres
    postgres:
      host: "$CLUSTER_NAME"
      port: 5433
      username:
        name: "$CLUSTER_NAME"
        key: superuser-username
      password:
        name: "$CLUSTER_NAME"
        key: superuser-password
      debeziumProperties:
INNER_EOF
  else
    cat << INNER_EOF
    type: SGCluster
    sgCluster:
      name: "$CLUSTER_NAME"
      debeziumProperties:
INNER_EOF
  fi
)
  target:
    type: SGCluster
    sgCluster:
      name: "$TARGET_CLUSTER_NAME"
      skipDdlImport: $SKIP_DDL_IMPORT
      debeziumProperties:
  pods:
    persistentVolume:
      size: 1Gi
  debeziumEngineProperties:
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.snapshot.snapshotCompleted | grep -qxF true'
  then
    success "snapshot completed"
  else
    fail "snapshot did not completed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq '.status.snapshot.rowsScanned["public.test"]' | grep -qxF 3
  then
    success "test table scanned"
  else
    fail "test table not scanned"
  fi

  cat "$LOG_PATH/types" | while read -r TYPE_CATEGORY TYPE_TYPE TYPE_NAME TYPE_FORMAT_TYPE
  do
    if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq '.status.snapshot.rowsScanned["public.complex_'"$TYPE_NAME"'"]' | grep -qxF 3
    then
      success "complex_$TYPE_NAME table scanned"
    else
      fail "complex_$TYPE_NAME table not scanned"
    fi
  done

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.connected | grep -qxF true'
  then
    success "streaming started"
  else
    fail "streaming not started"
  fi

  cat << 'EOF' | tee "$LOG_PATH/insert-tables.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
INSERT INTO test SELECT * FROM generate_series(4, 6);

SELECT insert_complex(i) FROM generate_series(4, 6) AS i; 
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.numberOfCommittedTransactions | grep -qxF '"$(( 2 ))"
  then
    success "streaming insert transaction successful"
  else
    fail "streaming insert transaction failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfCreateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 3 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfUpdateEventsSeen | grep -qxF 0 \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfDeleteEventsSeen | grep -qxF 0
  then
    success "streaming insert events successful"
  else
    fail "streaming insert events failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 3 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.events.totalNumberOfEventsSent | grep -qxF "$(( (TYPE_COUNT + 1) * 6 ))"
  then
    success "sent insert events successful"
  else
    fail "sent insert events failed"
  fi

  cat << EOF | tee "$LOG_PATH/delete-tables.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
DELETE FROM test WHERE i = 1;

$(
cat "$LOG_PATH/types" | while read -r TYPE_CATEGORY TYPE_TYPE TYPE_NAME TYPE_FORMAT_TYPE
do
  cat << INNER_EOF
DELETE FROM "complex_$TYPE_NAME" WHERE i = 1;
INNER_EOF
done
)
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.numberOfCommittedTransactions | grep -qxF '"$(( 3 + TYPE_COUNT ))"
  then
    success "streaming delete transaction successful"
  else
    fail "streaming delete transaction failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfCreateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 3 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfUpdateEventsSeen | grep -qxF 0 \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfDeleteEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) ))"
  then
    success "streaming delete events successful"
  else
    fail "streaming delete events failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 4 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.events.totalNumberOfEventsSent | grep -qxF "$(( (TYPE_COUNT + 1) * 8 ))"
  then
    success "sent delete events successful"
  else
    fail "sent delete events failed"
  fi

  cat << EOF | tee "$LOG_PATH/update-tables.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
UPDATE test SET t = 'hello' WHERE i = 6;

$(
cat "$LOG_PATH/types" | while read -r TYPE_CATEGORY TYPE_TYPE TYPE_NAME TYPE_FORMAT_TYPE
do
  cat << INNER_EOF
UPDATE "complex_$TYPE_NAME" SET "c_$TYPE_NAME" = (CASE
        WHEN '$TYPE_NAME' = 'aclitem' THEN makeaclitem('postgres'::regrole, 'authenticator'::regrole, 'UPDATE', FALSE)::text
        WHEN '$TYPE_NAME' IN ('json', 'jsonb') THEN 'false'
        WHEN '$TYPE_NAME' = 'jsonpath' THEN '$.b'
        WHEN '$TYPE_NAME' = 'macaddr' THEN '09:00:2b:01:02:03'
        WHEN '$TYPE_NAME' = 'macaddr8' THEN '09:00:2b:01:02:03:04:05'
        WHEN '$TYPE_NAME' = 'pg_lsn' THEN '0/0'
        WHEN '$TYPE_NAME' = 'pg_snapshot' THEN txid_current_snapshot()::text
        WHEN '$TYPE_NAME' = 'txid_snapshot' THEN '20:30:20,24,25'
        WHEN '$TYPE_NAME' = 'uuid' THEN 'b0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'
        WHEN '$TYPE_NAME' = 'datemultirange' THEN '{(,)}'
        WHEN '$TYPE_NAME' = 'daterange' THEN '(,)'
        WHEN '$TYPE_NAME' = 'int4multirange' THEN '{[2,3), [4,5)}'
        WHEN '$TYPE_NAME' = 'int4range' THEN '[3,5)'
        WHEN '$TYPE_NAME' = 'int8multirange' THEN '{[5,13)}'
        WHEN '$TYPE_NAME' = 'int8range' THEN '(4,8)'
        WHEN '$TYPE_NAME' = 'nummultirange' THEN '{[2.2,3.3)}'
        WHEN '$TYPE_NAME' = 'numrange' THEN '(2.2,3.3)'
        WHEN '$TYPE_NAME' IN ('tsmultirange', 'tstzmultirange') THEN '{[2011-01-02,2011-03-02)}'
        WHEN '$TYPE_NAME' IN ('tsrange', 'tstzrange') THEN '[2011-01-02,2011-03-02)'
        WHEN '$TYPE_NAME' = 'dblink_pkey_results' THEN '(2,3)'
        WHEN '$TYPE_NAME' = 'line' THEN '{2,3,4}'
        WHEN '$TYPE_NAME' IN ('tid', 'point') THEN '(2,3)'
        WHEN '$TYPE_NAME' = 'circle' THEN '<(2,3),4>'
        WHEN '$TYPE_NAME' IN ('lseg','box','path','polygon') THEN '((2,3),(4,5))'
        WHEN '$TYPE_CATEGORY' IN ('I') THEN '2.3.4.5'
        WHEN '$TYPE_CATEGORY' IN ('D') THEN NOW()::text
        WHEN '$TYPE_CATEGORY' IN ('Z') THEN 'f'
        WHEN '$TYPE_TYPE' IN ('r','m') OR '$TYPE_CATEGORY' IN ('A') THEN 'array[]'
        WHEN '$TYPE_CATEGORY' IN ('N','V','T') OR '$TYPE_NAME' IN ('cid', 'xid', 'xid8') THEN '0'
        ELSE 'f' END)::$TYPE_FORMAT_TYPE
     WHERE i = 6;
INNER_EOF
done
)
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.numberOfCommittedTransactions | grep -qxF '"$(( 4 + TYPE_COUNT * 2 ))"
  then
    success "streaming update transaction successful"
  else
    fail "streaming update transaction failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfCreateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 3 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfUpdateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfDeleteEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) ))"
  then
    success "streaming update events successful"
  else
    fail "streaming update events failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 5 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.events.totalNumberOfEventsSent | grep -qxF "$(( (TYPE_COUNT + 1) * 9 ))"
  then
    success "sent update events successful"
  else
    fail "sent update events failed"
  fi

  cat << 'EOF' | tee "$LOG_PATH/alter-tables.sql" | kubectl exec -i -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c postgres-util -- psql -q -v ON_ERROR_STOP=on
ALTER TABLE test ADD COLUMN n int DEFAULT 0;

ALTER TABLE test DROP COLUMN t;

INSERT INTO test SELECT i, i FROM generate_series(7, 9) AS i;

CREATE TABLE pop(i bigint, t text, PRIMARY KEY(i));

INSERT INTO pop SELECT i, 'test' FROM generate_series(1, 3) AS i;
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.numberOfCommittedTransactions | grep -qxF '"$(( 6 + TYPE_COUNT * 2 ))"
  then
    success "streaming alter transaction successful"
  else
    fail "streaming alter transaction failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfCreateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 3 + 6 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfUpdateEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfDeleteEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) ))"
  then
    success "streaming alter events successful"
  else
    fail "streaming alter events failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.totalNumberOfEventsSeen | grep -qxF "$(( (TYPE_COUNT + 1) * 5 + 6 ))" \
    && kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.events.totalNumberOfEventsSent | grep -qxF "$(( (TYPE_COUNT + 1) * 9 + 6 ))"
  then
    success "sent alter events successful"
  else
    fail "sent alter events failed"
  fi

  if kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.events.lastEventWasSent | grep -qxF true
  then
    success "sent last event successful"
  else
    fail "sent last event failed"
  fi

  kubectl annotate sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" debezium-signal.stackgres.io/tombstone=

  if kubectl wait --timeout="${E2E_TIMEOUT}s" sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" --for=condition=Completed
  then
    success "stream has completed"
  else
    fail "stream has not completed"
  fi

  if [ "$SKIP_DDL_IMPORT" != true ]
  then
    local QUERY
    QUERY="$(cat << 'EOF'
DROP TABLE IF EXISTS input; CREATE TEMPORARY TABLE input (line text);
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dumpall --clean --if-exists --roles-only; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dump --clean --if-exists --schema-only --dbname=postgres --exclude-table="(test|spatial_ref_sys)" --exclude-schema="__migration__" --no-publications --no-subscriptions; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dump --data-only --dbname=postgres --exclude-table="(test|spatial_ref_sys)" --exclude-schema="__migration__"; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
SELECT line FROM (SELECT regexp_split_to_table(convert_from(decode(line, 'base64'), 'UTF8'), E'\n') AS line FROM input) _
  WHERE line NOT LIKE '-- %' AND line NOT LIKE '--' AND line != '' -- Skip comments and empty lines
  AND line NOT SIMILAR TO '(CREATE|ALTER|DROP) ROLE(| IF EXISTS) (postgres|replicator|authenticator)%' -- Skip SGCluster existing roles
  AND line NOT SIMILAR TO '(DROP|CREATE) EXTENSION(| IF EXISTS| IF NOT EXISTS) (dblink|postgis)(;| %)'
  AND line NOT SIMILAR TO 'COMMENT ON EXTENSION (dblink|postgis) %'
  AND line NOT SIMILAR TO '% SET "sgstream.ddl_import_completed" TO ''true'';'
  ;
EOF
  )"
    run_query -p 5432 -i "0" -h "$CLUSTER_NAME" -c "$CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$QUERY" | grep -v '^COPY ' > "$LOG_PATH/expected-schema"
    run_query -p 5432 -i "0" -h "$TARGET_CLUSTER_NAME" -c "$TARGET_CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$QUERY" | grep -v '^COPY ' > "$LOG_PATH/actual-schema"
    if diff "$LOG_PATH/expected-schema" "$LOG_PATH/actual-schema"
    then
      success "schema was migrated successfully"
    else
      fail "schema was not migrated successfully"
    fi
  fi
}

check_stream_copy_schema_is_working() {
  wait_until check_sakila_database "$CLUSTER_NAME"

  cat << EOF | tee "$LOG_PATH/sgstream-copy-schema-working.yaml" | kubectl replace --force -f -
apiVersion: stackgres.io/v1alpha1
kind: SGStream
metadata:
  namespace: $CLUSTER_NAMESPACE 
  name: "$STREAM_NAME"
spec:
  maxRetries: 0
  source:
    type: SGCluster
    sgCluster:
      name: "$CLUSTER_NAME"
      database: sakila
      debeziumProperties:
        snapshotSelectStatementOverrides:
          public.payment: "SELECT * FROM ONLY payment"
  target:
    type: SGCluster
    sgCluster:
      name: "$TARGET_CLUSTER_NAME"
      database: sakila
      debeziumProperties:
  pods:
    persistentVolume:
      size: 1Gi
  debeziumEngineProperties:
EOF

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.snapshot.snapshotCompleted | grep -qxF true'
  then
    success "snapshot completed"
  else
    fail "snapshot did not completed"
  fi

  if wait_until eval 'kubectl get sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" -o json | jq .status.streaming.connected | grep -qxF true'
  then
    success "streaming started"
  else
    fail "streaming not started"
  fi

  kubectl annotate sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" debezium-signal.stackgres.io/tombstone=

  if kubectl wait --timeout="${E2E_TIMEOUT}s" sgstream -n "$CLUSTER_NAMESPACE" "$STREAM_NAME" --for=condition=Completed
  then
    success "stream has completed"
  else
    fail "stream has not completed"
  fi

  local SCHEMA_QUERY
  SCHEMA_QUERY="$(cat << 'EOF'
DROP TABLE IF EXISTS input; CREATE TEMPORARY TABLE input (line text);
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dumpall --clean --if-exists --roles-only; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dump --clean --if-exists --schema-only --dbname=sakila --exclude-table="(test|spatial_ref_sys)" --exclude-schema="__migration__" --no-publications --no-subscriptions; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
SELECT line FROM (SELECT regexp_split_to_table(convert_from(decode(line, 'base64'), 'UTF8'), E'\n') AS line FROM input) _
  WHERE line NOT LIKE '-- %' AND line NOT LIKE '--' AND line != '' -- Skip comments and empty lines
  AND line NOT SIMILAR TO '(CREATE|ALTER|DROP) ROLE(| IF EXISTS) (postgres|replicator|authenticator)%' -- Skip SGCluster existing roles
  AND line NOT SIMILAR TO '(DROP|CREATE) EXTENSION(| IF EXISTS| IF NOT EXISTS) (dblink|postgis)(;| %)'
  AND line NOT SIMILAR TO 'COMMENT ON EXTENSION (dblink|postgis) %'
  AND line NOT SIMILAR TO '% SET "sgstream.ddl_import_completed" TO ''true'';'
  ;
EOF
)"
  run_query -p 5432 -i "0" -h "$CLUSTER_NAME" -c "$CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$SCHEMA_QUERY" > "$LOG_PATH/expected-schema"
  run_query -p 5432 -i "0" -h "$TARGET_CLUSTER_NAME" -c "$TARGET_CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$SCHEMA_QUERY" > "$LOG_PATH/actual-schema"
  if diff "$LOG_PATH/expected-schema" "$LOG_PATH/actual-schema"
  then
    success "sakila schema was migrated successfully"
  else
    fail "sakila schema was not migrated successfully"
  fi

  local DATA_QUERY
  DATA_QUERY="$(cat << 'EOF'
DROP TABLE IF EXISTS input; CREATE TEMPORARY TABLE input (line text);
COPY input FROM PROGRAM $cmd$sh -c '{ { { pg_dump --data-only --inserts --rows-per-insert=1 --dbname=sakila --exclude-table="(test|spatial_ref_sys)" --exclude-schema="__migration__"; echo $? >&3; } | base64 -w 0 >&4; } 3>&1 | { read EXIT_CODE; exit "$EXIT_CODE"; }; } 4>&1'$cmd$ DELIMITER E'\1';
SELECT regexp_replace(line, 'setval\(([^,]*,[^,]*), true\)', 'setval(\1, false)') FROM (SELECT regexp_split_to_table(convert_from(decode(line, 'base64'), 'UTF8'), E'\n') AS line FROM input) _
  WHERE line NOT LIKE '-- %' AND line NOT LIKE '--' AND line != '' -- Skip comments and empty lines
  AND line NOT SIMILAR TO '(CREATE|ALTER|DROP) ROLE(| IF EXISTS) (postgres|replicator|authenticator)%' -- Skip SGCluster existing roles
  AND line NOT SIMILAR TO '(DROP|CREATE) EXTENSION(| IF EXISTS| IF NOT EXISTS) (dblink|postgis)(;| %)'
  AND line NOT SIMILAR TO 'COMMENT ON EXTENSION (dblink|postgis) %'
  AND line NOT SIMILAR TO '% SET "sgstream.ddl_import_completed" TO ''true'';'
  ;
EOF
)"
  run_query -p 5432 -i "0" -h "$CLUSTER_NAME" -c "$CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$DATA_QUERY" | sort > "$LOG_PATH/expected-data"
  run_query -p 5432 -i "0" -h "$TARGET_CLUSTER_NAME" -c "$TARGET_CLUSTER_NAME" -n "$CLUSTER_NAMESPACE" -q "$DATA_QUERY" | sort > "$LOG_PATH/actual-data"
  if diff "$LOG_PATH/expected-data" "$LOG_PATH/actual-data"
  then
    success "sakila data was migrated successfully"
  else
    fail "sakila data was not migrated successfully"
  fi
}

check_sakila_database() {
  local CLUSTER_NAME="$1"
  check_user "$1" 0
  check_database "$1" 0
  check_schema "$1" 0
}
