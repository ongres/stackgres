#!/bin/sh

check_backup() {
  local NODE="${1:-0}"

  check_wal_archive "$NODE"

  check_automatic_backup "$NODE"

  if check_timelines
  then
    echo "SUCCESS. All timelines are correct"
  else
    echo "FAIL. Some timelines are not correct"
    return 1
  fi

  if check_control_data
  then
    echo "SUCCESS. All backups have valid control data"
  else
    echo "FAIL. Some backups have invalid controld data"
    return 1
  fi
}

check_wal_archive() {
  local NODE="${1:-0}"
  local RESULT
  local EXIT_CODE
  try_function wait_until try_check_wal_archive "$NODE"
  if "$RESULT"
  then
    echo "SUCCESS. The WAL is available"
  else
    echo "FAIL. The WAL is not available"
    return 1
  fi
}

try_check_wal_archive() {
  local NODE="${1:-0}"
  local CURRENT_WAL_FILE
  CURRENT_WAL_FILE="$(rotate_wal_file "$1")"
  wait_until -t "$((E2E_TIMEOUT / 10))" timeout -s KILL "$((E2E_TIMEOUT / 20))" \
    kubectl exec -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c patroni -- \
    exec-with-env backup -- wal-g wal-fetch "$CURRENT_WAL_FILE" "/tmp/$CURRENT_WAL_FILE"
}

rotate_wal_file() {
  local NODE="${1:-0}"
  local CURRENT_WAL_FILE
  CURRENT_WAL_FILE="$(get_current_wal_file "$1")"
  kubectl exec -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
    psql -t -A -U postgres -p 5432 -c 'CHECKPOINT' >/dev/null
  wait_until -t "$((E2E_TIMEOUT / 10))" eval '[ "$(kubectl exec -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
      psql -t -A -U postgres -p 5432 -c \
      "SELECT r.file_name from pg_walfile_name_offset(pg_switch_wal()) as r")" != "$CURRENT_WAL_FILE" ]' >&2
  printf '%s' "$CURRENT_WAL_FILE"
}

get_current_wal_file() {
  local NODE="${1:-0}"
  kubectl exec -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME"-"$NODE" -c postgres-util -- \
    psql -t -A -U postgres -p 5432 -c \
    'SELECT r.file_name from pg_walfile_name_offset(pg_current_wal_lsn()) as r'
}

check_automatic_backup() {
  local NODE="${1:-0}"
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" --all
  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Running   " | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The automatic backup CR is running"
  else
    echo "FAIL. The automatic backup CR is not running"
    return 1
  fi

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The automatic backup CR has completed"
  else
    echo "FAIL. The automatic backup CR has failed"
    return 1
  fi
}

check_timelines(){
  local BACKUP_LINE
  local EXPECTED_TIMELINE
  local ACTUAL_TIMELINE
  local CUSTOM_COLUMNS=NAME:.metadata.name,PHASE:.status.process.status,WAL_FILE:.status.backupInformation.startWalFile,TIMELINE:.status.backupInformation.timeline
  BACKUP_LINE="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | sed 's/\s\+/ /g' | grep "Completed")"

  EXPECTED_TIMELINE="$(echo "$BACKUP_LINE" | cut -d ' ' -f 3)"
  EXPECTED_TIMELINE="$(printf "%d" 0x$(expr substr "$EXPECTED_TIMELINE" 1 8))"
  ACTUAL_TIMELINE="$(echo "$BACKUP_LINE" | cut -d ' ' -f 4)"

  if [ "$EXPECTED_TIMELINE" != "$ACTUAL_TIMELINE" ]
  then
   echo "FAIL. Timeline of $(echo "$BACKUP_LINE" | cut -d ' ' -f 1) with walFile $(echo "$BACKUP_LINE" | cut -d ' ' -f 3) is incorrect. Actual timeline: $ACTUAL_TIMELINE"
   return 1
  else
    echo "Backup $(echo "$BACKUP_LINE" | cut -d ' ' -f 1) timeline is correct"
  fi
}

check_control_data(){
  local CUSTOM_COLUMNS=NAME:.metadata.name,PHASE:.status.process.status

  local EXPECTED_CONTROL_DATA_LENGTH
  EXPECTED_CONTROL_DATA_LENGTH="$(kubectl exec -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME-0" -c patroni \
    -- bash -c 'pg_controldata --pgdata=$PG_DATA_PATH' | wc -l)"
  kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep "Completed" | awk '{print $1}' | while read BACKUP_NAME
    do
      local ACTUAL_CONTROL_DATA_LENGTH
      ACTUAL_CONTROL_DATA_LENGTH="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" -o json \
        | jq '.status.backupInformation.controlData | length')"

      if [ "$EXPECTED_CONTROL_DATA_LENGTH" != "$ACTUAL_CONTROL_DATA_LENGTH" ]
      then
        echo "FAIL. Backup $BACKUP_NAME has invalid controlData. Expected $EXPECTED_CONTROL_DATA_LENGTH fields, found $ACTUAL_CONTROL_DATA_LENGTH"
        return 1
      else
        echo "Backup $BACKUP_NAME has valid controlData"
      fi

    done

}

check_automatic_backup_cr() {
  local NODE="${1:-0}"

  if wait_until is_automatic_backup_cr_completed "$NODE"
  then
    echo "SUCCESS. The full backup is available"
  else
    echo "FAIL. The full backup is not available"
    return 1
  fi
}

is_automatic_backup_cr_completed() {
  local NODE="${1:-0}"
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod
  local BACKUP_NAME

  BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  [ "$(kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$BACKUP_NAME " | wc -l)" -gt 0 ]
}

check_manual_backup() {
  local NODE="${1:-0}"
  local BACKUP_NAME
  BACKUP_NAME="$(get_sgbackup_name "${CLUSTER_NAME}-${NODE}-$(shuf -i 0-65535 -n 1)")"
  local BACKUP_2_NAME
  BACKUP_2_NAME="$(get_sgbackup_name "${CLUSTER_NAME}-${NODE}-$(shuf -i 0-65535 -n 1)-2")"
  local BACKUP_3_NAME
  BACKUP_3_NAME="$(get_sgbackup_name "${CLUSTER_NAME}-${NODE}-$(shuf -i 0-65535 -n 1)-3")"
  local ACTUAL_BACKUP_NAME
  local CUSTOM_COLUMNS=BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  # Avoid automatic backups while testing manual backups
  disable_cronSchedule
  
  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" --all

  create_backup "$BACKUP_NAME" false

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Running   " | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The manual backup CR is running"
  else
    echo "FAIL. The manual backup CR is not running"
    return 1
  fi

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
      -o custom-columns="$CUSTOM_COLUMNS" \
      | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -gt 0 ]'
  then
    echo "SUCCESS. The manual backup CR has complete"
  else
    echo "FAIL. The manual backup CR has failed"
    return 1
  fi

  ACTUAL_BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  if [ -z "$ACTUAL_BACKUP_NAME" ]
  then
    echo "FAIL. Backup name not found"
    return 1
  fi

  if [ "$(kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$ACTUAL_BACKUP_NAME " | wc -l)" -gt 0 ]
  then
    echo "SUCCESS. The full backup of created CR is available"
  else
    echo "FAIL. The full backup of created CR is not available"
    return 1
  fi

  check_is_managed_lifecycle_value false

  kubectl patch sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/managedLifecycle","value":true}]'

  create_backup "$BACKUP_2_NAME" true

  wait_until check_is_managed_lifecycle_value true

  local DEFAULT_ANNOTATION
  DEFAULT_ANNOTATION="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" -o json | jq '.metadata.annotations["stackgres.io/operatorVersion"]')"

  if [ -z "$DEFAULT_ANNOTATION" ] || [ "$DEFAULT_ANNOTATION" = "null" ]
  then
    echo "FAIL. Default annotations not created"
    return 1
  else
    echo "SUCCESS. Defaults annotations created"
  fi

  create_backup "$BACKUP_3_NAME" true

  if wait_until eval '! kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" > /dev/null 2>&1'
  then
    echo "SUCCESS. The backup retention has been honored. Old backups deleted"
  else
    echo "FAIL. The backup retention has not been honored. Old backups not deleted"
    return 1
  fi

  if wait_until eval '[ "$(get_completed_backup_count)" = "2" ]'
  then
    echo "SUCCESS. The backup retention has been honored. Remaining backups are 2"
  else
    echo "FAIL. The backup retention has not been honored. Remaining backups are $(get_completed_backup_count)"
    return 1
  fi

  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_2_NAME"
  kubectl delete sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_3_NAME"

  # Avoid automatic backups while testing manual backups

  enable_cronSchedule
  
  if wait_until eval '[ "$(kubectl exec -n "$CLUSTER_NAMESPACE" "${CLUSTER_NAME}-${NODE}" -c patroni -- \
      exec-with-env backup -- wal-g backup-list | grep "^$ACTUAL_BACKUP_NAME " | wc -l)" -eq 0 ]'
  then
    echo "SUCCESS. The full backup of deleted CR has been removed"
  else
    echo "FAIL. The full backup of deleted CR has not been removed"
    return 1
  fi
}

create_backup() {
  cat << EOF | kubectl create -f -
apiVersion: stackgres.io/v1
kind: SGBackup
metadata:
  namespace: "$CLUSTER_NAMESPACE"
  name: "$1"
spec:
  sgCluster: "$CLUSTER_NAME"
  managedLifecycle: $2
EOF
}

check_is_managed_lifecycle_value() {
  local CUSTOM_COLUMNS=MANAGED_LYFECYCLE:.status.process.managedLifecycle,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod
  local ACTUAL_MANAGED_LYFECYCLE
  ACTUAL_MANAGED_LYFECYCLE="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" \
    -o custom-columns="$CUSTOM_COLUMNS" \
    | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | tail -n 1 | cut -d ' ' -f 1)"

  if [ "$ACTUAL_MANAGED_LYFECYCLE" != "$1" ]
  then
    echo "FAIL. Backup /status/process/managedLifecycle expected to be $1 but was $ACTUAL_MANAGED_LYFECYCLE"
    return 1
  else
    echo "SUCCESS. Backup /status/process/managedLifecycle was $1"
  fi
}

check_backup_retention() {
  local NODE="${1:-0}"
  local CUSTOM_COLUMNS=NAME:.metadata.name,BACKUP_NAME:.status.internalName,PHASE:.status.process.status,HOSTNAME:.status.backupInformation.sourcePod

  if wait_until eval '[ "$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" \
      -o custom-columns="$CUSTOM_COLUMNS" | tail -n +2 | grep " Completed   ${CLUSTER_NAME}-${NODE}$" | wc -l)" -ge 2 ]'
  then
    echo "SUCCESS. The maximum retainable backups are available"
  else
    echo "FAIL. The maximum retainable backups are not available"
    return 1
  fi

  local BACKUP_NAME
  BACKUP_NAME="$(kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o json \
    jq -r '.items | sort_by(.metadata.creationTimestamp)[]
      | select(
        .status != null and .status.process != null
        and .status.backupInformation.sourcePod != null
        and .status.process.status == "Completed"
        and .status.backupInformation.sourcePod == "'"${CLUSTER_NAME}-${NODE}"'"
        ).metadata.name' \
      | head -n 1)"

  if wait_until -t "$((E2E_TIMEOUT * 2))" eval '! kubectl get sgbackup -n "$CLUSTER_NAMESPACE" "$BACKUP_NAME" > /dev/null 2>&1'
  then
    echo "SUCCESS. The backup retention has been honored. Old backups $BACKUP_NAME deleted"
  else
    echo "FAIL. The backup retention has not been honored. Old backups $BACKUP_NAME not deleted"
    kubectl get sgbackup -n "$CLUSTER_NAMESPACE"
    return 1
  fi

  if wait_until eval '[ "$(get_completed_backup_count)" = "2" ]'
  then
    echo "SUCCESS. The backup retention has been honored. Remaining backups are 2"
  else
    echo "FAIL. The backup retention has not been honored. Remaining backups are $(get_completed_backup_count)"
    return 1
  fi
}

get_completed_backup_count() {
  kubectl get sgbackup -n "$CLUSTER_NAMESPACE" -o custom-columns=PHASE:.status.process.status \
    | grep 'Completed' | wc -l
}

enable_cronSchedule() {
  # Sets the value At every minute.
  kubectl patch sgcluster -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/configurations/backups/0/cronSchedule","value":"*/1 * * * *"}]'
}

disable_cronSchedule() {
  # Sets the value At 05:00 on day-of-month 31 in February.
  kubectl patch sgcluster -n "$CLUSTER_NAMESPACE" "$CLUSTER_NAME" --type json \
    --patch '[{"op":"replace","path":"/spec/configurations/backups/0/cronSchedule","value":"0 5 31 2 *"}]'
}
