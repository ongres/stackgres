{
  "apiVersion": "v1",
  "items": [
    {
      "apiVersion": "v1",
      "data": {
        "MD5SUM": "A0CFC18177E12463346D84C287DD800D",
        "PATRONI_KUBERNETES_LABELS": "{\"app\":\"StackGresCluster\",\"cluster-uid\":\"d23771de-533a-428d-8918-dbd8ab149614\",\"cluster-name\":\"test\",\"cluster\":\"true\"}",
        "PATRONI_KUBERNETES_PORTS": "[{\"protocol\":\"TCP\",\"name\":\"pgport\",\"port\":7432},{\"protocol\":\"TCP\",\"name\":\"pgreplication\",\"port\":7433}]",
        "PATRONI_KUBERNETES_SCOPE_LABEL": "cluster-name",
        "PATRONI_KUBERNETES_USE_ENDPOINTS": "true",
        "PATRONI_POSTGRESQL_BIN_DIR": "/usr/lib/postgresql/12.2/bin",
        "PATRONI_POSTGRESQL_CONNECT_ADDRESS": "${PATRONI_KUBERNETES_POD_IP}:7433",
        "PATRONI_POSTGRESQL_DATA_DIR": "/var/lib/postgresql/data",
        "PATRONI_POSTGRESQL_LISTEN": "127.0.0.1:5432",
        "PATRONI_POSTGRES_UNIX_SOCKET_DIRECTORY": "/var/run/postgresql",
        "PATRONI_REPLICATION_USERNAME": "replicator",
        "PATRONI_RESTAPI_LISTEN": "0.0.0.0:8008",
        "PATRONI_SCOPE": "test",
        "PATRONI_SCRIPTS": "0",
        "PATRONI_SUPERUSER_USERNAME": "postgres"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster": "true",
          "cluster-name": "test",
          "cluster-uid": "d23771de-533a-428d-8918-dbd8ab149614"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:MD5SUM": {},
                "f:PATRONI_KUBERNETES_LABELS": {},
                "f:PATRONI_KUBERNETES_PORTS": {},
                "f:PATRONI_KUBERNETES_SCOPE_LABEL": {},
                "f:PATRONI_KUBERNETES_USE_ENDPOINTS": {},
                "f:PATRONI_POSTGRESQL_BIN_DIR": {},
                "f:PATRONI_POSTGRESQL_CONNECT_ADDRESS": {},
                "f:PATRONI_POSTGRESQL_DATA_DIR": {},
                "f:PATRONI_POSTGRESQL_LISTEN": {},
                "f:PATRONI_POSTGRES_UNIX_SOCKET_DIRECTORY": {},
                "f:PATRONI_REPLICATION_USERNAME": {},
                "f:PATRONI_RESTAPI_LISTEN": {},
                "f:PATRONI_SCOPE": {},
                "f:PATRONI_SCRIPTS": {},
                "f:PATRONI_SUPERUSER_USERNAME": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster": {},
                  "f:cluster-name": {},
                  "f:cluster-uid": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1407",
        "uid": "d5b0844a-0861-40cd-a84b-42d22a4293ea"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "MD5SUM": "D41D8CD98F00B204E9800998ECF8427E"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:MD5SUM": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-backup",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1408",
        "uid": "a8947754-0689-4ec9-a287-df2c18a44a9d"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "pgbouncer.ini": "[databases]\n\n* = port=5432\n\n[pgbouncer]\nadmin_users = pgbouncer_admin\napplication_name_add_host = 1\nauth_file = /etc/pgbouncer/auth/users.txt\nauth_query = SELECT usename, passwd FROM pg_shadow WHERE usename=$1\nauth_type = md5\nauth_user = authenticator\ndefault_pool_size = 1000\nignore_startup_parameters = extra_float_digits\nlisten_addr = 127.0.0.1\nlisten_port = 6432\nmax_client_conn = 1000\nmax_db_connections = 0\nmax_user_connections = 0\npool_mode = session\nstats_users = pgbouncer_stats\nunix_socket_dir = /var/run/postgresql\n"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:pgbouncer.ini": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-connection-pooling-config",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1429",
        "uid": "c953972e-9067-450f-acdf-0896b0dbf858"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "envoy.json": "{\"static_resources\":{\"listeners\":[{\"name\":\"patroni_listener\",\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":8008}},\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.http_connection_manager\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\",\"stat_prefix\":\"ingress_patroni_rest\",\"codec_type\":\"AUTO\",\"route_config\":{\"name\":\"local_route\",\"virtual_hosts\":[{\"name\":\"patroni\",\"domains\":[\"*\"],\"routes\":[{\"match\":{\"prefix\":\"/\"},\"route\":{\"cluster\":\"patroni_cluster\"}}]}]},\"http_filters\":[{\"name\":\"envoy.filters.http.router\",\"typed_config\":{}}]}}]}]},{\"name\":\"postgres_entry_listener\",\"per_connection_buffer_limit_bytes\":1048576,\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":7432}},\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.filters.network.postgres_proxy\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.postgres_proxy.v3alpha.PostgresProxy\",\"stat_prefix\":\"ingress_postgres\",\"enable_sql_parsing\":false,\"terminate_ssl\":false}},{\"name\":\"envoy.tcp_proxy\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\"stat_prefix\":\"ingress_tcp\",\"cluster\":\"postgres_cluster_pool\",\"idle_timeout\":\"0s\"}}]}]},{\"name\":\"postgres_repl_entry_listener\",\"per_connection_buffer_limit_bytes\":1048576,\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":7433}},\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.tcp_proxy\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\"stat_prefix\":\"ingress_raw_tcp\",\"cluster\":\"postgres_cluster\",\"idle_timeout\":\"0s\"}}]}]},{\"name\":\"babelfish_entry_listener\",\"per_connection_buffer_limit_bytes\":1048576,\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":7434}},\"filter_chains\":[{\"filters\":[{\"name\":\"envoy.tcp_proxy\",\"typed_config\":{\"@type\":\"type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\",\"stat_prefix\":\"ingress_tcp\",\"cluster\":\"babelfish_cluster\",\"idle_timeout\":\"0s\"}}]}]}],\"clusters\":[{\"name\":\"patroni_cluster\",\"connect_timeout\":\"1s\",\"load_assignment\":{\"cluster_name\":\"patroni_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"address\":\"127.0.0.1\",\"port_value\":8009}}}}]}]}},{\"name\":\"postgres_cluster_pool\",\"type\":\"STATIC\",\"connect_timeout\":\"1s\",\"per_connection_buffer_limit_bytes\":1048576,\"circuit_breakers\":{\"thresholds\":[{\"max_connections\":65536,\"max_requests\":65536,\"track_remaining\":true}]},\"load_assignment\":{\"cluster_name\":\"postgres_cluster_pool\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"protocol\":\"TCP\",\"address\":\"127.0.0.1\",\"port_value\":6432}}}}]}]}},{\"name\":\"postgres_cluster\",\"type\":\"STATIC\",\"connect_timeout\":\"1s\",\"per_connection_buffer_limit_bytes\":1048576,\"circuit_breakers\":{\"thresholds\":[{\"max_connections\":65536,\"max_requests\":65536,\"track_remaining\":true}]},\"load_assignment\":{\"cluster_name\":\"postgres_cluster\",\"endpoints\":[{\"lb_endpoints\":[{\"endpoint\":{\"address\":{\"socket_address\":{\"protocol\":\"TCP\",\"address\":\"127.0.0.1\",\"port_value\":5432}}}}]}]}}]},\"admin\":{\"access_log_path\":\"/dev/null\",\"address\":{\"socket_address\":{\"address\":\"0.0.0.0\",\"port_value\":8001}}}}"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test",
          "cluster-uid": "d23771de-533a-428d-8918-dbd8ab149614"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:envoy.json": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {},
                  "f:cluster-uid": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-envoy-config",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1422",
        "uid": "7a57df96-79cc-4721-a19f-ecaa02dcf80e"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "00000-prometheus-postgres-exporter-init.postgres.sql": "CREATE EXTENSION IF NOT EXISTS dblink;\nCREATE EXTENSION IF NOT EXISTS plpython3u;\n\nCREATE OR REPLACE FUNCTION df(path text)\nRETURNS SETOF text\nAS\n$$\n  import subprocess\n  try:\n    result = subprocess.run(['df', '-B1', '--output=source,target,fstype,size,avail,itotal,iavail', path], timeout=1, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='UTF-8')\n  except:\n    return ['- ' + path + ' - - - - - timeout']\n  if result.returncode == 0:\n    return result.stdout.split('\\n')[1:2]\n  else:\n    return ['- ' + path + ' - - - - - ' + result.stderr.replace(' ', '\\\\s')]\n$$\nLANGUAGE plpython3u;\n\nCREATE OR REPLACE FUNCTION mounts()\nRETURNS SETOF text\nAS\n$$\n  import subprocess\n  return subprocess.run(['cat', '/proc/mounts'], stdout=subprocess.PIPE, encoding='UTF-8').stdout.split('\\n')\n$$\nLANGUAGE plpython3u;\n"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:00000-prometheus-postgres-exporter-init.postgres.sql": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-internal-00000-prometheus-postgres-exporter-init-postgres",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1415",
        "uid": "1566c850-3f14-4d56-8833-997c09e3a3b8"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "queries.yaml": "pg_replication:\n  master: true\n  query: |\n    select\n    case\n      when pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() then 0\n      else extract (EPOCH FROM now() - pg_last_xact_replay_timestamp())::integer\n    end as lag,\n    case\n      when pg_is_in_recovery() then 1\n      else 0\n    end as is_replica;\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\"\n    - is_replica:\n        usage: \"GAUGE\"\n        description: \"Indicates if this host is a replica\"\n\npg_postmaster:\n  master: true\n  query: \"SELECT pg_postmaster_start_time as start_time_seconds from pg_postmaster_start_time()\"\n  metrics:\n    - start_time_seconds:\n        usage: \"GAUGE\"\n        description: \"Time at which postmaster started\"\n\npg_stat_user_tables:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT schemaname, relname, seq_scan, seq_tup_read, idx_scan, idx_tup_fetch, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd, n_live_tup, n_dead_tup, n_mod_since_analyze, COALESCE(last_vacuum, ''1970-01-01Z'') as last_vacuum, COALESCE(last_autovacuum, ''1970-01-01Z'') as last_autovacuum, COALESCE(last_analyze, ''1970-01-01Z'') as last_analyze, COALESCE(last_autoanalyze, ''1970-01-01Z'') as last_autoanalyze, vacuum_count, autovacuum_count, analyze_count, autoanalyze_count FROM pg_catalog.pg_stat_user_tables')\n        AS (schemaname name, relname name, seq_scan bigint, seq_tup_read bigint, idx_scan bigint, idx_tup_fetch bigint, n_tup_ins bigint, n_tup_upd bigint, n_tup_del bigint, n_tup_hot_upd bigint, n_live_tup bigint, n_dead_tup bigint, n_mod_since_analyze bigint, last_vacuum timestamp with time zone, last_autovacuum timestamp with time zone, last_analyze timestamp with time zone, last_autoanalyze timestamp with time zone, vacuum_count bigint, autovacuum_count bigint, analyze_count bigint, autoanalyze_count bigint)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Name of current database\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that this table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of this table\"\n    - seq_scan:\n        usage: \"COUNTER\"\n        description: \"Number of sequential scans initiated on this table\"\n    - seq_tup_read:\n        usage: \"COUNTER\"\n        description: \"Number of live rows fetched by sequential scans\"\n    - idx_scan:\n        usage: \"COUNTER\"\n        description: \"Number of index scans initiated on this table\"\n    - idx_tup_fetch:\n        usage: \"COUNTER\"\n        description: \"Number of live rows fetched by index scans\"\n    - n_tup_ins:\n        usage: \"COUNTER\"\n        description: \"Number of rows inserted\"\n    - n_tup_upd:\n        usage: \"COUNTER\"\n        description: \"Number of rows updated\"\n    - n_tup_del:\n        usage: \"COUNTER\"\n        description: \"Number of rows deleted\"\n    - n_tup_hot_upd:\n        usage: \"COUNTER\"\n        description: \"Number of rows HOT updated (i.e., with no separate index update required)\"\n    - n_live_tup:\n        usage: \"GAUGE\"\n        description: \"Estimated number of live rows\"\n    - n_dead_tup:\n        usage: \"GAUGE\"\n        description: \"Estimated number of dead rows\"\n    - n_mod_since_analyze:\n        usage: \"GAUGE\"\n        description: \"Estimated number of rows changed since last analyze\"\n    - last_vacuum:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was manually vacuumed (not counting VACUUM FULL)\"\n    - last_autovacuum:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was vacuumed by the autovacuum daemon\"\n    - last_analyze:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was manually analyzed\"\n    - last_autoanalyze:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was analyzed by the autovacuum daemon\"\n    - vacuum_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been manually vacuumed (not counting VACUUM FULL)\"\n    - autovacuum_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been vacuumed by the autovacuum daemon\"\n    - analyze_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been manually analyzed\"\n    - autoanalyze_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been analyzed by the autovacuum daemon\"\n\npg_statio_user_tables:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT schemaname, relname, heap_blks_read, heap_blks_hit, idx_blks_read, idx_blks_hit, toast_blks_read, toast_blks_hit, tidx_blks_read, tidx_blks_hit FROM pg_catalog.pg_statio_user_tables')\n        AS (schemaname name, relname name, heap_blks_read bigint, heap_blks_hit bigint, idx_blks_read bigint, idx_blks_hit bigint, toast_blks_read bigint, toast_blks_hit bigint, tidx_blks_read bigint, tidx_blks_hit bigint)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Name of current database\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that this table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of this table\"\n    - heap_blks_read:\n        usage: \"COUNTER\"\n        description: \"Number of disk blocks read from this table\"\n    - heap_blks_hit:\n        usage: \"COUNTER\"\n        description: \"Number of buffer hits in this table\"\n    - idx_blks_read:\n        usage: \"COUNTER\"\n        description: \"Number of disk blocks read from all indexes on this table\"\n    - idx_blks_hit:\n        usage: \"COUNTER\"\n        description: \"Number of buffer hits in all indexes on this table\"\n    - toast_blks_read:\n        usage: \"COUNTER\"\n        description: \"Number of disk blocks read from this table's TOAST table (if any)\"\n    - toast_blks_hit:\n        usage: \"COUNTER\"\n        description: \"Number of buffer hits in this table's TOAST table (if any)\"\n    - tidx_blks_read:\n        usage: \"COUNTER\"\n        description: \"Number of disk blocks read from this table's TOAST table indexes (if any)\"\n    - tidx_blks_hit:\n        usage: \"COUNTER\"\n        description: \"Number of buffer hits in this table's TOAST table indexes (if any)\"\n\npg_database:\n  master: true\n  query: \"SELECT pg_database.datname, pg_database_size(pg_database.datname) as size FROM pg_catalog.pg_database\"\n  cache_seconds: 30\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Name of the database\"\n    - size_bytes:\n        usage: \"GAUGE\"\n        description: \"Disk space used by the database\"\n\npg_archiver:\n  master: true\n  query: |\n    WITH\n      current_wal_file AS (\n         SELECT CASE WHEN NOT pg_is_in_recovery() THEN pg_walfile_name(pg_last_wal_replay_lsn()) ELSE NULL END pg_walfile_name\n      ),\n      current_wal AS (\n        SELECT\n          ('x'||substring(pg_walfile_name,9,8))::bit(32)::int log,\n          ('x'||substring(pg_walfile_name,17,8))::bit(32)::int seg,\n          pg_walfile_name\n        FROM current_wal_file\n      ),\n      archive_wal AS(\n        SELECT\n          ('x'||substring(last_archived_wal,9,8))::bit(32)::int log,\n          ('x'||substring(last_archived_wal,17,8))::bit(32)::int seg,\n          last_archived_wal\n        FROM pg_catalog.pg_stat_archiver\n      )\n    SELECT coalesce(((cw.log - aw.log) * 256) + (cw.seg-aw.seg),'NaN'::float) as pending_wal_count FROM current_wal cw, archive_wal aw \n  metrics:\n    - pending_wal_count:\n        usage: \"GAUGE\"\n        description: \"No. of pending WAL files to be archived\"\n\npg_stat_user_indexes:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT schemaname, relname, indexrelname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_catalog.pg_stat_user_indexes')\n        AS (schemaname name, relname name, indexrelname name, idx_scan bigint, idx_tup_read bigint, idx_tup_fetch bigint)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that this table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of the table for this index\"\n    - indexrelname:\n        usage: \"LABEL\"\n        description: \"Name of this index\"\n    - idx_scan:\n        usage: \"COUNTER\"\n        description: \"Number of index scans initiated on this index\"\n    - idx_tup_read:\n        usage: \"COUNTER\"\n        description: \"Number of index entries returned by scans on this index\"\n    - idx_tup_fetch:\n        usage: \"COUNTER\"\n        description: \"Number of live table rows fetched by simple index scans using this index\"\n\npg_statio_user_indexes:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT schemaname, relname, indexrelname, idx_blks_read, idx_blks_hit FROM pg_catalog.pg_statio_user_indexes')\n        AS (schemaname name, relname name, indexrelname name, idx_blks_read bigint, idx_blks_hit bigint)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that this table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of the table for this index\"\n    - indexrelname:\n        usage: \"LABEL\"\n        description: \"Name of this index\"\n    - idx_blks_read:\n        usage: \"COUNTER\"\n        description: \"Number of disk blocks read from this index\"\n    - idx_blks_hit:\n        usage: \"COUNTER\"\n        description: \"Number of buffer hits in this index\"\n\npg_total_relation_size:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT relnamespace::regnamespace as schemaname, relname as relname, pg_total_relation_size(oid) bytes FROM pg_catalog.pg_class WHERE relkind = ''r''')\n        AS (schemaname name, relname name, bytes bigint)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of this table\"\n    - bytes:\n        usage: \"GAUGE\"\n        description: \"Total disk space usage for the specified table and associated indexes\"\n\npg_blocked:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT oid, datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT\n      locktype AS type,\n      NULL AS datname,\n      NULL AS schemaname,\n      NULL AS reltype,\n      NULL AS relname,\n      count(*) AS queries\n    FROM pg_catalog.pg_locks blocked\n    WHERE NOT blocked.granted AND relation IS NULL\n    GROUP BY locktype\n    UNION\n    SELECT\n      locktype AS type,\n      datname,\n      schemaname,\n      CASE relkind\n        WHEN 'r' THEN 'ordinary table'\n        WHEN 'i' THEN 'index'\n        WHEN 'S' THEN 'sequence'\n        WHEN 't' THEN 'TOAST table'\n        WHEN 'v' THEN 'view'\n        WHEN 'm' THEN 'materialized view'\n        WHEN 'c' THEN 'composite type'\n        WHEN 'f' THEN 'foreign table'\n        WHEN 'p' THEN 'partitioned table'\n        WHEN 'I' THEN 'partitioned index'\n        ELSE 'unknown type ''' || relkind || ''''\n        END AS reltype,\n      relname,\n      count(*) AS queries\n    FROM pg_catalog.pg_locks blocked\n    INNER JOIN databases\n      ON blocked.database = databases.oid,\n      LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        'SELECT nspname as schemaname, relkind, relname FROM pg_catalog.pg_class LEFT JOIN pg_catalog.pg_namespace ON (pg_namespace.oid = relnamespace) WHERE pg_class.oid = ' || blocked.relation)\n        AS (schemaname name, relkind char, relname name)) AS ss\n    WHERE NOT blocked.granted AND relation IS NOT NULL\n    GROUP BY locktype, datname, schemaname, reltype, relname\n  metrics:\n    - type:\n        usage: \"LABEL\"\n        description: \"The lock type\"\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"The schema on which a query is blocked\"\n    - reltype:\n        usage: \"LABEL\"\n        description: \"The type of relation\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"The relation on which a query is blocked\"\n    - queries:\n        usage: \"GAUGE\"\n        description: \"The current number of blocked queries\"\n\npg_oldest_blocked:\n  master: true\n  query: |\n    SELECT datname,\n      coalesce(extract('epoch' from max(clock_timestamp() - state_change)), 0) age_seconds\n    FROM pg_catalog.pg_stat_activity\n    WHERE wait_event_type = 'Lock'\n    AND state='active'\n    GROUP BY datname\n  metrics:\n    - age_seconds:\n        usage: \"GAUGE\"\n        description: \"Largest number of seconds any transaction is currently waiting on a lock\"\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n\npg_slow:\n  master: true\n  query: |\n    SELECT datname, COUNT(*) AS queries\n    FROM pg_catalog.pg_stat_activity\n    WHERE state = 'active' AND (now() - query_start) \u003e '1 seconds'::interval\n    GROUP BY datname;\n  metrics:\n    - queries:\n        usage: \"GAUGE\"\n        description: \"Current number of slow queries\"\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n\npg_long_running_transactions:\n  master: true\n  query: |\n    SELECT datname, COUNT(*) as transactions,\n    MAX(EXTRACT(EPOCH FROM (clock_timestamp() - xact_start))) AS age_in_seconds\n    FROM pg_catalog.pg_stat_activity\n    WHERE state is distinct from 'idle' AND (now() - xact_start) \u003e '1 minutes'::interval AND query not like '%VACUUM%'\n    GROUP BY datname\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - queries:\n        usage: \"GAUGE\"\n        description: \"Current number of long running transactions\"\n    - age_in_seconds:\n        usage: \"GAUGE\"\n        description: \"The current maximum transaction age in seconds\"\n\npg_vacuum:\n  master: true\n  query: |\n    SELECT\n      datname,\n      COUNT(*) AS queries,\n      MAX(EXTRACT(EPOCH FROM (clock_timestamp() - query_start))) AS age_in_seconds\n    FROM pg_catalog.pg_stat_activity\n    WHERE state = 'active' AND trim(query) ~* '\\AVACUUM (?!ANALYZE)'\n    GROUP BY datname\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - queries:\n        usage: \"GAUGE\"\n        description: \"The current number of VACUUM queries\"\n    - age_in_seconds:\n        usage: \"GAUGE\"\n        description: \"The current maximum VACUUM query age in seconds\"\n\npg_vacuum_analyze:\n  master: true\n  query: |\n    SELECT\n      datname,\n      COUNT(*) AS queries,\n      MAX(EXTRACT(EPOCH FROM (clock_timestamp() - query_start))) AS age_in_seconds\n    FROM pg_catalog.pg_stat_activity\n    WHERE state = 'active' AND trim(query) ~* '\\AVACUUM ANALYZE'\n    GROUP BY datname\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - queries:\n        usage: \"GAUGE\"\n        description: \"The current number of VACUUM ANALYZE queries\"\n    - age_in_seconds:\n        usage: \"GAUGE\"\n        description: \"The current maximum VACUUM ANALYZE query age in seconds\"\n\npg_stuck_idle_in_transaction:\n  master: true\n  query: |\n    SELECT datname,\n      COUNT(*) AS queries\n    FROM pg_catalog.pg_stat_activity\n    WHERE state = 'idle in transaction' AND (now() - query_start) \u003e '10 minutes'::interval\n    GROUP BY datname\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - queries:\n        usage: \"GAUGE\"\n        description: \"Current number of queries that are stuck being idle in transactions\"\n\npg_txid:\n  master: true\n  query: |\n    SELECT\n      CASE WHEN pg_is_in_recovery() THEN 'NaN'::float ELSE txid_current() % (2^52)::bigint END AS current,\n      CASE WHEN pg_is_in_recovery() THEN 'NaN'::float ELSE txid_snapshot_xmin(txid_current_snapshot()) % (2^52)::bigint END AS xmin,\n      CASE WHEN pg_is_in_recovery() THEN 'NaN'::float ELSE txid_current() - txid_snapshot_xmin(txid_current_snapshot()) END AS xmin_age\n  metrics:\n    - current:\n        usage: \"COUNTER\"\n        description: \"Current 64-bit transaction id of the query used to collect this metric (truncated to low 52 bits)\"\n    - xmin:\n        usage: \"COUNTER\"\n        description: \"Oldest transaction id of a transaction still in progress, i.e. not known committed or aborted (truncated to low 52 bits)\"\n    - xmin_age:\n        usage: \"GAUGE\"\n        description: \"Age of oldest transaction still not committed or aborted measured in transaction ids\"\n\npg_database_datfrozenxid:\n  master: true\n  query: \"SELECT datname, age(datfrozenxid) AS age FROM pg_catalog.pg_database\"\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database Name\"\n    - age:\n        usage: \"GAUGE\"\n        description: \"Age of the oldest transaction that has not been frozen.\"\n\npg_wal_position:\n  master: true\n  query: |\n    SELECT CASE\n           WHEN pg_is_in_recovery()\n           THEN (pg_last_wal_receive_lsn() - '0/0') % (2^52)::bigint\n           ELSE (pg_current_wal_lsn() - '0/0') % (2^52)::bigint\n           END AS bytes\n  metrics:\n    - bytes:\n        usage: \"COUNTER\"\n        description: \"Postgres LSN (log sequence number) being generated on primary or replayed on replica (truncated to low 52 bits)\"\n\n\npg_replication_slots:\n  master: true\n  query: |\n    SELECT slot_name, slot_type,\n           case when active then 1.0 else 0.0 end AS active,\n           age(xmin) AS xmin_age,\n           age(catalog_xmin) AS catalog_xmin_age,\n           CASE WHEN pg_is_in_recovery() THEN pg_last_wal_receive_lsn() ELSE pg_current_wal_lsn() END - restart_lsn AS restart_lsn_bytes,\n           CASE WHEN pg_is_in_recovery() THEN pg_last_wal_receive_lsn() ELSE pg_current_wal_lsn() END - confirmed_flush_lsn AS confirmed_flush_lsn_bytes\n      FROM pg_catalog.pg_replication_slots\n  metrics:\n    - slot_name:\n        usage: \"LABEL\"\n        description: \"Slot Name\"\n    - slot_type:\n        usage: \"LABEL\"\n        description: \"Slot Type\"\n    - active:\n        usage: \"GAUGE\"\n        description: \"Boolean flag indicating whether this slot has a consumer streaming from it\"\n    - xmin_age:\n        usage: \"GAUGE\"\n        description: \"Age of oldest transaction that cannot be vacuumed due to this replica\"\n    - catalog_xmin_age:\n        usage: \"GAUGE\"\n        description: \"Age of oldest transaction that cannot be vacuumed from catalogs due to this replica (used by logical replication)\"\n    - restart_lsn_bytes:\n        usage: \"GAUGE\"\n        description: \"Amount of data on in xlog that must be this replica may need to complete recovery\"\n    - confirmed_flush_lsn_bytes:\n        usage: \"GAUGE\"\n        description: \"Amount of data on in xlog that must be this replica has not yet received\"\n\npg_stat_ssl:\n  master: true\n  query: |\n    SELECT pid, bits,\n           CASE WHEN ssl THEN 1.0 ELSE 0.0 END AS active,\n           CASE WHEN compression THEN 1.0 ELSE 0.0 END AS compression\n           FROM pg_catalog.pg_stat_ssl\n  metrics:\n    - pid:\n        usage: \"LABEL\"\n        description: \"Process ID of a backend or WAL sender process\"\n    - active:\n        usage: \"GAUGE\"\n        description: \"Boolean flag indicating if SSL is used on this connection\"\n    - bits:\n        usage: \"GAUGE\"\n        description: \"Number of bits in the encryption algorithm is in use\"\n    - compression:\n        usage: \"GAUGE\"\n        description: \"Boolean flag indicating if SSL compression is in use\"\n\npg_table_bloat:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n    LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        '-- https://github.com/ioguix/pgsql-bloat-estimation' || E'\\n'\n        || ' SELECT schemaname, tablename, bs*tblpages AS real_size,' || E'\\n'\n        || ' (tblpages-est_tblpages)*bs AS extra_size,' || E'\\n'\n        || ' CASE WHEN tblpages - est_tblpages \u003e 0' || E'\\n'\n        || '     THEN 100 * (tblpages - est_tblpages)/tblpages::float' || E'\\n'\n        || '     ELSE 0' || E'\\n'\n        || ' END AS extra_ratio, fillfactor,' || E'\\n'\n        || ' CASE WHEN tblpages - est_tblpages_ff \u003e 0' || E'\\n'\n        || '     THEN (tblpages-est_tblpages_ff)*bs' || E'\\n'\n        || '     ELSE 0' || E'\\n'\n        || ' END AS bloat_size,' || E'\\n'\n        || ' CASE WHEN tblpages - est_tblpages_ff \u003e 0' || E'\\n'\n        || '     THEN 100 * (tblpages - est_tblpages_ff)/tblpages::float' || E'\\n'\n        || '     ELSE 0' || E'\\n'\n        || ' END AS bloat_ratio, is_na' || E'\\n'\n        || ' FROM (' || E'\\n'\n        || ' SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages,' || E'\\n'\n        || '     ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff,' || E'\\n'\n        || '     tblpages, fillfactor, bs, tblid, schemaname, tablename, heappages, toastpages, is_na' || E'\\n'\n        || ' FROM (' || E'\\n'\n        || '     SELECT' || E'\\n'\n        || '     ( 4 + tpl_hdr_size + tpl_data_size + (2*ma)' || E'\\n'\n        || '         - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END' || E'\\n'\n        || '         - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END' || E'\\n'\n        || '     ) AS tpl_size, bs - page_hdr AS size_per_block, (heappages + toastpages) AS tblpages, heappages,' || E'\\n'\n        || '     toastpages, reltuples, toasttuples, bs, page_hdr, tblid, schemaname, tablename, fillfactor, is_na' || E'\\n'\n        || '     FROM (' || E'\\n'\n        || '     SELECT' || E'\\n'\n        || '         tbl.oid AS tblid, ns.nspname AS schemaname, tbl.relname AS tablename, tbl.reltuples,' || E'\\n'\n        || '         tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages,' || E'\\n'\n        || '         coalesce(toast.reltuples, 0) AS toasttuples,' || E'\\n'\n        || '         coalesce(substring(' || E'\\n'\n        || '         array_to_string(tbl.reloptions, '' '')' || E'\\n'\n        || '         FROM ''fillfactor=([0-9]+)'')::smallint, 100) AS fillfactor,' || E'\\n'\n        || '         current_setting(''block_size'')::numeric AS bs,' || E'\\n'\n        || '         CASE WHEN version()~''mingw32'' OR version()~''64-bit|x86_64|ppc64|ia64|amd64'' THEN 8 ELSE 4 END AS ma,' || E'\\n'\n        || '         24 AS page_hdr,' || E'\\n'\n        || '         23 + CASE WHEN MAX(coalesce(s.null_frac,0)) \u003e 0 THEN ( 7 + count(s.attname) ) / 8 ELSE 0::int END' || E'\\n'\n        || '         + CASE WHEN bool_or(att.attname = ''oid'' and att.attnum \u003c 0) THEN 4 ELSE 0 END AS tpl_hdr_size,' || E'\\n'\n        || '         sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 0) ) AS tpl_data_size,' || E'\\n'\n        || '         bool_or(att.atttypid = ''pg_catalog.name''::regtype)' || E'\\n'\n        || '         OR sum(CASE WHEN att.attnum \u003e 0 THEN 1 ELSE 0 END) \u003c\u003e count(s.attname) AS is_na' || E'\\n'\n        || '     FROM pg_attribute AS att' || E'\\n'\n        || '         JOIN pg_class AS tbl ON att.attrelid = tbl.oid' || E'\\n'\n        || '         JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace' || E'\\n'\n        || '         LEFT JOIN pg_stats AS s ON s.schemaname=ns.nspname' || E'\\n'\n        || '         AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname' || E'\\n'\n        || '         LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid' || E'\\n'\n        || '     WHERE NOT att.attisdropped' || E'\\n'\n        || '         AND tbl.relkind in (''r'',''m'')' || E'\\n'\n        || '     GROUP BY 1,2,3,4,5,6,7,8,9,10' || E'\\n'\n        || '     ORDER BY 2,3' || E'\\n'\n        || '     ) AS s' || E'\\n'\n        || ' ) AS s2' || E'\\n'\n        || ' ) AS s3' || E'\\n'\n        || ' where schemaname not in (''information_schema'',''pg_catalog'')' || E'\\n'\n        || ' ORDER BY schemaname, tablename' || E'\\n')\n       As (schemaname character varying, tablename character varying, real_size numeric, extra_size numeric, extra_ratio numeric, fillfactor numeric, bloat_size numeric, bloat_ratio numeric, is_na boolean)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database Name\"\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Schema Name\"\n    - tablename:\n        usage: \"LABEL\"\n        description: \"Table Name\"\n    - real_size:\n        usage: \"GAUGE\"\n        description: \"Table real size\"\n    - extra_size:\n        usage: \"GAUGE\"\n        description: \"Estimated extra size not used/needed in the table. This extra size is composed by the fillfactor, bloat and alignment padding spaces\"\n    - extra_ratio:\n        usage: \"GAUGE\"\n        description: \"Estimated ratio of the real size used by extra_size\"\n    - fillfactor:\n        usage: \"GAUGE\"\n        description: \"Table fillfactor\"\n    - bloat_size:\n        usage: \"GAUGE\"\n        description: \"Estimated size of the bloat without the extra space kept for the fillfactor.\"\n    - bloat_ratio:\n        usage: \"GAUGE\"\n        description: \"Estimated ratio of the real size used by bloat_size\"\n    - is_na:\n        usage: \"GAUGE\"\n        description: \"Estimation not aplicable, If true, do not trust the stats\"\n\npg_index:\n  master: true\n  query: |\n    SET max_parallel_workers_per_gather = 0;\n    WITH databases AS (\n      SELECT datname FROM pg_database\n      WHERE datname NOT IN ('template0', 'template1')\n    )\n    SELECT datname, ss.* FROM databases,\n    LATERAL (SELECT * FROM dblink(\n        'host=/var/run/postgresql port=5432 user=postgres sslmode=disable dbname=''' || regexp_replace(datname, '([.\\\\])', '\\\\\\1', 'g') || '''',\n        '-- https://github.com/ioguix/pgsql-bloat-estimation' || E'\\n'\n      || ' SELECT nspname AS schema_name, tblname, idxname, bs*(relpages)::bigint AS real_size,' || E'\\n'\n      || '  bs*(relpages-est_pages)::bigint AS extra_size,' || E'\\n'\n      || '  100 * (relpages-est_pages)::float / relpages AS extra_ratio,' || E'\\n'\n      || '  fillfactor,' || E'\\n'\n      || '  CASE WHEN relpages \u003e est_pages_ff' || E'\\n'\n      || '  THEN bs*(relpages-est_pages_ff)' || E'\\n'\n      || '  ELSE 0' || E'\\n'\n      || '  END AS bloat_size,' || E'\\n'\n      || '  100 * (relpages-est_pages_ff)::float / relpages AS bloat_ratio,' || E'\\n'\n      || '  is_na' || E'\\n'\n      || '  FROM (' || E'\\n'\n      || '  SELECT coalesce(1 +' || E'\\n'\n      || '          ceil(reltuples/floor((bs-pageopqdata-pagehdr)/(4+nulldatahdrwidth)::float)), 0' || E'\\n'\n      || '      ) AS est_pages,' || E'\\n'\n      || '      coalesce(1 +' || E'\\n'\n      || '          ceil(reltuples/floor((bs-pageopqdata-pagehdr)*fillfactor/(100*(4+nulldatahdrwidth)::float))), 0' || E'\\n'\n      || '      ) AS est_pages_ff,' || E'\\n'\n      || '      bs, nspname, tblname, idxname, relpages, fillfactor, is_na' || E'\\n'\n      || '  FROM (' || E'\\n'\n      || '      SELECT maxalign, bs, nspname, tblname, idxname, reltuples, relpages, idxoid, fillfactor,' || E'\\n'\n      || '          ( index_tuple_hdr_bm +' || E'\\n'\n      || '              maxalign - CASE -- Add padding to the index tuple header to align on MAXALIGN' || E'\\n'\n      || '                  WHEN index_tuple_hdr_bm%maxalign = 0 THEN maxalign' || E'\\n'\n      || '                  ELSE index_tuple_hdr_bm%maxalign' || E'\\n'\n      || '              END' || E'\\n'\n      || '              + nulldatawidth + maxalign - CASE -- Add padding to the data to align on MAXALIGN' || E'\\n'\n      || '                  WHEN nulldatawidth = 0 THEN 0' || E'\\n'\n      || '                  WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign' || E'\\n'\n      || '                  ELSE nulldatawidth::integer%maxalign' || E'\\n'\n      || '              END' || E'\\n'\n      || '          )::numeric AS nulldatahdrwidth, pagehdr, pageopqdata, is_na' || E'\\n'\n      || '      FROM (' || E'\\n'\n      || '          SELECT n.nspname, i.tblname, i.idxname, i.reltuples, i.relpages,' || E'\\n'\n      || '              i.idxoid, i.fillfactor, current_setting(''block_size'')::numeric AS bs,' || E'\\n'\n      || '              CASE' || E'\\n'\n      || '              WHEN version() ~ ''mingw32'' OR version() ~ ''64-bit|x86_64|ppc64|ia64|amd64'' THEN 8' || E'\\n'\n      || '              ELSE 4' || E'\\n'\n      || '              END AS maxalign,' || E'\\n'\n      || '              24 AS pagehdr,' || E'\\n'\n      || '              16 AS pageopqdata,' || E'\\n'\n      || '              CASE WHEN max(coalesce(s.null_frac,0)) = 0' || E'\\n'\n      || '                  THEN 2 -- IndexTupleData size' || E'\\n'\n      || '                  ELSE 2 + (( 32 + 8 - 1 ) / 8)' || E'\\n'\n      || '              END AS index_tuple_hdr_bm,' || E'\\n'\n      || '              sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 1024)) AS nulldatawidth,' || E'\\n'\n      || '              max( CASE WHEN i.atttypid = ''pg_catalog.name''::regtype THEN 1 ELSE 0 END ) \u003e 0 AS is_na' || E'\\n'\n      || '          FROM (' || E'\\n'\n      || '              SELECT ct.relname AS tblname, ct.relnamespace, ic.idxname, ic.attpos, ic.indkey, ic.indkey[ic.attpos], ic.reltuples, ic.relpages, ic.tbloid, ic.idxoid, ic.fillfactor,' || E'\\n'\n      || '                  coalesce(a1.attnum, a2.attnum) AS attnum, coalesce(a1.attname, a2.attname) AS attname, coalesce(a1.atttypid, a2.atttypid) AS atttypid,' || E'\\n'\n      || '                  CASE WHEN a1.attnum IS NULL' || E'\\n'\n      || '                  THEN ic.idxname' || E'\\n'\n      || '                  ELSE ct.relname' || E'\\n'\n      || '                  END AS attrelname' || E'\\n'\n      || '              FROM (' || E'\\n'\n      || '                  SELECT idxname, reltuples, relpages, tbloid, idxoid, fillfactor, indkey,' || E'\\n'\n      || '                      pg_catalog.generate_series(1,indnatts) AS attpos' || E'\\n'\n      || '                  FROM (' || E'\\n'\n      || '                      SELECT ci.relname AS idxname, ci.reltuples, ci.relpages, i.indrelid AS tbloid,' || E'\\n'\n      || '                          i.indexrelid AS idxoid,' || E'\\n'\n      || '                          coalesce(substring(' || E'\\n'\n      || '                              array_to_string(ci.reloptions, '' '')' || E'\\n'\n      || '                              from ''fillfactor=([0-9]+)'')::smallint, 90) AS fillfactor,' || E'\\n'\n      || '                          i.indnatts,' || E'\\n'\n      || '                          pg_catalog.string_to_array(pg_catalog.textin(' || E'\\n'\n      || '                              pg_catalog.int2vectorout(i.indkey)),'' '')::int[] AS indkey' || E'\\n'\n      || '                      FROM pg_catalog.pg_index i' || E'\\n'\n      || '                      JOIN pg_catalog.pg_class ci ON ci.oid = i.indexrelid' || E'\\n'\n      || '                      WHERE ci.relam=(SELECT oid FROM pg_am WHERE amname = ''btree'')' || E'\\n'\n      || '                      AND ci.relpages \u003e 0' || E'\\n'\n      || '                  ) AS idx_data' || E'\\n'\n      || '              ) AS ic' || E'\\n'\n      || '              JOIN pg_catalog.pg_class ct ON ct.oid = ic.tbloid' || E'\\n'\n      || '              LEFT JOIN pg_catalog.pg_attribute a1 ON' || E'\\n'\n      || '                  ic.indkey[ic.attpos] \u003c\u003e 0' || E'\\n'\n      || '                  AND a1.attrelid = ic.tbloid' || E'\\n'\n      || '                  AND a1.attnum = ic.indkey[ic.attpos]' || E'\\n'\n      || '              LEFT JOIN pg_catalog.pg_attribute a2 ON' || E'\\n'\n      || '                  ic.indkey[ic.attpos] = 0' || E'\\n'\n      || '                  AND a2.attrelid = ic.idxoid' || E'\\n'\n      || '                  AND a2.attnum = ic.attpos' || E'\\n'\n      || '          ) i' || E'\\n'\n      || '          JOIN pg_catalog.pg_namespace n ON n.oid = i.relnamespace' || E'\\n'\n      || '          JOIN pg_catalog.pg_stats s ON s.schemaname = n.nspname' || E'\\n'\n      || '                                      AND s.tablename = i.attrelname' || E'\\n'\n      || '                                      AND s.attname = i.attname' || E'\\n'\n      || '          GROUP BY 1,2,3,4,5,6,7,8,9,10,11' || E'\\n'\n      || '      ) AS rows_data_stats' || E'\\n'\n      || '  ) AS rows_hdr_pdg_stats' || E'\\n'\n      || '  ) AS relation_stats' || E'\\n'\n      || '  WHERE nspname != ''pg_catalog'' ' || E'\\n'\n      || '  ORDER BY nspname, tblname, idxname;' || E'\\n')\n      As (schema_name character varying, tblname character varying, idxname character varying, real_size numeric, extra_size numeric, extra_ratio numeric, fillfactor numeric, bloat_size numeric, bloat_ratio numeric, is_na boolean)) AS ss;\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Database Name\"\n    - schema_name:\n        usage: \"LABEL\"\n        description: \"Schema Name\"\n    - tblname:\n        usage: \"LABEL\"\n        description: \"Table Name\"\n    - idxname:\n        usage: \"LABEL\"\n        description: \"Index Name\"\n    - real_size:\n        usage: \"GAUGE\"\n        description: \"Index size\"\n    - extra_size:\n        usage: \"GAUGE\"\n        description: \"Index extra size\"\n    - extra_ratio:\n        usage: \"GAUGE\"\n        description: \"Index extra size ratio\"\n    - fillfactor:\n        usage: \"GAUGE\"\n        description: \"Fillfactor\"\n    - bloat_size:\n        usage: \"GAUGE\"\n        description: \"Estimate index bloat size\"\n    - bloat_ratio:\n        usage: \"GAUGE\"\n        description: \"Estimate index bloat size ratio\"\n    - is_na:\n        usage: \"GAUGE\"\n        description: \"Estimate Not aplicable, bad statistic\"\n\npg_replication_status:\n  master: true\n  query: |\n    select application_name,\n    client_addr,\n    state,\n    pg_wal_lsn_diff(pg_stat_replication.sent_lsn, pg_stat_replication.replay_lsn) AS lag_size\n    FROM pg_stat_replication;\n  metrics:\n    - application_name:\n        usage: \"LABEL\"\n        description: \"Application or node name\"\n    - client_addr:\n        usage: \"LABEL\"\n        description: \"Client ip address\"\n    - state:\n        usage: \"LABEL\"\n        description: \"Client replication state\"\n    - lag_size_bytes:\n        usage: \"GAUGE\"\n        description: \"Replication lag size in bytes\"\n\n# PGBOUNCER QUERIES\n\npgbouncer_show_clients:\n  master: true\n  query: |\n    SELECT _.type,\n    _.\"user\",\n    _.database,\n    _.state,\n    _.addr,\n    _.port,\n    _.local_addr,\n    _.local_port,\n    _.connect_time,\n    _.request_time,\n    _.wait,\n    _.wait_us,\n    _.close_needed,\n    _.ptr,\n    _.link,\n    _.remote_pid,\n    _.tls\n    FROM dblink('host=/var/run/postgresql port=6432 dbname=pgbouncer user=pgbouncer', 'show clients'::text)\n    _(type text, \"user\" text, database text, state text, addr text, port integer, local_addr text, local_port integer,\n    connect_time timestamp with time zone, request_time timestamp with time zone, wait integer, wait_us integer, close_needed integer,\n    ptr text, link text, remote_pid integer, tls text);\n  metrics:\n    - type:\n        usage: \"LABEL\"\n        description: \"C, for client.\"\n    - user:\n        usage: \"LABEL\"\n        description: \"Client connected user\"\n    - database:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - state:\n        usage: \"LABEL\"\n        description: \"State of the client connection, one of active or waiting\"\n    - addr:\n        usage: \"LABEL\"\n        description: \"IP address of client\"\n    - port:\n        usage: \"GAUGE\"\n        description: \"Port client is connected to\"\n    - local_addr:\n        usage: \"LABEL\"\n        description: \"Connection end address on local machine\"\n    - local_port:\n        usage: \"GAUGE\"\n        description: \"Connection end port on local machine\"\n    - connect_time:\n        usage: \"LABEL\"\n        description: \"Timestamp of connect time\"\n    - request_time:\n        usage: \"LABEL\"\n        description: \"Timestamp of latest client request\"\n    - wait:\n        usage: \"GAUGE\"\n        description: \"Current waiting time in seconds\"\n    - wait_us:\n        usage: \"GAUGE\"\n        description: \"Microsecond part of the current waiting time\"\n    - close_needed:\n        usage: \"GAUGE\"\n        description: \"not used for clients\"\n    - ptr:\n        usage: \"LABEL\"\n        description: \"Address of internal object for this connection. Used as unique ID\"\n    - link:\n        usage: \"LABEL\"\n        description: \"Address of server connection the client is paired with\"\n    - remote_pid:\n        usage: \"GAUGE\"\n        description: \"Process ID, in case client connects over Unix socket and OS supports getting it\"\n    - tls:\n        usage: \"LABEL\"\n        description: \"A string with TLS connection information, or empty if not using TLS\"\n\npgbouncer_show_pools:\n  master: true\n  query: |\n    SELECT _.database,\n    _.\"user\",\n    _.cl_active,\n    _.cl_waiting,\n    _.sv_active,\n    _.sv_idle,\n    _.sv_used,\n    _.sv_tested,\n    _.sv_login,\n    _.maxwait,\n    _.maxwait_us,\n    _.pool_mode\n    FROM dblink('host=/var/run/postgresql port=6432 dbname=pgbouncer user=pgbouncer', 'show pools'::text)\n    _(database text, \"user\" text, cl_active integer, cl_waiting integer, sv_active integer, sv_idle integer,\n    sv_used integer, sv_tested integer, sv_login integer, maxwait integer, maxwait_us integer, pool_mode text);\n  metrics:\n    - database:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - user:\n        usage: \"LABEL\"\n        description: \"User name\"\n    - cl_active:\n        usage: \"GAUGE\"\n        description: \"Client connections that are linked to server connection and can process queries\"\n    - cl_waiting:\n        usage: \"GAUGE\"\n        description: \"Client connections that have sent queries but have not yet got a server connection\"\n    - sv_active:\n        usage: \"GAUGE\"\n        description: \"Server connections that are linked to a client\"\n    - sv_idle:\n        usage: \"GAUGE\"\n        description: \"Server connections that are unused and immediately usable for client queries\"\n    - sv_used:\n        usage: \"GAUGE\"\n        description: \"Server connections that have been idle for more than server_check_delay so they need server_check_query to run on them\"\n    - sv_tested:\n        usage: \"GAUGE\"\n        description: \"Server connections that are currently running either server_reset_query or server_check_query\"\n    - sv_login:\n        usage: \"GAUGE\"\n        description: \"Server connections currently in the process of logging in\"\n    - maxwait:\n        usage: \"GAUGE\"\n        description: \"How long the first oldest client in the queue has waited, in seconds\"\n    - maxwait_us:\n        usage: \"GAUGE\"\n        description: \"Microsecond part of the maximum waiting time\"\n    - pool_mode:\n        usage: \"LABEL\"\n        description: \"The pooling mode in use\"\n\npgbouncer_show_databases:\n  master: true\n  query: |\n    select _.name,\n    _.host,\n    _.port,\n    _.database,\n    _.force_user,\n    _.pool_size,\n    _.reserve_pool,\n    _.pool_mode,\n    _.max_connections,\n    _.current_connections,\n    _.paused,\n    _.disabled\n    FROM dblink('host=/var/run/postgresql port=6432 dbname=pgbouncer user=pgbouncer', 'show databases'::text)\n    _(name text, host text, port integer, database text, force_user text, pool_size integer, reserve_pool integer,\n    pool_mode text, max_connections integer, current_connections integer, paused boolean, disabled boolean);\n  metrics:\n    - name:\n        usage: \"LABEL\"\n        description: \"Name of configured database entry\"\n    - host:\n        usage: \"LABEL\"\n        description: \"Host pgbouncer connects to\"\n    - port:\n        usage: \"GAUGE\"\n        description: \"Port pgbouncer connects to\"\n    - database:\n        usage: \"LABEL\"\n        description: \"Actual database name pgbouncer connects to.\"\n    - force_user:\n        usage: \"LABEL\"\n        description: \"When the user is part of the connection string the connection between pgbouncer and PostgreSQL is forced to the given user\"\n    - pool_size:\n        usage: \"GAUGE\"\n        description: \"Maximum number of server connections\"\n    - reserve_pool:\n        usage: \"GAUGE\"\n        description: \"Maximum number of additional connections for this database\"\n    - pool_mode:\n        usage: \"LABEL\"\n        description: \"The database override pool_mode\"\n    - max_connections:\n        usage: \"GAUGE\"\n        description: \"Maximum number of allowed connections for this database\"\n    - current_connections:\n        usage: \"GAUGE\"\n        description: \"Current number of connections for this database\"\n    - paused:\n        usage: \"GAUGE\"\n        description: \"1 if this database is currently paused, else 0\"\n    - disabled:\n        usage: \"GAUGE\"\n        description: \"1 if this database is currently paused, else 0\"\n\npgbouncer_show_stats_totals:\n  master: true\n  query: |\n    select _.database,\n    _.xact_count,\n    _.query_count,\n    _.bytes_received,\n    _.bytes_sent,\n    _.xact_time,\n    _.query_time,\n    _.wait_time\n    FROM dblink('host=/var/run/postgresql port=6432 dbname=pgbouncer user=pgbouncer', 'show stats_totals'::text)\n    _(database text, xact_count bigint, query_count bigint, bytes_received bigint, bytes_sent bigint, xact_time bigint,\n    query_time bigint, wait_time bigint);\n  metrics:\n    - database:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - xact_count:\n        usage: \"GAUGE\"\n        description: \"Number of SQL transactions pooled\"\n    - query_count:\n        usage: \"GAUGE\"\n        description: \"Number of SQL queries pooled\"\n    - bytes_received:\n        usage: \"GAUGE\"\n        description: \"Volume in bytes of network traffic received\"\n    - bytes_sent:\n        usage: \"GAUGE\"\n        description: \"Volume in bytes of network traffic sent\"\n    - xact_time:\n        usage: \"GAUGE\"\n        description: \"Number of microseconds spent by pgbouncer when connected to PostgreSQL in a transaction\"\n    - query_time:\n        usage: \"GAUGE\"\n        description: \"Number of microseconds spent by pgbouncer when actively connected to PostgreSQL\"\n    - wait_time:\n        usage: \"GAUGE\"\n        description: \"Time spent by clients waiting for a server, in microseconds\"\n\npgbouncer_show_stats:\n  master: true\n  query: |\n    select _.database,\n    _.total_xact_count,\n    _.total_query_count,\n    _.total_received,\n    _.total_sent,\n    _.total_xact_time,\n    _.total_query_time,\n    _.total_wait_time,\n    _.avg_xact_count,\n    _.avg_query_count,\n    _.avg_recv,\n    _.avg_sent,\n    _.avg_xact_time,\n    _.avg_query_time,\n    _.avg_wait_time\n    FROM dblink('host=/var/run/postgresql port=6432 dbname=pgbouncer user=pgbouncer', 'show stats'::text)\n    _(database text, total_xact_count bigint, total_query_count bigint, total_received bigint, total_sent bigint,total_xact_time bigint, total_query_time bigint,\n    total_wait_time bigint, avg_xact_count bigint, avg_query_count bigint, avg_recv bigint, avg_sent bigint, avg_xact_time bigint, avg_query_time bigint,\n    avg_wait_time bigint);\n  metrics:\n    - database:\n        usage: \"LABEL\"\n        description: \"Database name\"\n    - total_xact_count:\n        usage: \"GAUGE\"\n        description: \"Total number of SQL transactions pooled\"\n    - total_query_count:\n        usage: \"GAUGE\"\n        description: \"Total number of SQL queries pooled\"\n    - total_received:\n        usage: \"GAUGE\"\n        description: \"Total volume in bytes of network traffic received\"\n    - total_sent:\n        usage: \"GAUGE\"\n        description: \"Total volume in bytes of network traffic sent\"\n    - total_xact_time:\n        usage: \"GAUGE\"\n        description: \"Total number of microseconds spent by pgbouncer when connected to PostgreSQL in a transaction\"\n    - total_query_time:\n        usage: \"GAUGE\"\n        description: \"Total number of microseconds spent by pgbouncer when actively connected to PostgreSQL\"\n    - total_wait_time:\n        usage: \"GAUGE\"\n        description: \"Time spent by clients waiting for a server, in microseconds\"\n    - avg_xact_count:\n        usage: \"GAUGE\"\n        description: \"Average transactions per second in last stat period\"\n    - avg_query_count:\n        usage: \"GAUGE\"\n        description: \"Average queries per second in last stat period\"\n    - avg_recv:\n        usage: \"GAUGE\"\n        description: \"Average received from clients bytes per second\"\n    - avg_sent:\n        usage: \"GAUGE\"\n        description: \"Average sent to clients bytes per second\"\n    - avg_xact_time:\n        usage: \"GAUGE\"\n        description: \"Average transaction duration, in microseconds\"\n    - avg_query_time:\n        usage: \"GAUGE\"\n        description: \"Average query duration, in microseconds\"\n    - avg_wait_time:\n        usage: \"GAUGE\"\n        description: \"Time spent by clients waiting for a server, in microseconds average per second\"\n\nnode_filesystem:\n  master: true\n  query: |\n    WITH mounts AS (\n      SELECT columns[1] AS device,\n        columns[2] AS mountpoint\n        FROM (SELECT regexp_split_to_array(line, E'\\\\s+') AS columns\n            FROM mounts() AS line) AS mounts\n      WHERE columns[2] LIKE '/var/%')\n    SELECT CASE WHEN columns[1] \u003c\u003e '-' THEN columns[1] ELSE NULL END AS device,\n        CASE WHEN columns[2] \u003c\u003e '-' THEN columns[2] ELSE NULL END AS mountpoint,\n        CASE WHEN columns[3] \u003c\u003e '-' THEN columns[3] ELSE NULL END AS fstype,\n        CASE WHEN columns[4] \u003c\u003e '-' THEN columns[4] ELSE NULL END AS size_bytes,\n        CASE WHEN columns[5] \u003c\u003e '-' THEN columns[5] ELSE NULL END AS avail_bytes,\n        CASE WHEN columns[6] \u003c\u003e '-' THEN columns[6] ELSE NULL END AS files,\n        CASE WHEN columns[7] \u003c\u003e '-' THEN columns[7] ELSE NULL END AS files_free,\n        CASE WHEN columns[8] \u003c\u003e '-' AND columns[8] \u003c\u003e 'timeout' THEN TRUE ELSE FALSE END AS device_error\n      FROM (SELECT regexp_split_to_array(line, E'\\\\s+') AS columns\n          FROM (SELECT df(mountpoint) AS line FROM mounts) AS df) AS df\n  metrics:\n    - device:\n        usage: \"LABEL\"\n        description: \"Device of the filesystem.\"\n    - mountpoint:\n        usage: \"LABEL\"\n        description: \"Mount point of the filesystem.\"\n    - fstype:\n        usage: \"LABEL\"\n        description: \"The type of filesystem.\"\n    - size_bytes:\n        usage: \"GAUGE\"\n        description: \"Filesystem size in bytes.\"\n    - avail_bytes:\n        usage: \"GAUGE\"\n        description: \"Filesystem space available to non-root users in bytes.\"\n    - files:\n        usage: \"GAUGE\"\n        description: \"Filesystem total file nodes.\"\n    - files_free:\n        usage: \"GAUGE\"\n        description: \"Filesystem total free file nodes.\"\n    - device_error:\n        usage: \"GAUGE\"\n        description: \"Whether an error occurred while getting statistics for the given device.\"\n"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:queries.yaml": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-prometheus-postgres-exporter-config",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1427",
        "uid": "c7db2563-02b1-420e-ac5e-53f56fb8226a"
      }
    },
    {
      "apiVersion": "v1",
      "data": {
        "create-backup.sh": "#!/bin/sh\n\nLOCK_RESOURCE=\"cronjob.batch\"\nLOCK_RESOURCE_NAME=\"$CRONJOB_NAME\"\n\n. \"$LOCAL_BIN_SHELL_UTILS_PATH\"\n\nrun() {\n  set -e\n\n  acquire_lock \u003e /tmp/try-lock 2\u003e\u00261\n  echo \"Lock acquired\"\n  maintain_lock \u003e\u003e /tmp/try-lock 2\u003e\u00261 \u0026\n  TRY_LOCK_PID=$!\n\n  reconcile_backups \u0026\n  PID=$!\n\n  set +e\n  (\n  set +x\n  while (kill -0 \"$PID\" \u0026\u0026 kill -0 \"$TRY_LOCK_PID\") 2\u003e/dev/null\n  do\n    true\n  done\n  )\n\n  if kill -0 \"$PID\" 2\u003e/dev/null\n  then\n    kill_with_childs \"$PID\"\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Lock lost:\\n'\"$(cat /tmp/try-lock | to_json_string)\"'\"}\n      ]'\n    cat /tmp/try-lock\n    echo \"Lock lost\"\n    return 1\n  else\n    kill_with_childs \"$TRY_LOCK_PID\"\n    release_lock \u003e\u003e /tmp/try-lock 2\u003e\u00261\n    echo \"Lock released\"\n    wait \"$PID\"\n    EXIT_CODE=\"$?\"\n    if [ \"$EXIT_CODE\" != 0 ]\n    then\n      cat /tmp/backup-push\n      echo \"Backup failed\"\n      [ -n \"$SCHEDULED_BACKUP_KEY\" ] || sleep 20\n      return 1\n    fi\n  fi\n}\n\nreconcile_backups() {\n  set -e\n  get_backup_crs\n\n  if [ -n \"$SCHEDULED_BACKUP_KEY\" ]\n  then\n    BACKUP_NAME=\"${CLUSTER_NAME}-$(date +%Y-%m-%d-%H-%M-%S --utc)\"\n  fi\n\n  BACKUP_CONFIG_RESOURCE_VERSION=\"$(kubectl get \"$BACKUP_CONFIG_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CONFIG\" --template='{{ .metadata.resourceVersion }}')\"\n  BACKUP_ALREADY_COMPLETED=false\n  create_or_update_backup_cr\n  if [ \"$BACKUP_ALREADY_COMPLETED\" = \"true\" ]\n  then\n    echo \"Already completed backup. Nothing to do!\"\n    return\n  fi\n\n  CURRENT_BACKUP_CONFIG=\"$(kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" \\\n    --template=\"{{ .status.sgBackupConfig.storage }}\")\"\n\n  set +e\n  echo \"Retrieving primary and replica\"\n  get_primary_and_replica_pods\n\n  echo \"Performing backup\"\n  do_backup\n  echo \"Backup completed\"\n\n  echo \"Extracting pg_controldata\"\n  extract_controldata\n  if [ \"$?\" = 0 ]\n  then\n    echo \"Extraction of pg_controldata completed\"\n  else\n    echo \"Extraction of pg_controldata failed\"\n  fi\n\n  echo \"Retain backups\"\n  retain_backups\n  if [ \"$?\" = 0 ]\n  then\n    echo \"Reconciliation of backups completed\"\n  else\n    echo \"Reconciliation of backups failed\"\n  fi\n\n  echo \"Listing existing backups\"\n  list_backups\n  if [ \"$?\" != 0 ]\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Backup can not be listed after creation '\"$(cat /tmp/backup-list | to_json_string)\"'\"}\n      ]'\n    cat /tmp/backup-list\n    echo \"Backups can not be listed after creation\"\n    return 1\n  fi\n  cat /tmp/backup-list | tr -d '[]' | sed 's/},{/}|{/g' | tr '|' '\\n' \\\n    | grep '\"backup_name\":\"'\"$CURRENT_BACKUP_NAME\"'\"' | tr -d '{}\"' | tr ',' '\\n' \u003e /tmp/current-backup\n  if [ \"$BACKUP_CONFIG_RESOURCE_VERSION\" != \"$(kubectl get \"$BACKUP_CONFIG_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CONFIG\" --template='{{ .metadata.resourceVersion }}')\" ]\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Backup configuration '\"$BACKUP_CONFIG\"' changed during backup\"}\n      ]'\n    cat /tmp/backup-list\n    echo \"Backup configuration '$BACKUP_CONFIG' changed during backup\"\n    return 1\n  elif ! grep -q \"^backup_name:${CURRENT_BACKUP_NAME}$\" /tmp/current-backup\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Backup '\"$CURRENT_BACKUP_NAME\"' was not found after creation\"}\n      ]'\n    cat /tmp/backup-list\n    echo \"Backup '$CURRENT_BACKUP_NAME' was not found after creation\"\n    return 1\n  fi\n\n  echo \"Updating backup CR as completed\"\n  set_backup_completed\n  echo \"Backup CR updated as completed\"\n\n  echo \"Reconcile backup CRs\"\n  reconcile_backup_crs\n  echo \"Reconciliation of backup CRs completed\"\n}\n\nget_backup_crs() {\n  BACKUP_CR_TEMPLATE=\"{{ range .items }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}{{ .spec.sgCluster }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ .metadata.name }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ with .status.process.status }}{{ . }}{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ with .status.internalName }}{{ . }}{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ with .status.process.jobPod }}{{ . }}{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ with .metadata.labels }}{{ with index . \\\"$SCHEDULED_BACKUP_KEY\\\" }}{{ . }}{{ end }}{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ if .spec.managedLifecycle }}true{{ else }}false{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}:{{ if .status.process.managedLifecycle }}true{{ else }}false{{ end }}\"\n  BACKUP_CR_TEMPLATE=\"${BACKUP_CR_TEMPLATE}{{ printf \"'\"\\n\"'\" }}{{ end }}\"\n  kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \\\n    --template=\"$BACKUP_CR_TEMPLATE\" \u003e /tmp/all-backups\n  grep \"^$CLUSTER_NAME:\" /tmp/all-backups \u003e /tmp/backups || true\n}\n\ncreate_or_update_backup_cr() {\n  BACKUP_CONFIG_YAML=$(cat \u003c\u003c BACKUP_CONFIG_YAML_EOF\n    baseBackups:\n      compression: \"{{ .spec.baseBackups.compression }}\"\n    storage:\n      type: \"{{ .spec.storage.type }}\"\n      {{- with .spec.storage.s3 }}\n      s3:\n        bucket: \"{{ .bucket }}\"\n        {{ with .path }}path: \"{{ . }}\"{{ end }}\n        awsCredentials:\n          secretKeySelectors:\n            accessKeyId:\n              key: \"{{ .awsCredentials.secretKeySelectors.accessKeyId.key }}\"\n              name: \"{{ .awsCredentials.secretKeySelectors.accessKeyId.name }}\"\n            secretAccessKey:\n              key: \"{{ .awsCredentials.secretKeySelectors.secretAccessKey.key }}\"\n              name: \"{{ .awsCredentials.secretKeySelectors.secretAccessKey.name }}\"\n        {{ with .region }}region: \"{{ . }}\"{{ end }}\n        {{ with .storageClass }}storageClass: \"{{ . }}\"{{ end }}\n      {{- end }}\n      {{- with .spec.storage.s3Compatible }}\n      s3Compatible:\n        bucket: \"{{ .bucket }}\"\n        {{ with .path }}path: \"{{ . }}\"{{ end }}\n        awsCredentials:\n          secretKeySelectors:\n            accessKeyId:\n              key: \"{{ .awsCredentials.secretKeySelectors.accessKeyId.key }}\"\n              name: \"{{ .awsCredentials.secretKeySelectors.accessKeyId.name }}\"\n            secretAccessKey:\n              key: \"{{ .awsCredentials.secretKeySelectors.secretAccessKey.key }}\"\n              name: \"{{ .awsCredentials.secretKeySelectors.secretAccessKey.name }}\"\n        {{ with .region }}region: \"{{ . }}\"{{ end }}\n        {{ with .endpoint }}endpoint: \"{{ . }}\"{{ end }}\n        {{ with .enablePathStyleAddressing }}enablePathStyleAddressing: {{ . }}{{ end }}\n        {{ with .storageClass }}storageClass: \"{{ . }}\"{{ end }}\n      {{- end }}\n      {{- with .spec.storage.gcs }}\n      gcs:\n        bucket: \"{{ .bucket }}\"\n        {{ with .path }}path: \"{{ . }}\"{{ end }}\n        gcpCredentials:\n          {{- if .gcpCredentials.fetchCredentialsFromMetadataService }}\n          fetchCredentialsFromMetadataService: true\n          {{- else }}\n          secretKeySelectors:\n            serviceAccountJSON:\n              key: \"{{ .gcpCredentials.secretKeySelectors.serviceAccountJSON.key }}\"\n              name: \"{{ .gcpCredentials.secretKeySelectors.serviceAccountJSON.name }}\"\n          {{- end }}\n      {{- end }}\n      {{- with .spec.storage.azureBlob }}\n      azureBlob:\n        bucket: \"{{ .bucket }}\"\n        {{ with .path }}path: \"{{ . }}\"{{ end }}\n        azureCredentials:\n          secretKeySelectors:\n            storageAccount:\n              key: \"{{ .azureCredentials.secretKeySelectors.storageAccount.key }}\"\n              name: \"{{ .azureCredentials.secretKeySelectors.storageAccount.name }}\"\n            accessKey:\n              key: \"{{ .azureCredentials.secretKeySelectors.accessKey.key }}\"\n              name: \"{{ .azureCredentials.secretKeySelectors.accessKey.name }}\"\n      {{- end }}\nBACKUP_CONFIG_YAML_EOF\n  )\nBACKUP_STATUS_YAML=$(cat \u003c\u003c BACKUP_STATUS_YAML_EOF\nstatus:\n  process:\n    status: \"$BACKUP_PHASE_RUNNING\"\n    jobPod: \"$POD_NAME\"\n  sgBackupConfig:\n$(kubectl get \"$BACKUP_CONFIG_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CONFIG\" --template=\"$BACKUP_CONFIG_YAML\")\nBACKUP_STATUS_YAML_EOF\n  )\n\n  if ! kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" -o name \u003e/dev/null 2\u003e\u00261\n  then\n    echo \"Creating backup CR\"\n    cat \u003c\u003c EOF | kubectl create -f - -o yaml\napiVersion: $BACKUP_CRD_APIVERSION\nkind: $BACKUP_CRD_KIND\nmetadata:\n  namespace: \"$CLUSTER_NAMESPACE\"\n  name: \"$BACKUP_NAME\"\n  annotations:\n    $SCHEDULED_BACKUP_KEY: \"$RIGHT_VALUE\"\nspec:\n  sgCluster: \"$CLUSTER_NAME\"\n  managedLifecycle: true\n$BACKUP_STATUS_YAML\nEOF\n  else\n    if ! kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --template=\"{{ .status.process.status }}\" \\\n      | grep -q \"^$BACKUP_PHASE_COMPLETED$\"\n    then\n      echo \"Updating backup CR\"\n      kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" -o yaml --type merge --patch \"$(\n        (\n          kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" -o yaml\n          echo \"$BACKUP_STATUS_YAML\"\n        ) | kubectl create --dry-run=client -f - -o json)\"\n    else\n      BACKUP_ALREADY_COMPLETED=true\n    fi\n  fi\n}\n\nget_primary_and_replica_pods() {\n  kubectl get pod -n \"$CLUSTER_NAMESPACE\" -l \"${PATRONI_CLUSTER_LABELS},${PATRONI_ROLE_KEY}=${PATRONI_PRIMARY_ROLE}\" -o name \u003e /tmp/current-primary\n  kubectl get pod -n \"$CLUSTER_NAMESPACE\" -l \"${PATRONI_CLUSTER_LABELS},${PATRONI_ROLE_KEY}=${PATRONI_REPLICA_ROLE}\" -o name | head -n 1 \u003e /tmp/current-replica-or-primary\n  if [ ! -s /tmp/current-primary ]\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Unable to find primary, backup aborted\"}\n      ]'\n    kubectl get pod -n \"$CLUSTER_NAMESPACE\" -l \"${PATRONI_CLUSTER_LABELS}\" \u003e\u00262\n    echo \u003e /tmp/backup-push\n    echo \"Unable to find primary, backup aborted\" \u003e\u003e /tmp/backup-push\n    exit 1\n  fi\n\n  if [ ! -s /tmp/current-replica-or-primary ]\n  then\n    cat /tmp/current-primary \u003e /tmp/current-replica-or-primary\n    echo \"Primary is $(cat /tmp/current-primary)\"\n    echo \"Replica not found, primary will be used for cleanups\"\n  else\n    echo \"Primary is $(cat /tmp/current-primary)\"\n    echo \"Replica is $(cat /tmp/current-replica-or-primary)\"\n  fi\n}\n\ndo_backup() {\n  cat \u003c\u003c EOF | kubectl exec -i -n \"$CLUSTER_NAMESPACE\" \"$(cat /tmp/current-primary)\" -c \"$PATRONI_CONTAINER_NAME\" \\\n    -- sh -e $SHELL_XTRACE \u003e /tmp/backup-push 2\u003e\u00261\nexec-with-env \"$BACKUP_ENV\" \\\\\n  -- wal-g backup-push \"$PG_DATA_PATH\" -f $([ \"$BACKUP_IS_PERMANENT\" = true ] \u0026\u0026 echo '-p' || true)\nEOF\n  if [ \"$?\" != 0 ]\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Backup failed: '\"$(cat /tmp/backup-push | to_json_string)\"'\"}\n      ]'\n    exit 1\n  fi\n  CURRENT_BACKUP_NAME=\n  if grep -q \" Wrote backup with name \" /tmp/backup-push\n  then\n    CURRENT_BACKUP_NAME=\"$(grep \" Wrote backup with name \" /tmp/backup-push | sed 's/.* \\([^ ]\\+\\)$/\\1/')\"\n  fi\n  if [ -z \"$CURRENT_BACKUP_NAME\" ]\n  then\n    kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch '[\n      {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_FAILED\"'\"},\n      {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"Backup name not found in backup-push log:\\n'\"$(cat /tmp/backup-push | to_json_string)\"'\"}\n      ]'\n    cat /tmp/backup-push\n    echo \"Backup name not found in backup-push log\"\n    exit 1\n  fi\n}\n\nextract_controldata() {\n  cat \u003c\u003c EOF | kubectl exec -i -n \"$CLUSTER_NAMESPACE\" \"$(cat /tmp/current-primary)\" -c \"$PATRONI_CONTAINER_NAME\" \\\n      -- sh -e $SHELL_XTRACE \u003e /tmp/pg_controldata\npg_controldata --pgdata=\"$PG_DATA_PATH\"\nEOF\n  if [ \"$?\" = 0 ]\n  then\n    cat /tmp/pg_controldata | awk -F ':' '{ printf \"%s: %s\\n\", $1, $2 }' | awk '{ $2=$2;print }'| awk -F ': ' '\n          BEGIN { print \"\\n            {\"}\n          {\n            if (NR \u003e 1)\n              printf \",\\n             \\\"%s\\\": \\\"%s\\\"\", $1, $2\n            else\n              printf \"             \\\"%s\\\": \\\"%s\\\"\", $1, $2\n          }\n          END { print \"\\n            }\" }' \u003e /tmp/json_controldata\n  else\n    echo '{}' \u003e /tmp/json_controldata\n    return 1\n  fi\n}\n\nretain_backups() {\n  cat \u003c\u003c EOF | kubectl exec -i -n \"$CLUSTER_NAMESPACE\" \"$(cat /tmp/current-replica-or-primary)\" -c \"$PATRONI_CONTAINER_NAME\" \\\n  -- sh -e $SHELL_XTRACE\n# for each existing backup sorted by backup name ascending (this also mean sorted by creation date ascending)\nexec-with-env \"$BACKUP_ENV\" \\\\\n  -- wal-g backup-list --detail --json \\\\\n  | tr -d '[]' | sed 's/},{/}|{/g' | tr '|' '\\\\n' \\\\\n  | grep '\"backup_name\"' \\\\\n  | sort -r -t , -k 2 \\\\\n  | (RETAIN=\"$RETAIN\"\n    while read BACKUP\n    do\n      BACKUP_NAME=\"\\$(echo \"\\$BACKUP\" | tr -d '{}\\\\42' | tr ',' '\\\\n' \\\\\n          | grep 'backup_name' | cut -d : -f 2-)\"\n      echo \"Check if backup \\$BACKUP_NAME has to be retained and will retain \\$RETAIN backups\"\n      # if is not the created backup and is not in backup CR list, mark as impermanent\n      if [ \"\\$BACKUP_NAME\" != \"$CURRENT_BACKUP_NAME\" ] \\\\\n        \u0026\u0026 ! echo '$(cat /tmp/backups)' \\\\\n        | cut -d : -f 4 \\\\\n        | grep -v '^\\$' \\\\\n        | grep -q \"^\\$BACKUP_NAME\\$\"\n      then\n        if echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":true\"\n        then\n          echo \"Mark \\$BACKUP_NAME as impermanent and will retain \\$RETAIN backups\"\n          exec-with-env \"$BACKUP_ENV\" \\\\\n            -- wal-g backup-mark -i \"\\$BACKUP_NAME\"\n        fi\n      # if is inside the retain window, mark as permanent and decrease RETAIN counter\n      elif [ \"\\$RETAIN\" -gt 0 ]\n      then\n        if [ \"\\$BACKUP_NAME\" = \"$CURRENT_BACKUP_NAME\" -a \"$BACKUP_IS_PERMANENT\" != true ] \\\\\n          || echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":false\"\n        then\n          echo \"Mark \\$BACKUP_NAME as permanent and will retain \\$((RETAIN-1)) more backups\"\n          exec-with-env \"$BACKUP_ENV\" \\\\\n            -- wal-g backup-mark \"\\$BACKUP_NAME\"\n        fi\n        RETAIN=\"\\$((RETAIN-1))\"\n      # if is outside the retain window...\n      elif [ \"\\$RETAIN\" -le 0 ]\n      then\n        # ... and has a managed lifecycle, mark as impermanent\n        if echo '$(cat /tmp/backups)' \\\\\n          | grep '^[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:true' \\\\\n          | cut -d : -f 4 \\\\\n          | grep -v '^\\$' \\\\\n          | grep -q \"^\\$BACKUP_NAME\\$\" \\\\\n          \u0026\u0026 echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":true\"\n        then\n          echo \"Mark \\$BACKUP_NAME as impermanent\"\n          exec-with-env \"$BACKUP_ENV\" \\\\\n            -- wal-g backup-mark -i \"\\$BACKUP_NAME\"\n        # ... and has not a managed lifecycle, mark as permanent\n        elif echo '$(cat /tmp/backups)' \\\\\n          | grep -v '^[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:true' \\\\\n          | cut -d : -f 4 \\\\\n          | grep -v '^\\$' \\\\\n          | grep -q \"^\\$BACKUP_NAME\\$\" \\\\\n          \u0026\u0026 echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":false\"\n        then\n          echo \"Mark \\$BACKUP_NAME as permanent\"\n          exec-with-env \"$BACKUP_ENV\" \\\\\n            -- wal-g backup-mark \"\\$BACKUP_NAME\"\n        fi\n      fi\n    done)\n\n# removes all backups that are marked as impermanent\necho \"Cleaning up impermanent backups\"\nexec-with-env \"$BACKUP_ENV\" \\\\\n  -- wal-g delete retain FIND_FULL \"0\" --confirm\n\n# for each existing backup\nexec-with-env \"$BACKUP_ENV\" \\\\\n  -- wal-g backup-list --detail --json \\\\\n  | tr -d '[]' | sed 's/},{/}|{/g' | tr '|' '\\\\n' \\\\\n  | grep '\"backup_name\"' \\\\\n  | while read BACKUP\n    do\n      BACKUP_NAME=\"\\$(echo \"\\$BACKUP\" | tr -d '{}\\\\42' | tr ',' '\\\\n' \\\\\n          | grep 'backup_name' | cut -d : -f 2-)\"\n      echo \"Check if backup \\$BACKUP_NAME has to be set permanent or impermanent\"\n      # if is the created backup and has a managed lifecycle, mark as impermanent\n      if [ \"\\$BACKUP_NAME\" = \"$CURRENT_BACKUP_NAME\" -a \"$BACKUP_IS_PERMANENT\" != true ] \\\\\n        || (echo '$(cat /tmp/backups)' \\\\\n        | grep '^[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:true' \\\\\n        | cut -d : -f 4 \\\\\n        | grep -v '^\\$' \\\\\n        | grep -q \"^\\$BACKUP_NAME\\$\" \\\\\n        \u0026\u0026 echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":true\")\n      then\n        echo \"Mark \\$BACKUP_NAME as impermanent\"\n        exec-with-env \"$BACKUP_ENV\" \\\\\n          -- wal-g backup-mark -i \"\\$BACKUP_NAME\"\n      # if has not a managed lifecycle, mark as permanent\n      elif echo '$(cat /tmp/backups)' \\\\\n        | grep -v '^[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:[^:]*:true' \\\\\n        | cut -d : -f 4 \\\\\n        | grep -v '^\\$' \\\\\n        | grep -q \"^\\$BACKUP_NAME\\$\" \\\\\n        \u0026\u0026 echo \"\\$BACKUP\" | grep -q \"\\\\\"is_permanent\\\\\":false\"\n      then\n        echo \"Mark \\$BACKUP_NAME as permanent\"\n        exec-with-env \"$BACKUP_ENV\" \\\\\n          -- wal-g backup-mark \"\\$BACKUP_NAME\"\n      fi\n    done\nEOF\n}\n\nlist_backups() {\n  cat \u003c\u003c EOF | kubectl exec -i -n \"$CLUSTER_NAMESPACE\" \"$(cat /tmp/current-replica-or-primary)\" -c \"$PATRONI_CONTAINER_NAME\" \\\n    -- sh -e $SHELL_XTRACE \u003e /tmp/backup-list\nWALG_LOG_LEVEL= exec-with-env \"$BACKUP_ENV\" \\\\\n-- wal-g backup-list --detail --json\nEOF\n  cat /tmp/backup-list | tr -d '[]' | sed 's/},{/}|{/g' | tr '|' '\\n' \\\n    | grep '\"backup_name\"' \\\n    \u003e /tmp/existing-backups\n}\n\nset_backup_completed() {\n  EXISTING_BACKUP_IS_PERMANENT=\"$(grep \"^is_permanent:\" /tmp/current-backup | cut -d : -f 2-)\"\n  IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"\"\n  if [ \"$EXISTING_BACKUP_IS_PERMANENT\" = \"true\" ]\n  then\n    IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"false\"\n  else\n    IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"true\"\n  fi\n\n  TIMELINE=\"$(grep \"^wal_file_name:\" /tmp/current-backup)\" # Read file name from current-backup\n  TIMELINE=\"$(printf \"$TIMELINE\" | cut -d ':' -f 2 | tr -d '[:blank:]')\" # Extract only the wal name value\n  TIMELINE=\"$(expr substr \"$TIMELINE\" 1 8)\" # Get the first 8 digits\n  TIMELINE=\"$(printf \"%d\" \"0x$TIMELINE\")\" # Convert hex to decimal\n\n  BACKUP_PATCH='[\n    {\"op\":\"replace\",\"path\":\"/status/internalName\",\"value\":\"'\"$CURRENT_BACKUP_NAME\"'\"},\n    {\"op\":\"replace\",\"path\":\"/status/process/status\",\"value\":\"'\"$BACKUP_PHASE_COMPLETED\"'\"},\n    {\"op\":\"replace\",\"path\":\"/status/process/failure\",\"value\":\"\"},\n    {\"op\":\"replace\",\"path\":\"/status/process/managedLifecycle\",\"value\":'$IS_BACKUP_SUBJECT_TO_RETENTION_POLICY'},\n    {\"op\":\"replace\",\"path\":\"/status/process/timing\",\"value\":{\n        \"stored\":\"'\"$(grep \"^time:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"start\":\"'\"$(grep \"^start_time:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"end\":\"'\"$(grep \"^finish_time:\" /tmp/current-backup | cut -d : -f 2-)\"'\"\n      }\n    },\n    {\"op\":\"replace\",\"path\":\"/status/backupInformation\",\"value\":{\n        \"startWalFile\":\"'\"$(grep \"^wal_file_name:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"timeline\":\"'\"$TIMELINE\"'\",\n        \"hostname\":\"'\"$(grep \"^hostname:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"sourcePod\":\"'\"$(grep \"^hostname:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"pgData\":\"'\"$(grep \"^data_dir:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"postgresVersion\":\"'\"$(grep \"^pg_version:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"systemIdentifier\":\"'\"$(grep \"^system_identifier:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n        \"lsn\":{\n          \"start\":\"'\"$(grep \"^start_lsn:\" /tmp/current-backup | cut -d : -f 2-)\"'\",\n          \"end\":\"'\"$(grep \"^finish_lsn:\" /tmp/current-backup | cut -d : -f 2-)\"'\"\n        },\n        \"size\":{\n          \"uncompressed\":'\"$(grep \"^uncompressed_size:\" /tmp/current-backup | cut -d : -f 2-)\"',\n          \"compressed\":'\"$(grep \"^compressed_size:\" /tmp/current-backup | cut -d : -f 2-)\"'\n        },\n        \"controlData\": '\"$(cat /tmp/json_controldata)\"'\n      }\n    }\n  ]'\n  kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_NAME\" --type json --patch \"$BACKUP_PATCH\"\n}\n\nreconcile_backup_crs() {\n  kubectl get pod -n \"$CLUSTER_NAMESPACE\" \\\n    --template=\"{{ range .items }}{{ .metadata.name }}{{ printf \"'\"\\n\"'\" }}{{ end }}\" \\\n    \u003e /tmp/pods\n  for BACKUP in $(cat /tmp/backups)\n  do\n    BACKUP_CR_NAME=\"$(echo \"$BACKUP\" | cut -d : -f 2)\"\n    BACKUP_PHASE=\"$(echo \"$BACKUP\" | cut -d : -f 3)\"\n    BACKUP_NAME=\"$(echo \"$BACKUP\" | cut -d : -f 4)\"\n    BACKUP_POD=\"$(echo \"$BACKUP\" | cut -d : -f 5)\"\n    BACKUP_SHEDULED_BACKUP=\"$(echo \"$BACKUP\" | cut -d : -f 6)\"\n    BACKUP_MANAGED_LIFECYCLE=\"$(echo \"$BACKUP\" | cut -d : -f 8)\"\n    BACKUP_IS_PERMANENT=\"$([ \"$BACKUP_MANAGED_LIFECYCLE\" = true ] \u0026\u0026 echo false || echo true)\"\n    BACKUP_CONFIG=\"$(kubectl get \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CR_NAME\" \\\n      --template=\"{{ .status.sgBackupConfig.storage }}\")\"\n    # if backup CR has backup internal name, is marked as completed, uses the same current\n    # backup config but is not found in the storage, delete it\n    if [ -n \"$BACKUP_NAME\" ] \u0026\u0026 [ \"$BACKUP_PHASE\" = \"$BACKUP_PHASE_COMPLETED\" ] \\\n      \u0026\u0026 [ \"$BACKUP_CONFIG\" = \"$CURRENT_BACKUP_CONFIG\" ] \\\n      \u0026\u0026 ! grep -q \"\\\"backup_name\\\":\\\"$BACKUP_NAME\\\"\" /tmp/existing-backups\n    then\n      echo \"Deleting backup CR $BACKUP_CR_NAME since backup does not exists\"\n      kubectl delete \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CR_NAME\"\n    # if backup CR is a scheduled backup, is marked as running, has no pod or pod\n    # has been terminated, delete it\n    elif [ \"$BACKUP_SHEDULED_BACKUP\" = \"$RIGHT_VALUE\" ] \\\n      \u0026\u0026 [ \"$BACKUP_PHASE\" = \"$BACKUP_PHASE_RUNNING\" ] \\\n      \u0026\u0026 ([ -z \"$BACKUP_POD\" ] || ! grep -q \"^$BACKUP_POD$\" /tmp/pods)\n    then\n      echo \"Deleting backup CR $BACKUP_CR_NAME since backup is running but pod does not exists\"\n      kubectl delete \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CR_NAME\"\n    # if backup CR has backup internal name, is marked as completed, and is marked as\n    # stored as not managed lifecycle or managed lifecycle and is found as managed lifecycle or\n    # not managed lifecycle respectively, then mark it as stored as managed lifecycle or\n    # not managed lifecycle respectively\n    elif [ -n \"$BACKUP_NAME\" ] \u0026\u0026 [ \"$BACKUP_PHASE\" = \"$BACKUP_PHASE_COMPLETED\" ] \\\n      \u0026\u0026 ! grep \"\\\"backup_name\\\":\\\"$BACKUP_NAME\\\"\" /tmp/existing-backups \\\n        | grep -q \"\\\"is_permanent\\\":$BACKUP_IS_PERMANENT\"\n    then\n      EXISTING_BACKUP_IS_PERMANENT=\"$(grep \"\\\"backup_name\\\":\\\"$BACKUP_NAME\\\"\" /tmp/existing-backups \\\n        | tr -d '{}\"' | tr ',' '\\n' | grep \"^is_permanent:\" | cut -d : -f 2-)\"\n      IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"\"\n      if [ \"$EXISTING_BACKUP_IS_PERMANENT\" = \"true\" ]\n      then\n        IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"false\"\n      else\n        IS_BACKUP_SUBJECT_TO_RETENTION_POLICY=\"true\"\n      fi\n      echo \"Updating backup CR $BACKUP_CR_NAME .status.process.managedLifecycle to $IS_BACKUP_SUBJECT_TO_RETENTION_POLICY since was updated in the backup\"\n      kubectl patch \"$BACKUP_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$BACKUP_CR_NAME\" --type json --patch '[\n        {\"op\":\"replace\",\"path\":\"/status/process/managedLifecycle\",\"value\":'$IS_BACKUP_SUBJECT_TO_RETENTION_POLICY'}\n        ]'\n    fi\n  done\n}\n\nrun\n",
        "exec-with-env": "#!/bin/sh\n\nset -e\n\ndie() {\n  \u003e\u00262 echo \"$@\"\n  exit 1\n}\n\nREPLACES=\"\"\nOVERWRITE=false\n\nwhile [ \"$#\" -gt 0 ]\ndo\n  case \"$1\" in\n  -r|--replace)\n    shift\n    if [ -z \"$REPLACES\" ]\n    then\n      REPLACES=\"$1\"\n    else\n      REPLACES=\"$REPLACES,$1\"\n    fi\n    shift\n    ;;\n  -o|--overwrite)\n    shift\n    OVERWRITE=true\n    ;;\n  --)\n    shift\n    break\n    ;;\n  *)\n    if echo \"$1\" | grep -q \"^/\"\n    then\n      plain_envdir=\"$1\"\n      secret_envdir=\"\"\n      [ -d \"$plain_envdir\" ] \\\n        || die \"$plain_envdir is not a directory\"\n    else\n      secret_envdir=\"${BASE_SECRET_PATH}/$1\"\n      plain_envdir=\"${BASE_ENV_PATH}/$1\"\n      [ -d \"$plain_envdir\" -o -d \"$secret_envdir\" ] \\\n        || die \"None of $plain_envdir or $secret_envdir is a directory\"\n    fi\n    shift\n    for envdir in \"$plain_envdir\" \"$secret_envdir\"\n    do\n      [ -d \"$envdir\" ] || continue\n      # When md5sum of values of environment variables ordered alphabetically (excluding variable\n      # MD5SUM) does not match variable MD5SUM we fail since in transition state\n      [ \"$(ls -1a \"$envdir\" | grep -v \"^MD5SUM$\" \\\n        | while read envvar; do [ ! -f \"$envdir/$envvar\" ] || cat \"$envdir/$envvar\"; done \\\n        | md5sum | cut -d ' ' -f 1 | tr 'a-z' 'A-Z')\" = \"$(cat \"$envdir/MD5SUM\")\" ] \\\n        || die \"Environment variable in transient state\"\n      for envvar in $(ls -1a \"$envdir\")\n      do\n        # Only export if \"$envdir/$envvar\" is a file\n        # and environment variable with name $envvar is not set\n        [ ! -f \"$envdir/$envvar\" ] || [ \"$OVERWRITE\" != \"true\" -a -n \"$(eval \"echo \\\"\\$$envvar\\\"\")\" ] \\\n          || eval \"export $envvar='$(cat \"$envdir/$envvar\")'\"\n      done\n    done\n    ;;\n  esac\ndone\n\nif [ -n \"$REPLACES\" ]\nthen\n  for REPLACE in $(echo \"$REPLACES\" | tr ',' '\\n')\n  do\n    eval \"export ${REPLACE%=*}=\\\"\\$${REPLACE#*=}\\\"\"\n  done\nfi\n\nif [ -n \"$1\" ]\nthen\n  exec \"$@\"\nfi\n",
        "group": "root:x:0:\nbin:x:1:\ndaemon:x:2:\nsys:x:3:\nadm:x:4:\ntty:x:5:\ndisk:x:6:\nlp:x:7:\nmem:x:8:\nkmem:x:9:\nwheel:x:10:\ncdrom:x:11:\nmail:x:12:\nman:x:15:\ndialout:x:18:\nfloppy:x:19:\ngames:x:20:\ntape:x:33:\nvideo:x:39:\nftp:x:50:\nlock:x:54:\naudio:x:63:\nusers:x:100:\nnobody:x:65534:\n",
        "gshadow": "root:::\nbin:::\ndaemon:::\nsys:::\nadm:::\ntty:::\ndisk:::\nlp:::\nmem:::\nkmem:::\nwheel:::\ncdrom:::\nmail:::\nman:::\ndialout:::\nfloppy:::\ngames:::\ntape:::\nvideo:::\nftp:::\nlock:::\naudio:::\nusers:::\nnobody:::\n",
        "major-version-upgrade.sh": "#!/bin/sh\n\nset -e\n\nif [ -f \"$PG_UPGRADE_PATH/.upgraded-from-$SOURCE_VERSION-to-$TARGET_VERSION\" ]\nthen\n  echo \"Major version upgrade already performed\"\n  exit 0\nfi\n\nif [ \"$POSTGRES_VERSION\" != \"$TARGET_VERSION\" ]\nthen\n  echo \"Can not perform major version upgrade, postgres version has not been updated\"\n  exit 1\nfi\n\nif [ \"$PRIMARY_INSTANCE\" != \"$POD_NAME\" ]\nthen\n  echo \"Removing data of non primary instance\"\n  rm -rf \"$PG_DATA_PATH\"\n  mkdir -p \"$PG_UPGRADE_PATH\"\n  touch \"$PG_UPGRADE_PATH/.upgraded-from-$SOURCE_VERSION-to-$TARGET_VERSION\"\n  echo \"Major version upgrade not needed for non primary instance\"\n  exit 0\nfi\n\nif [ ! -f \"$PG_DATA_PATH/.upgraded-from-$SOURCE_VERSION-to-$TARGET_VERSION\" ]\nthen\n  echo \"Creating new database\"\n  rm -rf \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\"\n  mkdir -p \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\"\n  initdb \\\n    -D \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\" \\\n    -E \"$ENCODING\" \\\n    --locale \"$LOCALE\" \\\n    $(\"$DATA_CHECKSUM\" \u0026\u0026 echo \"-k\" || true)\n  (\n  cd \"$PG_UPGRADE_PATH/$TARGET_VERSION\"\n  if [ \"$CHECK\" ]\n  then\n    echo \"Checking major version upgrade\"\n    if ! pg_upgrade -c \\\n      -b \"/usr/lib/postgresql/$SOURCE_VERSION/bin\" \\\n      -B \"/usr/lib/postgresql/$TARGET_VERSION/bin\" \\\n      -d \"$PG_DATA_PATH\" \\\n      -D \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\" \\\n      -o \"-c 'dynamic_library_path=$SOURCE_PG_LIB_PATH:$SOURCE_PG_EXTRA_LIB_PATH'\" \\\n      -O \"-c 'dynamic_library_path=$TARGET_PG_LIB_PATH:$TARGET_PG_EXTRA_LIB_PATH'\" \\\n      $(\"$LINK\" \u0026\u0026 echo \"-k\" || true) \\\n      $(\"$CLONE\" \u0026\u0026 echo \"--clone\" || true)\n    then\n      grep . *.txt *.log 2\u003e/dev/null | cat \u003e\u00262\n      exit 1\n    fi\n  fi\n  echo \"Performing major version upgrade\"\n  if ! pg_upgrade \\\n    -b \"/usr/lib/postgresql/$SOURCE_VERSION/bin\" \\\n    -B \"/usr/lib/postgresql/$TARGET_VERSION/bin\" \\\n    -d \"$PG_DATA_PATH\" \\\n    -D \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\" \\\n    -o \"-c 'dynamic_library_path=$SOURCE_PG_LIB_PATH:$SOURCE_PG_EXTRA_LIB_PATH'\" \\\n    -O \"-c 'dynamic_library_path=$TARGET_PG_LIB_PATH:$TARGET_PG_EXTRA_LIB_PATH'\" \\\n    $(\"$LINK\" \u0026\u0026 echo \"-k\" || true) \\\n    $(\"$CLONE\" \u0026\u0026 echo \"--clone\" || true)\n  then\n    grep . *.txt *.log 2\u003e/dev/null | cat \u003e\u00262\n    exit 1\n  fi\n  )\nfi\n\nif [ ! -d \"$PG_UPGRADE_PATH/$SOURCE_VERSION/data\" ]\nthen\n  mkdir -p \"$PG_UPGRADE_PATH/$SOURCE_VERSION\"\n  mv \"$PG_DATA_PATH\" \"$PG_UPGRADE_PATH/$SOURCE_VERSION/data\"\nfi\nif [ ! -d \"$PG_DATA_PATH\" ]\nthen\n  if [ ! -d \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\" ]\n  then\n    echo \"Upgraded data not found!\"\n    exit 1\n  fi\n  mv \"$PG_UPGRADE_PATH/$TARGET_VERSION/data\" \"$PG_DATA_PATH\"\nfi\nrm -rf \"$PG_UPGRADE_PATH/$SOURCE_VERSION/data\"\ntouch \"$PG_UPGRADE_PATH/.upgraded-from-$SOURCE_VERSION-to-$TARGET_VERSION\"\necho \"Major version upgrade performed\"\n",
        "passwd": "root:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\ndaemon:x:2:2:daemon:/sbin:/sbin/nologin\nadm:x:3:4:adm:/var/adm:/sbin/nologin\nlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\nsync:x:5:0:sync:/sbin:/bin/sync\nshutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\nhalt:x:7:0:halt:/sbin:/sbin/halt\nmail:x:8:12:mail:/var/spool/mail:/sbin/nologin\noperator:x:11:0:operator:/root:/sbin/nologin\ngames:x:12:100:games:/usr/games:/sbin/nologin\nftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\n",
        "post-init.sh": "#!/bin/bash\nset -e\nIFS=$'\\n'\nINIT_SCRIPT_PATH=/etc/patroni/init-script.d\n\n# check if path exist\nif [ -d \"$INIT_SCRIPT_PATH\" -a \"$(ls -1 \"$INIT_SCRIPT_PATH\" 2\u003e/dev/null|wc -l)\" -ge 1 ]\nthen\n  for FILE in $(ls -1 \"$INIT_SCRIPT_PATH\")\n  do\n    FILE=\"$(realpath \"$INIT_SCRIPT_PATH/$FILE\")\"\n    if [ \"${FILE: -3}\" == \".sh\" -a -f \"$FILE\" ]\n    then\n      echo \"Executing shell script $FILE\"\n      bash \"$FILE\"\n      echo \"Shell script $FILE executed\"\n    fi\n  done\n  # search for .sql file and execute them\n  for FILE in $(ls -1 \"$INIT_SCRIPT_PATH\" | sort -n -t -)\n  do\n    FILE=\"$(realpath \"$INIT_SCRIPT_PATH/$FILE\")\"\n    if [ \"${FILE: -4}\" == \".sql\" -a -f \"$FILE\" ]\n    then\n      DATABASE=\"$([ \"$(basename \"$FILE\" | tr '.' '\\n' | wc -l)\" -gt 2 ] \\\n        \u0026\u0026 echo \"$(basename \"$FILE\" | tr '.' '\\n' \\\n          | tail -n +2 | head -n -1 | tr '\\n' '.' \\\n          | sed 's#\\\\\\\\#\\\\#g' | sed 's#\\\\h#/#g')\" \\\n        || echo postgres)\"\n      DATABASE=\"${DATABASE%.}\"\n      echo \"Executing SQL script $FILE for DATABASE $DATABASE with user postgres on port ${POSTGRES_PORT}\"\n      cat \"$FILE\" | python3 -c \"$(cat \u003c\u003c EOF\nimport psycopg2,sys\nconnection = psycopg2.connect(\"user=postgres dbname='$DATABASE' port=${POSTGRES_PORT}\")\nconnection.autocommit = True\ncursor = connection.cursor()\ncursor.execute(sys.stdin.read())\ntry: print(cursor.fetchall())\nexcept: print()\nEOF\n)\"\n      echo \"Shell SQL $FILE executed\"\n    fi\n  done\nfi\n",
        "relocate-binaries.sh": "#!/bin/sh\n\nset -e\n\nmkdir -p \"$PG_EXTENSIONS_PATH\"\n\nchmod 700 \"$PG_EXTENSIONS_PATH\"\n\nmkdir -p \"$PG_EXTENSIONS_BIN_PATH\"\n\nmkdir -p \"$PG_EXTENSIONS_LIB_PATH\"\n\nmkdir -p \"$PG_EXTENSIONS_EXTENSION_PATH\"\n\nmkdir -p \"$PG_EXTENSIONS_LIB64_PATH\"\n\nmkdir -p \"$PG_RELOCATED_PATH\"\n\nfor RELOCATE_PATH in \"$PG_BIN_PATH:$PG_RELOCATED_BIN_PATH\" \\\n  \"$PG_LIB_PATH:$PG_RELOCATED_LIB_PATH\" \\\n  \"$PG_SHARE_PATH:$PG_RELOCATED_SHARE_PATH\" \\\n  \"$PG_LIB64_PATH:$PG_RELOCATED_LIB64_PATH\"\ndo\n  if [ ! -f \"${RELOCATE_PATH#*:}/.done\" ]\n  then\n    echo \"Relocating ${RELOCATE_PATH%:*} to ${RELOCATE_PATH#*:} ...\"\n    mkdir -p \"${RELOCATE_PATH#*:}\"\n    rm -fr \"${RELOCATE_PATH#*:}\"\n    cp -a \"${RELOCATE_PATH%:*}\" \"${RELOCATE_PATH#*:}\"\n    chmod 700 \"${RELOCATE_PATH#*:}\" -R\n    touch \"${RELOCATE_PATH#*:}/.done\"\n    echo \"done.\"\n  else\n    echo \"${RELOCATE_PATH%:*} already relocated to ${RELOCATE_PATH#*:}, skipping\"\n  fi\ndone\n\nfor EXTENSION_CONTROL_FILE in \"$PG_EXTENSION_PATH\"/*.control\ndo\n  if ! [ -f \"$EXTENSION_CONTROL_FILE\" ]\n  then\n    continue\n  fi\n  EXTENSION_NAME=\"${EXTENSION_CONTROL_FILE%.*}\"\n  EXTENSION_NAME=\"${EXTENSION_NAME##*/}\"\n  echo \"Relocating $EXTENSION_CONTROL_FILE (and $EXTENSION_NAME--*.sql) to $PG_EXTENSIONS_EXTENSION_PATH/. ...\"\n  cp -a \"$EXTENSION_CONTROL_FILE\" \"${EXTENSION_CONTROL_FILE%.*}\"--*.sql \\\n    \"$PG_EXTENSIONS_EXTENSION_PATH/.\"\n  echo \"done.\"\ndone\n",
        "reset-patroni.sh": "#!/bin/sh\n\nset -e\n\nif [ \"$PRIMARY_INSTANCE\" = \"$POD_NAME\" ]\nthen\n  PATRONI_INIT=\"$(kubectl get endpoints -n \"$CLUSTER_NAMESPACE\" \"$PATRONI_ENDPOINT_NAME\" \\\n    --template='{{ if .metadata.annotations.initialize }}true{{ end }}')\"\n  if [ \"$PATRONI_INIT\" = \"true\" ]\n  then\n    echo \"Resetting patroni initialize\"\n    kubectl patch endpoints -n \"$CLUSTER_NAMESPACE\" \"$PATRONI_ENDPOINT_NAME\" \\\n      --type json -p '[{\"op\":\"remove\",\"path\":\"/metadata/annotations/initialize\"}]'\n  else\n    echo \"Patroni initialize already resetted\"\n  fi\nelse\n  echo \"Skip resetting patroni initialize\"\nfi\n",
        "run-dbops.sh": "#!/bin/sh\n\nLOCK_RESOURCE=\"$CLUSTER_CRD_NAME\"\nLOCK_RESOURCE_NAME=\"$CLUSTER_NAME\"\n\n. \"$LOCAL_BIN_SHELL_UTILS_PATH\"\n\nrun() {\n  set +e\n\n  touch \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n  touch \"$SHARED_PATH/$KEBAB_OP_NAME.err\"\n\n  sleep \"$TIMEOUT\" \u0026\n\n  TIMEOUT_PID=\"$!\"\n\n  tail -q -f \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \"$SHARED_PATH/$KEBAB_OP_NAME.err\" \u0026\n\n  TAIL_PID=\"$!\"\n\n  if [ \"$EXCLUSIVE_OP\" = true ]\n  then\n    (set -e; acquire_lock) \u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261 \u0026\n    ACQUIRE_LOCK_PID=$!\n    (\n    while (kill -0 \"$TIMEOUT_PID\" \\\n      \u0026\u0026 kill -0 \"$ACQUIRE_LOCK_PID\") 2\u003e/dev/null\n    do\n      true\n    done\n    )\n    if ! kill -0 \"$TIMEOUT_PID\" 2\u003e/dev/null\n    then\n      kill_with_childs \"$ACQUIRE_LOCK_PID\"\n      release_lock \u003e\u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261\n      echo \"Lock released\"\n      echo \"LOCK_LOST=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"TIMED_OUT=true\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"EXIT_CODE=1\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      kill_with_childs \"$TAIL_PID\"\n      return 0\n    else\n      wait \"$ACQUIRE_LOCK_PID\"\n      LOCK_ACQUIRED=\"$?\"\n      if [ \"$LOCK_ACQUIRED\" = 0 ]\n      then\n        echo \"Lock acquired\"\n        (set -e; maintain_lock) \u003e\u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261 \u0026\n        TRY_LOCK_PID=$!\n      else\n        kill_with_childs \"$TIMEOUT_PID\"\n        echo \"Can not acquire lock\"\n        echo \"LOCK_LOST=true\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n        echo \"TIMED_OUT=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n        echo \"EXIT_CODE=1\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n        kill_with_childs \"$TAIL_PID\"\n        return 0\n      fi\n    fi\n  fi\n\n  run_op \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\" 2\u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.err\" \u0026\n\n  PID=\"$!\"\n\n  (\n  while (kill -0 \"$PID\" \u0026\u0026 kill -0 \"$TIMEOUT_PID\" \\\n    \u0026\u0026 ([ \"$EXCLUSIVE_OP\" != true ] || kill -0 \"$TRY_LOCK_PID\")) 2\u003e/dev/null\n  do\n    true\n  done\n  )\n\n  if kill -0 \"$PID\" 2\u003e/dev/null\n  then\n    kill_with_childs \"$PID\"\n    if ! kill -0 \"$TIMEOUT_PID\" 2\u003e/dev/null\n    then\n      if [ \"$EXCLUSIVE_OP\" = true ]\n      then\n        kill_with_childs \"$TRY_LOCK_PID\"\n        release_lock \u003e\u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261\n        echo \"Lock released\"\n      fi\n      echo \"LOCK_LOST=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"TIMED_OUT=true\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"EXIT_CODE=1\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    elif [ \"$EXCLUSIVE_OP\" = true ] \u0026\u0026 ! kill -0 \"$TRY_LOCK_PID\" 2\u003e/dev/null\n    then\n      kill_with_childs \"$TIMEOUT_PID\"\n      echo \"LOCK_LOST=true\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"TIMED_OUT=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"EXIT_CODE=1\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    else\n      if [ \"$EXCLUSIVE_OP\" = true ]\n      then\n        kill_with_childs \"$TRY_LOCK_PID\"\n        release_lock \u003e\u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261\n        echo \"Lock released\"\n      fi\n      kill_with_childs \"$TIMEOUT_PID\"\n      echo \"LOCK_LOST=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"TIMED_OUT=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      echo \"EXIT_CODE=1\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    fi\n  else\n    kill_with_childs \"$TIMEOUT_PID\"\n    if [ \"$EXCLUSIVE_OP\" = true ]\n    then\n      kill_with_childs \"$TRY_LOCK_PID\"\n      release_lock \u003e\u003e \"$SHARED_PATH/try-lock\" 2\u003e\u00261\n      echo \"Lock released\"\n    fi\n    wait \"$PID\"\n    EXIT_CODE=\"$?\"\n    echo \"LOCK_LOST=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    echo \"TIMED_OUT=false\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    echo \"EXIT_CODE=$EXIT_CODE\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n  fi\n\n  kill_with_childs \"$TAIL_PID\"\n\n  return 0\n}\n\nif [ -n \"$RUN_SCRIPT_PATH\" ]\nthen\n  . \"$RUN_SCRIPT_PATH\"\nfi\n\nrun\n",
        "run-major-version-upgrade.sh": "#!/bin/sh\n\nrun_op() {\n  set -e\n\n\n  if [ \"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n    --template='{{ if .status.dbOps }}{{ if .status.dbOps.majorVersionUpgrade }}true{{ end }}{{ end }}')\" != \"true\" ]\n  then\n    set_first_statefulset_instance_as_primary\n\n    INITIAL_PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n    INITIAL_INSTANCES=\"$(printf '%s' \"$INITIAL_PODS\" | cut -d / -f 2 | sort)\"\n    PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n    if [ \"x$PRIMARY_INSTANCE\" = \"x\" ] \\\n      || ! kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -o name \u003e /dev/null\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Primary instance not found!\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      return 1\n    fi\n    echo \"Found primary instance $PRIMARY_INSTANCE\"\n    echo\n    SOURCE_VERSION=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template='{{ .spec.postgres.version }}')\"\n    LOCALE=\"$(kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" \\\n      -- psql -t -A -c \"SHOW lc_collate\")\"\n    ENCODING=\"$(kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" \\\n      -- psql -t -A -c \"SHOW server_encoding\")\"\n    DATA_CHECKSUM=\"$(kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" \\\n      -- psql -t -A -c \"SELECT CASE WHEN current_setting('data_checksums')::bool THEN 'true' ELSE 'false' END\")\"\n\n    if ! ([ -n \"${TARGET_VERSION}\" ] \u0026\u0026 [ \"${SOURCE_VERSION%%.*}\" -lt \"${TARGET_VERSION%%.*}\" ])\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Can not perform major version upgrade from version $SOURCE_VERSION to version $TARGET_VERSION\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      exit 1\n    fi\n\n    echo \"Signaling major version upgrade started to cluster\"\n    echo\n    DB_OPS_PATCH=\"$(cat \u003c\u003c EOF\n      {\n        \"dbOps\": {\n          \"majorVersionUpgrade\":{\n            \"initialInstances\": [$(\n              FIRST=true\n              for INSTANCE in $INITIAL_INSTANCES\n              do\n                if \"$FIRST\"\n                then\n                  printf '%s' \"\\\"$INSTANCE\\\"\"\n                  FIRST=false\n                else\n                  printf '%s' \",\\\"$INSTANCE\\\"\"\n                fi\n              done\n              )],\n            \"primaryInstance\": \"$PRIMARY_INSTANCE\",\n            \"sourcePostgresVersion\": \"$SOURCE_VERSION\",\n            \"targetPostgresVersion\": \"$TARGET_VERSION\",\n            \"locale\": \"$LOCALE\",\n            \"encoding\": \"$ENCODING\",\n            \"dataChecksum\": $DATA_CHECKSUM,\n            \"link\": $LINK,\n            \"clone\": $CLONE,\n            \"check\": $CHECK\n          }\n        }\n      }\nEOF\n    )\"\n\n    until (\n      DBOPS=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json)\"\n      DBOPS=\"$(printf '%s' \"$DBOPS\" | jq '.status |= . + '\"$DB_OPS_PATCH\")\"\n      printf '%s' \"$DBOPS\" | kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n      )\n    do\n      (\n        DBOPS=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json)\"\n        DBOPS=\"$(printf '%s' \"$DBOPS\" | jq 'del(.status.dbOps)')\"\n        printf '%s' \"$DBOPS\" | kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n      )\n    done\n  else\n    SOURCE_VERSION=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template='{{ .status.dbOps.majorVersionUpgrade.sourcePostgresVersion }}')\"\n    INITIAL_INSTANCES=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template='{{ .status.dbOps.majorVersionUpgrade.initialInstances }}')\"\n    INITIAL_INSTANCES=\"$(printf '%s' \"$INITIAL_INSTANCES\" | tr -d '[]' | tr ' ' '\\n')\"\n    PRIMARY_INSTANCE=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template='{{ .status.dbOps.majorVersionUpgrade.primaryInstance }}')\"\n\n    until kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n        -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/status/dbOps/majorVersionUpgrade/targetPostgresVersion\",\"value\": $TARGET_VERSION},\n  {\"op\":\"replace\",\"path\":\"/status/dbOps/majorVersionUpgrade/link\",\"value\": $LINK},\n  {\"op\":\"replace\",\"path\":\"/status/dbOps/majorVersionUpgrade/clone\",\"value\": $CLONE},\n  {\"op\":\"replace\",\"path\":\"/status/dbOps/majorVersionUpgrade/check\",\"value\": $CHECK}\n]\nEOF\n        )\"\n    do\n      sleep 1\n    done\n  fi\n\n  echo \"Waiting StatefulSet to be updated...\"\n  echo\n  while true\n  do\n    IS_STATEFULSET_UPDATED=\"$(kubectl get sts -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template=\"{{ range .spec.template.spec.initContainers }}{{ if eq .name \\\"$MAJOR_VERSION_UPGRADE_CONTAINER_NAME\\\" }}true{{ end }}{{ end }}\")\"\n    if [ \"$IS_STATEFULSET_UPDATED\" = \"true\" ]\n    then\n      break\n    fi\n    sleep 1\n  done\n  echo \"done\"\n  echo\n\n  echo \"Setting postgres version to $TARGET_VERSION and postgres config to $TARGET_POSTGRES_CONFIG...\"\n  echo\n  until (\n    CLUSTER=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json)\"\n    CLUSTER=\"$(printf '%s' \"$CLUSTER\" | jq '.spec.postgres.version = \"'\"$TARGET_VERSION\"'\"')\"\n    CLUSTER=\"$(printf '%s' \"$CLUSTER\" | jq '.spec.configurations.sgPostgresConfig = \"'\"$TARGET_POSTGRES_CONFIG\"'\"')\"\n    printf '%s' \"$CLUSTER\" | kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\" -f -\n    )\n  do\n    sleep 1\n  done\n  echo \"done\"\n  echo\n\n  INITIAL_INSTANCES_COUNT=\"$(printf '%s' \"$INITIAL_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  echo \"Initial instances:\"\n  echo \"$INITIAL_INSTANCES\" | sed 's/^/ - /'\n  echo\n\n  update_status init\n\n  RETRY=3\n  while [ \"$RETRY\" != 0 ]\n  do\n    CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    if [ -n \"$CURRENT_PRIMARY_POD\" ]\n    then\n      break\n    fi\n    RETRY=\"$((RETRY-1))\"\n    sleep 5\n  done\n  CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n  if [ \"${PRIMARY_INSTANCE##*-}\" != \"0\" ] \\\n     || ( [ \"x$CURRENT_PRIMARY_INSTANCE\" != \"x\" ] \\\n        \u0026\u0026 [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ] )\n  then\n    echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $PRIMARY_INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    exit 1\n  fi\n\n  if [ \"$INITIAL_INSTANCES_COUNT\" -gt 1 ]\n  then\n    echo \"Downscaling cluster to 1 instance\"\n    create_event \"DecreasingInstances\" \"Normal\" \"Decreasing instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n    echo\n\n    kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n      -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/spec/instances\",\"value\":1}\n]\nEOF\n        )\"\n\n    echo \"Waiting cluster downscale...\"\n\n    until [ \"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name | cut -d / -f 2)\" = \"$PRIMARY_INSTANCE\" ]\n    do\n      sleep 1\n    done\n\n    echo \"done\"\n    create_event \"InstancesDecreased\" \"Normal\" \"Decreased instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n    echo\n  fi\n\n  echo \"Restarting primary instance $PRIMARY_INSTANCE...\"\n  create_event \"RestartingPod\" \"Normal\" \"Restarting pod $PRIMARY_INSTANCE\"\n\n  kubectl delete pod -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\"\n\n  echo \"done\"\n  echo\n\n  echo \"Waiting primary instance $PRIMARY_INSTANCE major version upgrade...\"\n\n  wait_for_instance \"$PRIMARY_INSTANCE\"\n  create_event \"PodRestarted\" \"Normal\" \"Pod $PRIMARY_INSTANCE successfully restarted\"\n\n  CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n  CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n  if [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ]\n  then\n    echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $PRIMARY_INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    return 1\n  fi\n\n  echo \"done\"\n  echo\n\n  update_status\n\n  if [ \"$INITIAL_INSTANCES_COUNT\" -gt 1 ]\n  then\n    echo \"Upscaling cluster to $INITIAL_INSTANCES_COUNT instances\"\n    create_event \"IncreasingInstances\" \"Normal\" \"Increasing instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n    echo\n\n    kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n      -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/spec/instances\",\"value\":$INITIAL_INSTANCES_COUNT}\n]\nEOF\n        )\"\n\n    echo \"Waiting cluster upscale\"\n    echo\n\n    for INSTANCE in $INITIAL_INSTANCES\n    do\n      if [ \"$INSTANCE\" = \"$PRIMARY_INSTANCE\" ]\n      then\n        continue\n      fi\n\n      echo \"Waiting instance $INSTANCE to become ready...\"\n\n      wait_for_instance \"$INSTANCE\"\n\n      echo \"done\"\n      echo\n\n      update_status\n    done\n\n    echo \"Cluster upscale done\"\n    create_event \"InstancesIncreased\" \"Normal\" \"Increased instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n    echo\n  fi\n\n  echo \"Signaling major version upgrade finished to cluster\"\n  echo\n\n  until kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json | \\\n      jq 'del(.status.dbOps)' | \\\n      kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n  do\n    sleep 1\n  done\n}\n\nupdate_status() {\n  STATEFULSET_UPDATE_REVISION=\"$(kubectl get sts -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n    --template='{{ .status.updateRevision }}')\"\n  if [ \"$1\" = \"init\" ]\n  then\n    PENDING_TO_RESTART_INSTANCES=\"$INITIAL_INSTANCES\"\n  else\n    PENDING_TO_RESTART_INSTANCES=\"$(echo \"$INITIAL_INSTANCES\" \\\n      | while read INSTANCE\n        do\n          PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n          if ! printf '%s' \"$PODS\" | cut -d / -f 2 | grep -q \"^$INSTANCE$\"\n          then\n            echo \"$INSTANCE\"\n            continue\n          fi\n          PATRONI_STATUS=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" \\\n            --template='{{ .metadata.annotations.status }}')\"\n          POD_STATEFULSET_REVISION=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" \\\n            --template='{{ index .metadata.labels \"controller-revision-hash\" }}')\"\n          if [ \"$STATEFULSET_UPDATE_REVISION\" != \"$POD_STATEFULSET_REVISION\" ] \\\n            || echo \"$PATRONI_STATUS\" | grep -q '\"pending_restart\":true'\n          then\n            echo \"$INSTANCE\"\n          fi\n        done)\"\n  fi\n  PENDING_TO_RESTART_INSTANCES_COUNT=\"$(echo \"$PENDING_TO_RESTART_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  EXISTING_PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n  RESTARTED_INSTANCES=\"$(echo \"$EXISTING_PODS\" | cut -d / -f 2 | grep -v \"^\\($(\n      echo \"$PENDING_TO_RESTART_INSTANCES\" | tr '\\n' ' ' | sed '{s/ $//;s/ /\\\\|/g}'\n    )\\)$\" | sort)\"\n  RESTARTED_INSTANCES_COUNT=\"$(echo \"$RESTARTED_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  echo \"Pending to $NORMALIZED_OP_NAME instances:\"\n  if [ \"$PENDING_TO_RESTART_INSTANCES_COUNT\" = 0 ]\n  then\n    echo '\u003cnone\u003e'\n  else\n    echo \"$PENDING_TO_RESTART_INSTANCES\" | sed 's/^/ - /'\n  fi\n  echo\n\n  OPERATION=\"$(kubectl get \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" \\\n    --template='{{ if .status.majorVersionUpgrade }}replace{{ else }}add{{ end }}')\"\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=json \\\n    -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"$OPERATION\",\"path\":\"/status/majorVersionUpgrade\",\"value\":{\n      \"sourcePostgresVersion\": \"$SOURCE_VERSION\",\n      \"targetPostgresVersion\": \"$TARGET_VERSION\",\n      \"primaryInstance\": \"$PRIMARY_INSTANCE\",\n      \"initialInstances\": [$(\n        FIRST=true\n        for INSTANCE in $INITIAL_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )],\n      \"pendingToRestartInstances\": [$(\n        FIRST=true\n        for INSTANCE in $PENDING_TO_RESTART_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )],\n      \"restartedInstances\": [$(\n        FIRST=true\n        for INSTANCE in $RESTARTED_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )]\n    }\n  }\n]\nEOF\n    )\"\n}\n\nset_first_statefulset_instance_as_primary() {\n  PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n  PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n  if [ \"${PRIMARY_INSTANCE##*-}\" != \"0\" ]\n  then\n    TARGET_INSTANCE=\"${PRIMARY_INSTANCE%-*}-0\"\n    echo \"Primary is not $TARGET_INSTANCE, doing switchover...\"\n    echo\n\n    if [ \"$INITIAL_INSTANCES_COUNT\" = 1 ]\n    then\n      echo \"Upscaling cluster to 2 instances\"\n      echo\n\n      create_event \"IncreasingInstances\" \"Normal\" \"Increasing instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n\n      kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n        -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/spec/instances\",\"value\":2}\n]\nEOF\n          )\"\n\n      echo \"Waiting cluster upscale\"\n      echo\n\n      echo \"Waiting instance $INSTANCE to become ready...\"\n\n      wait_for_instance \"$TARGET_INSTANCE\"\n\n      create_event \"InstancesIncreased\" \"Normal\" \"Increased instances of $CLUSTER_CRD_NAME $CLUSTER_NAME\"\n\n      echo \"done\"\n      echo\n\n      echo \"Cluster upscale done\"\n      echo\n    fi\n    PREVIOUS_PRIMARY_INSTANCE=\"$PRIMARY_INSTANCE\"\n    if ! kubectl wait pod -n \"$CLUSTER_NAMESPACE\" \"$TARGET_INSTANCE\" --for condition=Ready --timeout 0 \u003e/dev/null 2\u003e\u00261\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Primary instance not found!\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      exit 1\n    fi\n\n    echo \"Performing switchover from primary $PRIMARY_INSTANCE to replica $TARGET_INSTANCE...\"\n\n    create_event \"SwitchoverInitiated\" \"Normal\" \"Switchover of $CLUSTER_CRD_NAME $CLUSTER_NAME initiated\"\n\n    kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" -- \\\n      patronictl switchover \"$CLUSTER_NAME\" --master \"$PRIMARY_INSTANCE\" --candidate \"$TARGET_INSTANCE\" --force \\\n\n    echo \"done\"\n\n    create_event \"SwitchoverFinalized\" \"Normal\" \"Switchover of $CLUSTER_CRD_NAME $CLUSTER_NAME completed\"\n    echo\n\n    PRIMARY_INSTANCE=\"$TARGET_INSTANCE\"\n\n    echo \"Waiting primary instance $PRIMARY_INSTANCE to be ready...\"\n\n    wait_for_instance \"$PRIMARY_INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    RETRY=3\n    while [ \"$RETRY\" != 0 ]\n    do\n      CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n      if [ -n \"$CURRENT_PRIMARY_POD\" ]\n      then\n        break\n      fi\n      RETRY=\"$((RETRY-1))\"\n      sleep 5\n    done\n    CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n    if [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ]\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $PRIMARY_INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      exit 1\n    fi\n  fi\n}\n\nwait_for_instance() {\n  local INSTANCE=\"$1\"\n  until kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" -o name \u003e/dev/null 2\u003e\u00261\n  do\n    sleep 1\n  done\n  until kubectl wait pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" --for condition=Ready --timeout 0 \u003e/dev/null 2\u003e\u00261\n  do\n    PHASE=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" --template='{{ .status.phase }}')\"\n    if [ \"$PHASE\" = \"Failed\" ] || [ \"$PHASE\" = \"Unknown\" ]\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      return 1\n    fi\n    sleep 1\n  done\n}\n",
        "run-pgbench.sh": "#!/bin/sh\n\nDATABASE_NAME=\"pgbench_$(printf '%x' \"$(date +%s)\")\"\n\nrun_op() {\n  set +e\n\n  (\n  set -e\n\n  run_pgbench\n  )\n\n  EXIT_CODE=\"$?\"\n\n  try_drop_pgbench_database\n\n  return \"$EXIT_CODE\"\n}\n\nrun_pgbench() {\n  (\n  export PGHOST=\"$PRIMARY_PGHOST\"\n\n  DATABASE_EXISTS=\"$(psql -t -A \\\n    -c \"SELECT EXISTS (SELECT * FROM pg_database WHERE datname = '$DATABASE_NAME')\")\"\n  if [ \"$DATABASE_EXISTS\" != 'f' ]\n  then\n    try_drop_pgbench_database\n  fi\n\n  if MESSAGE=\"$(psql -c \"CREATE DATABASE $DATABASE_NAME\" 2\u003e\u00261)\"\n  then\n    echo \"$MESSAGE\"\n    create_event \"DatabaseCreated\" \"Normal\" \"Database $DATABASE_NAME created\"\n  else\n    create_event \"CreateDatabaseFailed\" \"Warning\" \"Can not create database $DATABASE_NAME: $MESSAGE\"\n    return 1\n  fi\n\n  create_event \"BenchmarkInitializationStarted\" \"Normal\" \"Benchamrk initialization started\"\n  if MESSAGE=\"$(pgbench -s \"$SCALE\" -i \"$DATABASE_NAME\" 2\u003e\u00261)\"\n  then\n    echo \"$MESSAGE\"\n    create_event \"BenchmarkInitialized\" \"Normal\" \"Benchamrk initialized\"\n  else\n    create_event \"BenchmarkInitializationFailed\" \"Warning\" \"Can not initialize benchmark: $MESSAGE\"\n    return 1\n  fi\n  )\n\n  if \"$READ_WRITE\"\n  then\n    create_event \"BenchmarkStarted\" \"Normal\" \"Benchamrk started\"\n    if MESSAGE=\"$(pgbench -M \"$PROTOCOL\" -s \"$SCALE\" -T \"$DURATION\" -c \"$CLIENTS\" -j \"$JOBS\" -r -P 1 -d \"$DATABASE_NAME\" 2\u003e\u00261)\"\n    then\n      echo \"$MESSAGE\"\n      create_event \"BenchmarkCompleted\" \"Normal\" \"Benchmark completed\"\n    else\n      create_event \"BenchmarkFailed\" \"Warning\" \"Can not complete benchmark: $MESSAGE\"\n      return 1\n    fi\n  else\n    create_event \"BenchmarkPostInitializationStarted\" \"Normal\" \"Benchamrk post initialization started\"\n    PGBENCH_ACCOUNTS_COUNT=\"$(PGHOST=\"$PRIMARY_PGHOST\" psql -t -A -d \"$DATABASE_NAME\" \\\n      -c \"SELECT COUNT(*) FROM pgbench_accounts\")\"\n\n    until [ \"$(psql -t -A -d \"$DATABASE_NAME\" \\\n      -c \"SELECT COUNT(*) FROM pgbench_accounts\")\" = \"$PGBENCH_ACCOUNTS_COUNT\" ]\n    do\n      sleep 1\n    done\n    create_event \"BenchmarkPostInitializationCompleted\" \"Normal\" \"Benchamrk post initialization completed\"\n\n    create_event \"BenchmarkStarted\" \"Normal\" \"Benchamrk started\"\n    if MESSAGE=\"$(pgbench -b \"select-only\" -M \"$PROTOCOL\" -s \"$SCALE\" -T \"$DURATION\" -c \"$CLIENTS\" -j \"$JOBS\" -r -P 1 -d \"$DATABASE_NAME\" 2\u003e\u00261)\"\n    then\n      echo \"$MESSAGE\"\n      create_event \"BenchmarkCompleted\" \"Normal\" \"Benchmark completed\"\n    else\n      create_event \"BenchmarkFailed\" \"Warning\" \"Can not complete benchmark: $MESSAGE\"\n      return 1\n    fi\n  fi\n}\n\ntry_drop_pgbench_database() {\n  (\n  set +e\n  DROP_RETRY=3\n  while [ \"$DROP_RETRY\" -ge 0 ]\n  do\n      if MESSAGE=\"$(psql \\\n        -c \"SELECT pg_cancel_backend(pid) FROM pg_stat_activity WHERE datname = '$DATABASE_NAME' AND pid != pg_backend_pid()\" \\\n        -c \"DROP DATABASE $DATABASE_NAME\" 2\u003e\u00261)\"\n    then\n      break\n    fi\n    create_event \"DropDatabaseFailed\" \"Warning\" \"Can not drop $DATABASE_NAME database: $MESSAGE\"\n    DROP_RETRY=\"$((DROP_RETRY - 1))\"\n    sleep 3\n  done\n  )\n}\n\n",
        "run-repack.sh": "#!/bin/sh\n\nrun_op() {\n  set -e\n\n  if [ -z \"$DATABASES\" ]\n  then\n    run_command -a\n  else\n    echo \"$DATABASES\" | while read CONFIG DATABASE\n      do\n        eval \"$CONFIG\"\n        run_command -d \"$DATABASE\"\n      done\n  fi\n}\n\nrun_command() {\n  if \"$NO_ORDER\"\n  then\n    COMMAND=\"$COMMAND\"' -n'\n  fi\n  if [ -n \"$WAIT_TIMEOUT\" ]\n  then\n    COMMAND=\"$COMMAND\"\" -T $WAIT_TIMEOUT\"\n  fi\n  if \"$NO_KILL_BACKEND\"\n  then\n    COMMAND=\"$COMMAND\"' -D'\n  fi\n  if \"$NO_ANALYZE\"\n  then\n    COMMAND=\"$COMMAND\"' -Z'\n  fi\n  if \"$EXCLUDE_EXTENSION\"\n  then\n    COMMAND=\"$COMMAND\"' -C'\n  fi\n  get_primary_instance\n  kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" -- pg_repack $ARGS \"$@\"\n}\n\nget_primary_instance() {\n  PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n  PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n  until kubectl wait pod -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" --for condition=Ready --timeout 0 \u003e/dev/null 2\u003e\u00261\n  do\n    sleep 1\n    PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n  done\n}\n",
        "run-restart.sh": "#!/bin/sh\n\nrun_op() {\n  set -e\n\n  START_TIMESTAMP=\"$(date +%s)\"\n\n  if [ \"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n    --template=\"{{ if .status.dbOps }}{{ if .status.dbOps.$OP_NAME }}true{{ end }}{{ end }}\")\" != \"true\" ]\n  then\n    INITIAL_PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n    INITIAL_INSTANCES=\"$(printf '%s' \"$INITIAL_PODS\" | cut -d / -f 2 | sort)\"\n    PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n\n    echo \"Signaling $NORMALIZED_OP_NAME started to cluster\"\n    echo\n\n    DB_OPS_PATCH=\"$(cat \u003c\u003c EOF\n      {\n        \"dbOps\": {\n          \"$OP_NAME\":{\n            \"initialInstances\": [$(\n              FIRST=true\n              for INSTANCE in $INITIAL_INSTANCES\n              do\n                if \"$FIRST\"\n                then\n                  printf '%s' \"\\\"$INSTANCE\\\"\"\n                  FIRST=false\n                else\n                  printf '%s' \",\\\"$INSTANCE\\\"\"\n                fi\n              done\n              )],\n            \"primaryInstance\": \"$PRIMARY_INSTANCE\"\n          }\n        }\n      }\nEOF\n    )\"\n    until (\n      DBOPS=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json)\"\n      DBOPS=\"$(printf '%s' \"$DBOPS\" | jq '.status |= . + '\"$DB_OPS_PATCH\")\"\n      printf '%s' \"$DBOPS\" | kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n      )\n    do\n      (\n      DBOPS=\"$(printf '%s' \"$DBOPS\" | kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json)\"\n      DBOPS=\"$(printf '%s' \"$DBOPS\" | jq 'del(.status.dbOps)')\"\n      printf '%s' \"$DBOPS\" | kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n      )\n    done\n\n  else\n    INITIAL_INSTANCES=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template=\"{{ .status.dbOps.$OP_NAME.initialInstances }}\")\"\n    INITIAL_INSTANCES=\"$(printf '%s' \"$INITIAL_INSTANCES\" | tr -d '[]' | tr ' ' '\\n')\"\n    PRIMARY_INSTANCE=\"$(kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n      --template=\"{{ .status.dbOps.$OP_NAME.primaryInstance }}\")\"\n\n    CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n    if [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ]\n    then\n      PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n      PRIMARY_INSTANCE=\"$(printf '%s' \"$PRIMARY_POD\" | cut -d / -f 2)\"\n    fi\n  fi\n\n  if ! kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -o name \u003e /dev/null\n  then\n    echo FAILURE=\"Primary instance not found!\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n    return 1\n  fi\n  echo \"Found primary instance $PRIMARY_INSTANCE\"\n  echo\n\n  INITIAL_INSTANCES_COUNT=\"$(printf '%s' \"$INITIAL_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  echo \"Initial instances:\"\n  if [ \"$INITIAL_INSTANCES_COUNT\" = 0 ]\n  then\n    echo '\u003cnone\u003e'\n  else\n    echo \"$INITIAL_INSTANCES\" | sed 's/^/ - /'\n  fi\n  echo\n\n  update_status\n\n  if [ \"$RESTART_PRIMARY_FIRST\" = \"true\" ]\n  then\n    echo \"Restarting primary inscante first...\"\n\n    kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" -- \\\n      patronictl restart \"$CLUSTER_NAME\" -r master --force \\\n\n    echo \"Waiting primary instance $PRIMARY_INSTANCE to be ready...\"\n\n    wait_for_instance \"$PRIMARY_INSTANCE\"\n\n    CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n    CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n    if [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ]\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $PRIMARY_INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      return 1\n    fi\n\n    echo \"done\"\n    echo\n\n    update_status\n  fi\n\n  if [ \"$REDUCED_IMPACT\" = \"true\" ]\n  then\n    echo \"Upscaling cluster to $((INITIAL_INSTANCES_COUNT + 1)) instances\"\n    echo\n\n    kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n      -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/spec/instances\",\"value\":$((INITIAL_INSTANCES_COUNT + 1))}\n]\nEOF\n        )\"\n\n    INSTANCE=\"${PRIMARY_INSTANCE%-*}-$INITIAL_INSTANCES_COUNT\"\n\n    echo \"Waiting instance $INSTANCE to be ready...\"\n\n    wait_for_instance \"$INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    update_status\n  fi\n\n  echo \"Restarting replicas\"\n  echo\n\n  while true\n  do\n    if [ \"$PENDING_TO_RESTART_REPLICAS_COUNT\" = 0 ]\n    then\n      break\n    fi\n\n    INSTANCE=\"$(printf '%s' \"$PENDING_TO_RESTART_REPLICAS\" | head -n 1)\"\n\n    echo \"Restarting instance $INSTANCE...\"\n\n    kubectl delete pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    echo \"Waiting instance $INSTANCE to be ready...\"\n\n    wait_for_instance \"$INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    update_status\n  done\n\n  if echo \"$PENDING_TO_RESTART_INSTANCES\" | grep -q '^'\"$PRIMARY_INSTANCE\"'$'\n  then\n    PREVIOUS_PRIMARY_INSTANCE=\"$PRIMARY_INSTANCE\"\n    TARGET_INSTANCE=\"$(echo \"$RESTARTED_REPLICAS\" | head -n 1)\"\n    if kubectl wait pod -n \"$CLUSTER_NAMESPACE\" \"$TARGET_INSTANCE\" --for condition=Ready --timeout 0 \u003e/dev/null 2\u003e\u00261\n    then\n      echo \"Performing switchover from primary $PRIMARY_INSTANCE to replica $TARGET_INSTANCE...\"\n\n      kubectl exec -n \"$CLUSTER_NAMESPACE\" \"$PRIMARY_INSTANCE\" -c \"$PATRONI_CONTAINER_NAME\" -- \\\n        patronictl switchover \"$CLUSTER_NAME\" --master \"$PRIMARY_INSTANCE\" --candidate \"$TARGET_INSTANCE\" --force \\\n\n      echo \"done\"\n      echo\n\n      PRIMARY_INSTANCE=\"$TARGET_INSTANCE\"\n\n      echo \"Waiting primary instance $PRIMARY_INSTANCE to be ready...\"\n\n      wait_for_instance \"$PRIMARY_INSTANCE\"\n\n      echo \"done\"\n      echo\n\n      RETRY=3\n      while [ \"$RETRY\" != 0 ]\n      do\n        CURRENT_PRIMARY_POD=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_PRIMARY_POD_LABELS\" -o name)\"\n        if [ -n \"$CURRENT_PRIMARY_POD\" ]\n        then\n          break\n        fi\n        RETRY=\"$((RETRY-1))\"\n        sleep 5\n      done\n      CURRENT_PRIMARY_INSTANCE=\"$(printf '%s' \"$CURRENT_PRIMARY_POD\" | cut -d / -f 2)\"\n      if [ \"$PRIMARY_INSTANCE\" != \"$CURRENT_PRIMARY_INSTANCE\" ]\n      then\n        echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $PRIMARY_INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n        return 1\n      fi\n    fi\n\n    if [ \"$PREVIOUS_PRIMARY_INSTANCE\" = \"$PRIMARY_INSTANCE\" ]\n    then\n      echo \"Restarting instance $PREVIOUS_PRIMARY_INSTANCE...\"\n    else\n      echo \"Restarting primary instance $PREVIOUS_PRIMARY_INSTANCE...\"\n    fi\n\n    kubectl delete pod -n \"$CLUSTER_NAMESPACE\" \"$PREVIOUS_PRIMARY_INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    if [ \"$PREVIOUS_PRIMARY_INSTANCE\" = \"$PRIMARY_INSTANCE\" ]\n    then\n      echo \"Waiting instance $PREVIOUS_PRIMARY_INSTANCE to be ready...\"\n    else\n      echo \"Waiting primary instance $PREVIOUS_PRIMARY_INSTANCE to be ready...\"\n    fi\n\n    wait_for_instance \"$PREVIOUS_PRIMARY_INSTANCE\"\n\n    echo \"done\"\n    echo\n\n    update_status\n  fi\n\n  if [ \"$REDUCED_IMPACT\" = \"true\" ]\n  then\n    echo \"Downscaling cluster to $INITIAL_INSTANCES_COUNT instances\"\n    echo\n\n    kubectl patch \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" --type=json \\\n      -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/spec/instances\",\"value\":$INITIAL_INSTANCES_COUNT}\n]\nEOF\n        )\"\n\n    echo \"Waiting cluster downscale...\"\n\n    until [ \"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name | cut -d / -f 2 | sort)\" = \"$INITIAL_INSTANCES\" ]\n    do\n      sleep 1\n    done\n\n    echo \"done\"\n    echo\n  fi\n\n  echo \"Signaling $NORMALIZED_OP_NAME finished to cluster\"\n  echo\n\n  until kubectl get \"$CLUSTER_CRD_NAME.$CRD_GROUP\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" -o json | \\\n    jq 'del(.status.dbOps)' | \\\n    kubectl replace --raw /apis/\"$CRD_GROUP\"/v1/namespaces/\"$CLUSTER_NAMESPACE\"/\"$CLUSTER_CRD_NAME\"/\"$CLUSTER_NAME\"/status -f -\n  do\n    sleep 1\n  done\n}\n\nupdate_status() {\n  STATEFULSET_UPDATE_REVISION=\"$(kubectl get sts -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n    --template='{{ .status.updateRevision }}')\"\n  PENDING_TO_RESTART_INSTANCES=\"$(echo \"$INITIAL_INSTANCES\" \\\n    | while read -r INSTANCE\n      do\n        PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n        if ! printf '%s' \"$PODS\" | cut -d / -f 2 | grep -q \"^$INSTANCE$\"\n        then\n          echo \"$INSTANCE\"\n          continue\n        fi\n        if [ \"$ONLY_PENDING_RESTART\" != true ]\n        then\n          POD_CREATION_TIMESTAMP=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" \\\n            --template '{{ .metadata.creationTimestamp }}')\"\n          POD_CREATION_TIMESTAMP=\"$(from_date_iso8601_to_unix_timestamp \"$POD_CREATION_TIMESTAMP\")\"\n          if [ \"$POD_CREATION_TIMESTAMP\" -lt \"$START_TIMESTAMP\" ]\n          then\n            echo \"$INSTANCE\"\n          fi\n        else\n          CLUSTER_POD_STATUS=\"$(kubectl get \"$CLUSTER_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$CLUSTER_NAME\" \\\n            -o=jsonpath='{ .status.podStatuses[?(@.name == \"'\"$INSTANCE\"'\")].pendingRestart }')\"\n          PATRONI_STATUS=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" \\\n            --template='{{ .metadata.annotations.status }}')\"\n          POD_STATEFULSET_REVISION=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" \\\n            --template='{{ index .metadata.labels \"controller-revision-hash\" }}')\"\n          if [ \"$CLUSTER_POD_STATUS\" = true ] \\\n            || echo \"$PATRONI_STATUS\" | grep -q '\"pending_restart\":true' \\\n            || [ \"$STATEFULSET_UPDATE_REVISION\" != \"$POD_STATEFULSET_REVISION\" ]\n          then\n            echo \"$INSTANCE\"\n          fi\n        fi\n      done)\"\n  PENDING_TO_RESTART_INSTANCES_COUNT=\"$(echo \"$PENDING_TO_RESTART_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  PENDING_TO_RESTART_REPLICAS=\"$(printf '%s' \"$PENDING_TO_RESTART_INSTANCES\" | grep -v '^'\"$PRIMARY_INSTANCE\"'$' || true)\"\n  PENDING_TO_RESTART_REPLICAS_COUNT=\"$(printf '%s' \"$PENDING_TO_RESTART_REPLICAS\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  EXISTING_PODS=\"$(kubectl get pods -n \"$CLUSTER_NAMESPACE\" -l \"$CLUSTER_POD_LABELS\" -o name)\"\n  RESTARTED_INSTANCES=\"$(echo \"$EXISTING_PODS\" | cut -d / -f 2 | grep -v \"^\\($(\n      echo \"$PENDING_TO_RESTART_INSTANCES\" | tr '\\n' ' ' | sed '{s/ $//;s/ /\\\\|/g}'\n    )\\)$\" | sort)\"\n  RESTARTED_INSTANCES_COUNT=\"$(echo \"$RESTARTED_INSTANCES\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  RESTARTED_REPLICAS=\"$(echo \"$RESTARTED_INSTANCES\" | grep -v '^'\"$PRIMARY_INSTANCE\"'$' || true)\"\n  RESTARTED_REPLICAS_COUNT=\"$(echo \"$RESTARTED_REPLICAS\" | tr ' ' 's' | tr '\\n' ' ' | wc -w)\"\n  echo \"Pending to $NORMALIZED_OP_NAME instances:\"\n  if [ \"$PENDING_TO_RESTART_INSTANCES_COUNT\" = 0 ]\n  then\n    echo '\u003cnone\u003e'\n  else\n    echo \"$PENDING_TO_RESTART_INSTANCES\" | sed 's/^/ - /'\n  fi\n  echo\n\n  OPERATION=\"$(kubectl get \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" \\\n    --template=\"{{ if .status.$OP_NAME }}replace{{ else }}add{{ end }}\")\"\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=json \\\n    -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"$OPERATION\",\"path\":\"/status/$OP_NAME\",\"value\":{\n      \"primaryInstance\": \"$PRIMARY_INSTANCE\",\n      \"initialInstances\": [$(\n        FIRST=true\n        for INSTANCE in $INITIAL_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )],\n      \"pendingToRestartInstances\": [$(\n        FIRST=true\n        for INSTANCE in $PENDING_TO_RESTART_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )],\n      \"restartedInstances\": [$(\n        FIRST=true\n        for INSTANCE in $RESTARTED_INSTANCES\n        do\n          if \"$FIRST\"\n          then\n            printf '%s' \"\\\"$INSTANCE\\\"\"\n            FIRST=false\n          else\n            printf '%s' \",\\\"$INSTANCE\\\"\"\n          fi\n        done\n        )]\n    }\n  }\n]\nEOF\n    )\"\n}\n\nwait_for_instance() {\n  local INSTANCE=\"$1\"\n  until kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" -o name \u003e/dev/null 2\u003e\u00261\n  do\n    sleep 1\n  done\n  until kubectl wait pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" --for condition=Ready --timeout 0 \u003e/dev/null 2\u003e\u00261\n  do\n    PHASE=\"$(kubectl get pod -n \"$CLUSTER_NAMESPACE\" \"$INSTANCE\" --template='{{ .status.phase }}')\"\n    if [ \"$PHASE\" = \"Failed\" ] || [ \"$PHASE\" = \"Unknown\" ]\n    then\n      echo \"FAILURE=$NORMALIZED_OP_NAME failed. Please check pod $INSTANCE logs for more info\" \u003e\u003e \"$SHARED_PATH/$KEBAB_OP_NAME.out\"\n      return 1\n    fi\n    sleep 1\n  done\n}\n",
        "run-vacuum.sh": "#!/bin/sh\n\nrun_op() {\n  set -e\n  \n  if [ -z \"$DATABASES\" ]\n  then\n    run_command -a\n  else\n    echo \"$DATABASES\" | while read CONFIG DATABASE\n      do\n        eval \"$CONFIG\"\n        run_command -d \"$DATABASE\"\n      done\n  fi\n}\n\nrun_command() {\n  COMMAND='vacuumdb -v'\n  if \"$FULL\"\n  then\n    COMMAND=\"$COMMAND\"' -f'\n  else\n    if \"$FREEZE\"\n    then\n      COMMAND=\"$COMMAND\"' -F'\n    fi\n    if \"$DISABLE_PAGE_SKIPPING\"\n    then\n      COMMAND=\"$COMMAND\"' --disable-page-skipping'\n    fi\n  fi\n  if \"$ANALYZE\"\n  then\n    COMMAND=\"$COMMAND\"' -z'\n  fi\n  $COMMAND \"$@\"\n}\n",
        "set-dbops-result.sh": "#!/bin/sh\n\n. \"$LOCAL_BIN_SHELL_UTILS_PATH\"\n\neval_in_place() {\neval \"cat \u003c\u003c EVAL_IN_PLACE_EOF\n$*\nEVAL_IN_PLACE_EOF\n\"\n}\n\nset_completed() {\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n    -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_FAILED\")\n    ]\n  }\n}\nEOF\n    )\"\n  create_event \"DbOpCompleted\" \"Normal\" \"Database operation $OP_NAME completed\"\n}\n\nset_timed_out() {\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n    -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_TIMED_OUT\")\n    ]\n  }\n}\nEOF\n    )\"\n\n  create_event \"DbOpTimeOut\" \"Warning\" \"Database operation $OP_NAME timed out\"\n}\n\nset_lock_lost() {\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n    -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_LOCK_LOST\")\n    ]\n  }\n}\nEOF\n    )\"\n}\n\nset_failed() {\n  if [ -z \"$FAILURE\" ]\n  then\n    kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n      -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FAILED\")\n    ]\n  }\n}\nEOF\n      )\"\n\n    create_event \"DbOpFailed\" \"Warning\" \"Database operation $OP_NAME failed\"\n  else\n    kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n      -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FAILED\")\n    ],\n    \"$OP_NAME\": {\n      \"failure\": $FAILURE\n    }\n  }\n}\nEOF\n      )\"\n\n    create_event \"DbOpFailed\" \"Warning\" \"Database operation $OP_NAME failed: $FAILURE\"\n  fi\n}\n\nset_result() {\n  until grep -q '^EXIT_CODE=' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" 2\u003e/dev/null\n  do\n    sleep 1\n  done\n\n  EXIT_CODE=\"$(grep '^EXIT_CODE=' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" | cut -d = -f 2)\"\n  TIMED_OUT=\"$(grep '^TIMED_OUT=' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" | cut -d = -f 2)\"\n  LOCK_LOST=\"$(grep '^LOCK_LOST=' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" | cut -d = -f 2)\"\n  FAILURE=\"$(grep '^FAILURE=' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" | cut -d = -f 2 | sed 's/^\\(.*\\)$/\"\\1\"/')\"\n  LAST_TRANSITION_TIME=\"$(date_iso8601)\"\n\n  if [ \"$EXIT_CODE\" = 0 ]\n  then\n    set_completed\n  elif [ \"$TIMED_OUT\" = \"true\" ]\n  then\n    set_timed_out\n  elif [ \"$LOCK_LOST\" = \"true\" ]\n  then\n    set_lock_lost\n  else\n    set_failed\n  fi\n\n  return \"$EXIT_CODE\"\n}\n\nif [ -n \"$SET_RESULT_SCRIPT_PATH\" ]\nthen\n  . \"$SET_RESULT_SCRIPT_PATH\"\nfi\n\nset_result\n",
        "set-dbops-running.sh": "#!/bin/sh\n\n. \"$LOCAL_BIN_SHELL_UTILS_PATH\"\n\neval_in_place() {\neval \"cat \u003c\u003c EVAL_IN_PLACE_EOF\n$*\nEVAL_IN_PLACE_EOF\n\"\n}\n\nLAST_TRANSITION_TIME=\"$(date_iso8601)\"\nSTARTED=\"$LAST_TRANSITION_TIME\"\n\nkubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=merge \\\n  -p \"$(cat \u003c\u003c EOF\n{\n  \"status\": {\n    \"conditions\":[\n        $(eval_in_place \"$CONDITION_DB_OPS_RUNNING\"),\n        $(eval_in_place \"$CONDITION_DB_OPS_FALSE_COMPLETED\"),\n        $(eval_in_place \"$CONDITION_DB_OPS_FALSE_FAILED\")\n    ],\n    \"opRetries\": $CURRENT_RETRY,\n    \"opStarted\": \"$STARTED\"\n  }\n}\nEOF\n    )\"\n\ncreate_event \"DbOpStarted\" \"Normal\" \"Database operation $OP_NAME started\"\n",
        "set-pgbench-result.sh": "#!/bin/sh\n\nset_completed() {\n  SCALE_FACTOR=\"$(grep '^\\s*scaling factor: ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" | sed 's/\\s\\+//g' | cut -d : -f 2 \\\n    | grep -v '^$' || echo null)\"\n  TRANSACTION_PROCESSED=\"$(grep '^\\s*number of transactions actually processed: ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \\\n    | sed 's/\\s\\+//g' | cut -d : -f 2 | cut -d / -f 1 | grep -v '^$' || echo null)\"\n  LATENCY_AVERAGE=\"$(grep '^\\s*latency average = ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \\\n    | sed 's/\\s\\+//g' | cut -d = -f 2 | sed 's/[^0-9.]\\+$//' | grep -v '^$' | format_measure || echo null)\"\n  LATENCY_STDDEV=\"$(grep '^\\s*latency stddev = ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \\\n    | sed 's/\\s\\+//g' | cut -d = -f 2 | sed 's/[^0-9.]\\+$//' | grep -v '^$' | format_measure || echo null)\"\n  TPS_INCLUDING_CONNECTIONS_ESTABLISHING=\"$(grep '^\\s*tps = ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \\\n    | grep '(including connections establishing)$' | sed 's/\\s\\+//g' | cut -d = -f 2 | cut -d '(' -f 1 \\\n    | grep -v '^$' | format_measure || echo null)\"\n  TPS_EXCLUDING_CONNECTIONS_ESTABLISHING=\"$(grep '^\\s*tps = ' \"$SHARED_PATH/$KEBAB_OP_NAME.out\" \\\n    | grep '(excluding connections establishing)$' | sed 's/\\s\\+//g' | cut -d = -f 2 | cut -d '(' -f 1 \\\n    | grep -v '^$' | format_measure || echo null)\"\n  kubectl patch \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" --type=json \\\n    -p \"$(cat \u003c\u003c EOF\n[\n  {\"op\":\"replace\",\"path\":\"/status/conditions\",\"value\":[\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_RUNNING\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_COMPLETED\"),\n      $(eval_in_place \"$CONDITION_DB_OPS_FALSE_FAILED\")\n    ]\n  },\n  {\"op\":\"replace\",\"path\":\"/status/benchmark\",\"value\":{\n      \"pgbench\": {\n        \"scaleFactor\": $SCALE_FACTOR,\n        \"transactionsProcessed\": $TRANSACTION_PROCESSED,\n        \"latency\": {\n          \"average\": {\n            \"value\": $LATENCY_AVERAGE,\n            \"unit\": \"ms\"\n          },\n          \"standardDeviation\": {\n            \"value\": $LATENCY_STDDEV,\n            \"unit\": \"ms\"\n          }\n        },\n        \"transactionsPerSecond\": {\n          \"includingConnectionsEstablishing\": {\n            \"value\": $TPS_INCLUDING_CONNECTIONS_ESTABLISHING,\n            \"unit\": \"tps\"\n          },\n          \"excludingConnectionsEstablishing\": {\n            \"value\": $TPS_EXCLUDING_CONNECTIONS_ESTABLISHING,\n            \"unit\": \"tps\"\n          }\n        }\n      }\n    }\n  }\n]\nEOF\n    )\"\n}\n\nformat_measure() {\n  read INPUT\n  LC_NUMERIC=\"en_US.UTF-8\" printf '%.2f' $INPUT\n}",
        "setup-arbitrary-user.sh": "#!/bin/sh\n\nset -e\n\nUSER_NAME=postgres\nUSER_ID=\"$(id -u)\"\nGROUP_ID=\"$(id -g)\"\nUSER_SHELL=/bin/sh\ncp \"$TEMPLATES_PATH/passwd\" /local/etc/.\ncp \"$TEMPLATES_PATH/group\" /local/etc/.\ncp \"$TEMPLATES_PATH/shadow\" /local/etc/.\ncp \"$TEMPLATES_PATH/gshadow\" /local/etc/.\necho \"$USER_NAME:x:$USER_ID:$GROUP_ID::$PG_BASE_PATH:$USER_SHELL\" \u003e\u003e /local/etc/passwd\nchmod 644 /local/etc/passwd\necho \"$USER_NAME:x:$GROUP_ID:\" \u003e\u003e /local/etc/group\nchmod 644 /local/etc/group\necho \"$USER_NAME\"':!!:18179:0:99999:7:::' \u003e\u003e /local/etc/shadow\nchmod 600 /local/etc/shadow\necho \"$USER_NAME\"':!::' \u003e\u003e /local/etc/gshadow\nchmod 600 /local/etc/gshadow\n",
        "setup-data-paths.sh": "#!/bin/sh\n\nset -e\n\nmkdir -p \"$PG_DATA_PATH\"\nchmod -R 700 \"$PG_DATA_PATH\"\n",
        "setup-scripts.sh": "#!/bin/sh\n\nset -e\n\ncp \"$TEMPLATES_PATH/start-patroni.sh\" \"$LOCAL_BIN_PATH\"\ncp \"$TEMPLATES_PATH/start-patroni-with-restore.sh\" \"$LOCAL_BIN_PATH\"\ncp \"$TEMPLATES_PATH/post-init.sh\" \"$LOCAL_BIN_PATH\"\ncp \"$TEMPLATES_PATH/exec-with-env\" \"$LOCAL_BIN_PATH\"\nsed -i \"s#\\${POSTGRES_PORT}#${POSTGRES_PORT}#g\" \\\n  \"$LOCAL_BIN_PATH/post-init.sh\"\nsed -i \"s#\\${BASE_ENV_PATH}#${BASE_ENV_PATH}#g\" \\\n  \"$LOCAL_BIN_PATH/exec-with-env\"\nsed -i \"s#\\${BASE_SECRET_PATH}#${BASE_SECRET_PATH}#g\" \\\n  \"$LOCAL_BIN_PATH/exec-with-env\"\nchmod a+x \"$LOCAL_BIN_PATH/start-patroni.sh\"\nchmod a+x \"$LOCAL_BIN_PATH/start-patroni-with-restore.sh\"\nchmod a+x \"$LOCAL_BIN_PATH/post-init.sh\"\nchmod a+x \"$LOCAL_BIN_PATH/exec-with-env\"\n",
        "shadow": "root:!locked::0:99999:7:::\nbin:*:18199:0:99999:7:::\ndaemon:*:18199:0:99999:7:::\nadm:*:18199:0:99999:7:::\nlp:*:18199:0:99999:7:::\nsync:*:18199:0:99999:7:::\nshutdown:*:18199:0:99999:7:::\nhalt:*:18199:0:99999:7:::\nmail:*:18199:0:99999:7:::\noperator:*:18199:0:99999:7:::\ngames:*:18199:0:99999:7:::\nftp:*:18199:0:99999:7:::\nnobody:*:18199:0:99999:7:::\n",
        "shell-utils": "#!/bin/sh\n\nLOCK_TIMEOUT=\"${LOCK_TIMEOUT:-60}\"\nLOCK_SLEEP=\"${LOCK_SLEEP:-5}\"\n\nSHELL=\"$(readlink /proc/$$/exe)\"\nif [ \"$(basename \"$SHELL\")\" = busybox ]\nthen\n  SHELL=sh\nfi\nSHELL_XTRACE=$(! echo $- | grep -q x || echo \" -x\")\n\nacquire_lock() {\n  try_lock true\n}\n\nmaintain_lock() {\n  while true\n  do\n    sleep \"$LOCK_SLEEP\"\n    try_lock false\n    if [ \"$?\" != 0 ]\n    then\n      return 1\n    fi\n  done\n}\n\ntry_lock() {\n  if [ -z \"$CLUSTER_NAMESPACE\" -o -z \"$POD_NAME\" -o -z \"$SERVICE_ACCOUNT\" \\\n    -o -z \"$LOCK_RESOURCE\" -o -z \"$LOCK_RESOURCE_NAME\" ]\n  then\n    echo \"CLUSTER_NAMESPACE, SERVICE_ACCOUNT, POD_NAME, LOCK_RESOURCE and LOCK_RESOURCE_NAME environmant variables must be defined\"\n    return 1\n  fi\n  local AQUIRE=\"$1\"\n  local TEMPLATE='\n  LOCK_POD={{ if .metadata.annotations.lockPod }}{{ .metadata.annotations.lockPod }}{{ else }}{{ end }}\n  LOCK_TIMESTAMP={{ if .metadata.annotations.lockTimestamp }}{{ .metadata.annotations.lockTimestamp }}{{ else }}0{{ end }}\n  RESOURCE_VERSION={{ .metadata.resourceVersion }}\n  '\n  kubectl get \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" --template=\"$TEMPLATE\" \u003e /tmp/lock-resource\n  . /tmp/lock-resource\n  CURRENT_TIMESTAMP=\"$(date +%s)\"\n  if [ \"$POD_NAME\" != \"$LOCK_POD\" ]\n  then\n    if \"$AQUIRE\"\n    then\n      if [ \"$((CURRENT_TIMESTAMP-LOCK_TIMESTAMP))\" -lt \"$LOCK_TIMEOUT\" ]\n      then\n        local WAIT_DURATION=\"$((LOCK_TIMEOUT + LOCK_SLEEP))\"\n        echo \"Locked already by $LOCK_POD at $(date -d @\"$LOCK_TIMESTAMP\" -Iseconds --utc), will retry in $WAIT_DURATION seconds\"\n        sleep \"$WAIT_DURATION\"\n        try_lock true\n      fi\n    else\n      echo \"Locked already by $LOCK_POD at $(date -d @\"$LOCK_TIMESTAMP\" -Iseconds --utc)\"\n      return 1\n    fi\n  else\n    if ! \"$AQUIRE\" \u0026\u0026 [ \"$((CURRENT_TIMESTAMP-LOCK_TIMESTAMP))\" -gt \"$LOCK_TIMEOUT\" ]\n    then\n      echo \"Lock expired!\"\n      return 1\n    fi\n  fi\n  if ! kubectl annotate \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" \\\n    --resource-version \"$RESOURCE_VERSION\" --overwrite \\\n    \"lockServiceAccount=$SERVICE_ACCOUNT\" \"lockPod=$POD_NAME\" \"lockTimestamp=$CURRENT_TIMESTAMP\"\n  then\n    kubectl get \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" --template=\"$TEMPLATE\" \u003e /tmp/lock-resource\n    . /tmp/lock-resource\n    if [ \"$POD_NAME\" = \"$LOCK_POD\" ]\n    then\n      try_lock \"$AQUIRE\"\n      return 0\n    fi\n    echo \"Locked by $LOCK_POD at $(date -d @\"$LOCK_TIMESTAMP\" -Iseconds --utc)\"\n    if \"$AQUIRE\"\n    then\n      sleep \"$((LOCK_SLEEP * 4))\"\n      try_lock true\n    else\n      return 1\n    fi\n  fi\n}\n\nrelease_lock() {\n  if [ -z \"$CLUSTER_NAMESPACE\" -o -z \"$SERVICE_ACCOUNT\" -o -z \"$POD_NAME\" \\\n    -o -z \"$LOCK_RESOURCE\" -o -z \"$LOCK_RESOURCE_NAME\" ]\n  then\n    echo \"CLUSTER_NAMESPACE, SERVICE_ACCOUNT, POD_NAME, LOCK_RESOURCE and LOCK_RESOURCE_NAME environmant variables must be defined\"\n    return 1\n  fi\n  local TEMPLATE='\n  LOCK_POD={{ if .metadata.annotations.lockPod }}{{ .metadata.annotations.lockPod }}{{ else }}{{ end }}\n  LOCK_TIMESTAMP={{ if .metadata.annotations.lockTimestamp }}{{ .metadata.annotations.lockTimestamp }}{{ else }}0{{ end }}\n  RESOURCE_VERSION={{ .metadata.resourceVersion }}\n  '\n  kubectl get \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" --template=\"$TEMPLATE\" \u003e /tmp/lock-resource\n  . /tmp/lock-resource\n  if [ \"$POD_NAME\" != \"$LOCK_POD\" ]\n  then\n    return 0\n  fi\n  if ! kubectl annotate \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" \\\n    --resource-version \"$RESOURCE_VERSION\" --overwrite \"lockServiceAccount=\" \"lockPod=\" \"lockTimestamp=0\"\n  then\n    kubectl get \"$LOCK_RESOURCE\" -n \"$CLUSTER_NAMESPACE\" \"$LOCK_RESOURCE_NAME\" --template=\"$TEMPLATE\" \u003e /tmp/lock-resource\n    . /tmp/lock-resource\n    if [ \"$POD_NAME\" = \"$LOCK_POD\" ]\n    then\n      release_lock\n    fi\n    return 0\n  fi\n}\n\nto_json_string() {\n  sed ':a;N;$!ba;s/\\n/\\\\n/g' | sed 's/\\([\"\\\\\\t]\\)/\\\\\\1/g' | tr '\\t' 't'\n}\n\ndate_iso8601() {\n  date +%Y-%m-%dT%H:%M:%SZ --utc\n}\n\ncreate_event() {\n  create_event_try \"$@\" || true\n}\n\ncreate_event_try() {\n  local REASON=\"$1\"\n  local TYPE=\"$2\"\n  local MESSAGE=\"$3\"\n  local EVENT_ID=\"$(get_event_id)\"\n  local EVENT_NAME=\"$DB_OPS_NAME.$EVENT_ID\"\n  local EVENT_TIMESTAMP=\"$(date_iso8601)\"\n  local COMPONENT=\"$OP_NAME\"\n  local RESULT\n\n  RESULT=\"$(kubectl get \"$DB_OPS_CRD_NAME\" -n \"$CLUSTER_NAMESPACE\" \"$DB_OPS_NAME\" -o json)\"\n  MESSAGE=\"$(printf '%s' \"$MESSAGE\" | jq -R .)\"\n  RESULT=\"$(printf '%s' \"$RESULT\" | jq '{\n      apiVersion: \"v1\",\n      type: \"'\"$TYPE\"'\",\n      reason: \"'\"$REASON\"'\",\n      message: '\"$MESSAGE\"',\n      metadata: {\n        name: \"'\"$EVENT_NAME\"'\",\n        namespace: \"'\"$CLUSTER_NAMESPACE\"'\",\n        labels: .metadata.labels\n      },\n      count: 1,\n      firstTimestamp: \"'\"$EVENT_TIMESTAMP\"'\",\n      lastTimestamp: \"'\"$EVENT_TIMESTAMP\"'\",\n      source: {\n        component: \"'\"$COMPONENT\"'\"\n      },\n      involvedObject: {\n        apiVersion: .apiVersion,\n        kind: .kind,\n        namespace: .metadata.namespace,\n        name: .metadata.name,\n        resourceVersion: .metadata.resourceVersion,\n        uid: .metadata.uid\n      }\n    }')\"\n  printf '%s' \"$RESULT\" | kubectl create --raw \"/api/v1/namespaces/$CLUSTER_NAMESPACE/events\" -f -\n}\n\nget_event_id() {\n  local MILLIS_SINCE_EPOCH=\"$(date +%s%N | cut -b1-13)\"\n  local RANDOM_LONG=\"$(get_random_long)\"\n  local HEX_MILLIS=\"$(printf '%x\\n' \"$MILLIS_SINCE_EPOCH\")\"\n  local HEX_RANDOM_LONG=\"$(printf '%x\\n' \"$RANDOM_LONG\")\"\n  echo \"$HEX_MILLIS$HEX_RANDOM_LONG\"\n}\n\nget_random_long() {\n  cat /dev/urandom | tr -dc '0-9' | head -c 15 | sed 's/^0\\+//'\n}\n\nfrom_date_iso8601_to_unix_timestamp() {\n  [ -n \"$1\" ]\n  date -d \"$1\" +%s\n}\n\nkill_with_childs() {\n  (\n  set +e\n  local PID=\"$1\"\n  local SPIDS=\"x\"\n  local OPIDS\n  OPIDS=\"$PID\"\n  local OPPID=\"$OPIDS\"\n  while [ -n \"$OPPID\" ]\n  do\n      OPPID=\"$(grep '^PPid:[[:space:]]'\"\\($(\n            echo \"$OPPID\" | sed '{s/ $//;s/ /\\\\|/g}'\n          )\\)\"'$' /proc/[0-9]*/status 2\u003e/dev/null \\\n        | cut -d / -f 3 | tr '\\n' ' ')\"\n      OPIDS=\"$OPIDS $OPPID\"\n  done\n  kill -13 $OPIDS 2\u003e/dev/null || true\n  )\n}\n\nkill_session_siblings() {\n  (\n  set +e\n  local PID=\"$(exec \"$SHELL\" -c 'echo $PPID')\"\n  local NSUID=\"$(id -u)\"\n  local NSSID=\"$(grep '^NSsid:[[:space:]]' \"/proc/$PID/status\" \\\n        | tr -d '[:space:]' | cut -d : -f 2)\"\n  local SPIDS=\"x\"\n  local OPIDS\n  OPIDS=\"$PID\"\n  local OPPID=\"$OPIDS\"\n  while [ \"$OPPID\" != \"$NSSID\" ]\n  do\n      OPPID=\"$(grep '^PPid:[[:space:]]' \"/proc/$OPPID/status\" \\\n        | tr -d '[:space:]' | cut -d : -f 2)\"\n      OPIDS=\"$OPIDS $OPPID\"\n  done\n  while [ \"$SPIDS\" = x ] \\\n    || [ \"$(ls -d $(echo \"$SPIDS\" | sed 's#\\([0-9]\\+\\) #/proc/\\1 #g') 2\u003e\u00261 | grep -i 'no such' | wc -l)\" \\\n      -lt \"$(echo \"$SPIDS\" | wc -w)\" ]\n  do\n    test \"$SPIDS\" = \"x\" || kill -13 $SPIDS 2\u003e/dev/null || true\n    SPIDS=\"$(grep '\\(^Uid:[[:space:]]\\+'\"$NSUID\"'[[:space:]]\\|^NSsid:[[:space:]]'\"$NSSID\"'$\\)' -c /proc/[0-9]*/status 2\u003e/dev/null \\\n      | grep ':2$' | cut -d / -f 3 | grep -v '^'\"\\($(echo \"$OPIDS\" | sed 's/ /\\\\|/g')\\)\"'$' | tr '\\n' ' ')\"\n  done\n  ) || true\n}\n\ntrap_callback() {\n  kill_session_siblings\n}\n\ntrap_callback_and_exit() {\n  trap_callback\n  exit \"$1\"\n}\n\nset_trap() {\n  trap 'trap_callback_and_exit $?' HUP INT QUIT PIPE TERM ABRT\n  trap 'trap_callback $?' EXIT\n}\n",
        "start-patroni-with-restore.sh": "export HOME=\"$PG_BASE_PATH\"\nexport PATRONI_POSTGRESQL_LISTEN=\"$(eval \"echo $PATRONI_POSTGRESQL_LISTEN\")\"\nexport PATRONI_POSTGRESQL_CONNECT_ADDRESS=\"$(eval \"echo $PATRONI_POSTGRESQL_CONNECT_ADDRESS\")\"\nexport PATRONI_RESTAPI_CONNECT_ADDRESS=\"$(eval \"echo $PATRONI_RESTAPI_CONNECT_ADDRESS\")\"\n\ncat \u003c\u003c 'EOF' | exec-with-env \"${RESTORE_ENV}\" -- sh -ex\nif [ -n \"$ENDPOINT_HOSTNAME\" ] \u0026\u0026 [ -n \"$ENDPOINT_PORT\" ]\nthen\n  if cat \u003c /dev/null \u003e \"/dev/tcp/$ENDPOINT_HOSTNAME/$ENDPOINT_PORT\"\n  then\n    echo \"Host $ENDPOINT_HOSTNAME:$ENDPOINT_PORT reachable\"\n  else\n    echo \"ERROR: Host $ENDPOINT_HOSTNAME:$ENDPOINT_PORT not reachable\"\n    exit 1\n  fi\nfi\nEOF\n\ncat \u003c\u003c EOF \u003e \"$PATRONI_CONFIG_PATH/postgres.yml\"\nscope: ${PATRONI_SCOPE}\nname: ${PATRONI_NAME}\n\nbootstrap:\n  post_init: '${LOCAL_BIN_PATH}/post-init.sh'\n  method: wal_g\n  wal_g:\n    command: '${PATRONI_CONFIG_PATH}/bootstrap'\n    keep_existing_recovery_conf: False\n    recovery_conf:\n      restore_command: 'exec-with-env \"${RESTORE_ENV}\" -- wal-g wal-fetch %f %p'\n$(\n  if [ -n \"$RECOVERY_TARGET_TIME\" ]\n  then\n    cat \u003c\u003c SUB_EOF\n      recovery_target_time: '$RECOVERY_TARGET_TIME'\nSUB_EOF\n  fi\n)\n      recovery_target_timeline: 'latest'\n      recovery_target_action: 'promote'\n  initdb:\n  - auth-host: md5\n  - auth-local: trust\n  - encoding: UTF8\n  - locale: C.UTF-8\n  - data-checksums\n  pg_hba:\n  - 'host all all 0.0.0.0/0 md5'\n  - 'host replication ${PATRONI_REPLICATION_USERNAME} 0.0.0.0/0 md5'\nrestapi:\n  connect_address: '${PATRONI_KUBERNETES_POD_IP}:8008'\n  listen: 0.0.0.0:8008\npostgresql:\n  use_pg_rewind: true\n  remove_data_directory_on_rewind_failure: true\n  use_unix_socket: true\n  connect_address: '${PATRONI_KUBERNETES_POD_IP}:5432'\n  listen: 0.0.0.0:5432\n  authentication:\n    superuser:\n      password: '${PATRONI_SUPERUSER_PASSWORD}'\n    replication:\n      password: '${PATRONI_REPLICATION_PASSWORD}'\n  parameters:\n    unix_socket_directories: '${PATRONI_POSTGRES_UNIX_SOCKET_DIRECTORY}'\n    dynamic_library_path: '${PG_LIB_PATH}:${PG_EXTRA_LIB_PATH}'\n  basebackup:\n    checkpoint: 'fast'\nwatchdog:\n  mode: off\nEOF\n\ncat \u003c\u003c EOF \u003e \"$PATRONI_CONFIG_PATH/bootstrap\"\n#!/bin/sh\n\nexec-with-env \"$RESTORE_ENV\" \\\\\n  -- sh -ec 'if test -n \"$RESTORE_BACKUP_ERROR\"; then echo \"$RESTORE_BACKUP_ERROR\" \u003e\u00262; exit 1; fi'\n\nexec-with-env \"$RESTORE_ENV\" \\\\\n  -- sh -ec 'wal-g backup-fetch \"\\$PG_DATA_PATH\" \"\\$RESTORE_BACKUP_ID\"'\nEOF\nchmod a+x \"$PATRONI_CONFIG_PATH/bootstrap\"\n\nexport LC_ALL=C.UTF-8\n\nunset PATRONI_SUPERUSER_PASSWORD PATRONI_REPLICATION_PASSWORD\n\nexec /usr/bin/patroni \"$PATRONI_CONFIG_PATH/postgres.yml\"\n",
        "start-patroni.sh": "export HOME=\"$PG_BASE_PATH\"\nexport PATRONI_POSTGRESQL_LISTEN=\"$(eval \"echo $PATRONI_POSTGRESQL_LISTEN\")\"\nexport PATRONI_POSTGRESQL_CONNECT_ADDRESS=\"$(eval \"echo $PATRONI_POSTGRESQL_CONNECT_ADDRESS\")\"\nexport PATRONI_RESTAPI_CONNECT_ADDRESS=\"$(eval \"echo $PATRONI_RESTAPI_CONNECT_ADDRESS\")\"\n\ncat \u003c\u003c EOF \u003e \"$PATRONI_CONFIG_PATH/postgres.yml\"\nscope: ${PATRONI_SCOPE}\nname: ${PATRONI_NAME}\n\nbootstrap:\n  post_init: '${LOCAL_BIN_PATH}/post-init.sh'\n  initdb:\n  - auth-host: md5\n  - auth-local: trust\n  - encoding: UTF8\n  - locale: C.UTF-8\n  - data-checksums\n  pg_hba:\n  - 'host all all 0.0.0.0/0 md5'\n  - 'host replication ${PATRONI_REPLICATION_USERNAME} 0.0.0.0/0 md5'\nrestapi:\n  connect_address: '${PATRONI_KUBERNETES_POD_IP}:8008'\n  listen: 0.0.0.0:8008\npostgresql:\n  use_pg_rewind: true\n  remove_data_directory_on_rewind_failure: true\n  use_unix_socket: true\n  connect_address: '${PATRONI_KUBERNETES_POD_IP}:5432'\n  listen: 0.0.0.0:5432\n  authentication:\n    superuser:\n      password: '${PATRONI_SUPERUSER_PASSWORD}'\n    replication:\n      password: '${PATRONI_REPLICATION_PASSWORD}'\n  parameters:\n    unix_socket_directories: '${PATRONI_POSTGRES_UNIX_SOCKET_DIRECTORY}'\n    dynamic_library_path: '${PG_LIB_PATH}:${PG_EXTRA_LIB_PATH}'\n  basebackup:\n    checkpoint: 'fast'\nwatchdog:\n  mode: off\nEOF\n\nexport LC_ALL=C.UTF-8\n\nunset PATRONI_SUPERUSER_PASSWORD PATRONI_REPLICATION_PASSWORD\n\nexec /usr/bin/patroni \"$PATRONI_CONFIG_PATH/postgres.yml\"\n"
      },
      "kind": "ConfigMap",
      "metadata": {
        "annotations": {
          "allResourceAnnotation": "allResourceValue"
        },
        "creationTimestamp": "2021-11-23T12:21:31Z",
        "labels": {
          "app": "StackGresCluster",
          "cluster-name": "test"
        },
        "managedFields": [
          {
            "apiVersion": "v1",
            "fieldsType": "FieldsV1",
            "fieldsV1": {
              "f:data": {
                "f:create-backup.sh": {},
                "f:exec-with-env": {},
                "f:group": {},
                "f:gshadow": {},
                "f:major-version-upgrade.sh": {},
                "f:passwd": {},
                "f:post-init.sh": {},
                "f:relocate-binaries.sh": {},
                "f:reset-patroni.sh": {},
                "f:run-dbops.sh": {},
                "f:run-major-version-upgrade.sh": {},
                "f:run-pgbench.sh": {},
                "f:run-repack.sh": {},
                "f:run-restart.sh": {},
                "f:run-vacuum.sh": {},
                "f:set-dbops-result.sh": {},
                "f:set-dbops-running.sh": {},
                "f:set-pgbench-result.sh": {},
                "f:setup-arbitrary-user.sh": {},
                "f:setup-data-paths.sh": {},
                "f:setup-scripts.sh": {},
                "f:shadow": {},
                "f:shell-utils": {},
                "f:start-patroni-with-restore.sh": {},
                "f:start-patroni.sh": {}
              },
              "f:metadata": {
                "f:annotations": {
                  "f:allResourceAnnotation": {}
                },
                "f:labels": {
                  "f:app": {},
                  "f:cluster-name": {}
                },
                "f:ownerReferences": {
                  "k:{\"uid\":\"d23771de-533a-428d-8918-dbd8ab149614\"}": {
                    ".": {},
                    "f:apiVersion": {},
                    "f:controller": {},
                    "f:kind": {},
                    "f:name": {},
                    "f:uid": {}
                  }
                }
              }
            },
            "manager": "StackGres",
            "operation": "Apply",
            "time": "2021-11-23T12:21:31Z"
          }
        ],
        "name": "test-templates",
        "namespace": "annotations",
        "ownerReferences": [
          {
            "apiVersion": "stackgres.io/v1",
            "controller": true,
            "kind": "SGCluster",
            "name": "test",
            "uid": "d23771de-533a-428d-8918-dbd8ab149614"
          }
        ],
        "resourceVersion": "1432",
        "uid": "bf0fd913-786d-4fed-a870-fde9e0357715"
      }
    }
  ],
  "kind": "List",
  "metadata": {
    "resourceVersion": "",
    "selfLink": ""
  }
}
